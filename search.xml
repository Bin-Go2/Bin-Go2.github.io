<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>数据挖掘, MapReduce 介绍</title>
    <url>/2020/11/10/553week1/</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="Terms"><a href="#Terms" class="headerlink" title="Terms"></a>Terms</h2><ul>
<li><p>Data mining: Discover patterns and models that are:</p>
<ul>
<li>Valid:  hold on new data with some certainty</li>
<li>Useful:  should be possible to act on the item </li>
<li>Unexpected:  non-obvious to the system</li>
<li>Understandable: humans should be able to interpret the pattern</li>
</ul>
</li>
<li><p>DIKW Pyramid: Data, Information, Knowledge and wisdom<br>  <img src="/image/DIKW.png"></p>
</li>
<li><p>Data mining task: </p>
<ul>
<li>Descriptive Models: Find human-interpretable patterns that describe the data</li>
<li>Predictive Models: Use some variables to predict unknown or future values of other variables</li>
</ul>
</li>
<li><p>Data science pipeline<br><img src="/image/ds_pro.png"></p>
</li>
<li><p>Bonferroni’s principle:(roughly) if you look in more places for interesting patterns than your amount of data will support, you are bound to find crap.(数据支撑的前提是数据) </p>
</li>
</ul>
<h2 id="Descriptive-Models"><a href="#Descriptive-Models" class="headerlink" title="Descriptive Models"></a>Descriptive Models</h2><p>Often, especially for ML-type algorithms, the result is a model = a simple representation of the data, typically used for prediction.<br>example: PageRank : representing the “importance” of the page.</p>
<h2 id="Predictive-Models"><a href="#Predictive-Models" class="headerlink" title="Predictive Models"></a>Predictive Models</h2><p>In many applications, all we want is an algorithm that will say “yes” or “no”<br>example: spam email detection</p>
<h2 id="Thing-will-be-convered"><a href="#Thing-will-be-convered" class="headerlink" title="Thing will be convered"></a>Thing will be convered</h2><p>Data: high dimensional｜graph｜infinite/never-ending ｜ labeled<br>Model: MapReduce ｜Streams and online algorithms ｜Single machine in-memory<br>Real-world problems:  Recommender systems| Market Basket Analysis| Spam detection| Duplicate document detection<br>Theory:</p>
<ul>
<li>Linear algebra (SVD, Rec. Sys., Communities)</li>
<li>Optimization (stochastic gradient descent)</li>
<li>Dynamic programming (frequent itemsets)</li>
<li>Hashing (LSH, Bloom filters)</li>
</ul>
<p><img src="/image/553overview.png"></p>
<h2 id="Bonferroni’s-Principle"><a href="#Bonferroni’s-Principle" class="headerlink" title="Bonferroni’s Principle"></a>Bonferroni’s Principle</h2><p>an informal presentation of a statistical theorem:</p>
<ul>
<li>that states if your method of finding significant items returns significantly more items that you would expect in the actual population, </li>
<li>you can assume most of the items you find with it are bogus.</li>
</ul>
<p>简单的说就是FP过多，误诊率太高。</p>
<p>example:</p>
<ol>
<li>Rhine’s Paradox(莱茵悖论), 猜卡片红蓝</li>
<li>Total Information Awareness (TIA)，根据信息预测犯罪, 这个挺有意思。</li>
</ol>
<p>TIA:<br>问题定义: “evil-doers” periodically gather at a hotel to plot their evil.<br>假设: One billion people might be evil-doers; Everyone goes to a hotel one day in 100; A hotel holds 100 people; Examine hotel records for 1000 days;100,000 hotels<br>策略: look for people who, on two different days, were both at the same hotel.<br>真实情况: no evil-doers or few evil-doers(let’s say 10), everyone behaves randomly </p>
<p>根据假设得到的结果:<br>Probability that given persons p  and q  will be at the same hotel on given day d :<br>1/100 x 1/100 x 1/100,000 = 10^-9<br>Probability that p  and q  will be at the same hotel on given days d1 and d2:<br>10^-9 * 10^-9 = 10^-18<br>Pairs of days:<br>1000 x 999/2 = 5x10^5(roughly)<br>Probability that p and q will be at the same hotel on some two days:<br>10^-18 x 5x10^5 = = 5 x 10^-13<br>Pairs of people:<br>10^9 x 10^9 /2 = 5 x 10^17 (roughly)<br>Expected number of “suspicious” pairs of people(根据策略得到的可疑犯罪):<br>5 x 10^-13  x 5 x 10^17 = 250,000 &gt;&gt; 10</p>
<h2 id="Moral-DS道义"><a href="#Moral-DS道义" class="headerlink" title="Moral (DS道义)"></a>Moral (DS道义)</h2><p>When looking for a property (e.g., “two people stayed at the same hotel twice”), make sure that the property does not allow so many possibilities that random data will surely produce facts “of interest.”<br>简单的说，就是需要发现的规则不要受到太多因素的影响。</p>
<h1 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h1><h2 id="Work-flow-of-MapReduce"><a href="#Work-flow-of-MapReduce" class="headerlink" title="Work flow of MapReduce"></a>Work flow of MapReduce</h2><ul>
<li>Read inputs as a set of key-value-pairs</li>
<li>Map transforms input kv-pairs into a new set of k’v’-pairs</li>
<li>Sorts &amp; Shuffles the k’v’-pairs to output nodes</li>
<li>All k’v’-pairs with a given k’ are sent to the same reduce</li>
<li>Reduce processes all k’v’-pairs grouped by key into new k’’v’’-pairs</li>
<li>Write the resulting pairs to files</li>
</ul>
<p>example: documents word count</p>
<p>Problem: Counting the number of occurrences for each word in a collection of documents<br>Input: A repository of documents, and each document is an element</p>
<p>Map function:<br>The Map task reads a document and breaks it into its sequence of words w1, w2, . . . , wn.<br>It then emits a sequence of key-value pairs where the value is always 1.   (w1, 1), (w2, 1), . . . ,(wn, 1)<br>When the Reduce function is associative and commutative, we can push some of what the reducers do to the Map tasks:<br>An option, is to combine m key-value pairs (w, 1) into a single pair (w, m).</p>
<p>Reduce function:<br>The output of a reducer consists of the word and the sum (w, m), where w is a word that appears at least once among.<br>Collects all values and aggregates by key.</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql">input format<br><br>Input: a <span class="hljs-keyword">set</span> <span class="hljs-keyword">of</span> <span class="hljs-keyword">key</span>-<span class="hljs-keyword">value</span> pairs<br>Programmer specifies two methods:<br><span class="hljs-keyword">Map</span>(k, v)  -&gt;  &lt;k’, v’&gt;*<br>Takes a <span class="hljs-keyword">key</span>-<span class="hljs-keyword">value</span> pair <span class="hljs-keyword">and</span> outputs a <span class="hljs-keyword">set</span> <span class="hljs-keyword">of</span> <span class="hljs-keyword">key</span>-<span class="hljs-keyword">value</span> pairs<br>E.g., <span class="hljs-keyword">key</span> <span class="hljs-keyword">is</span> the filename, <span class="hljs-keyword">value</span> <span class="hljs-keyword">is</span> a single line <span class="hljs-keyword">in</span> the <span class="hljs-keyword">file</span><br>There <span class="hljs-keyword">is</span> one <span class="hljs-keyword">Map</span> <span class="hljs-keyword">call</span> <span class="hljs-keyword">for</span> every (k,v) pair<br><br>Reduce(k’, &lt;v’&gt;*) -&gt; &lt;k’, v’’&gt;*<br><span class="hljs-keyword">All</span> <span class="hljs-keyword">values</span> v’ <span class="hljs-keyword">with</span> same <span class="hljs-keyword">key</span> k’ <span class="hljs-keyword">are</span> <span class="hljs-keyword">reduced</span> together <span class="hljs-keyword">and</span> processed <span class="hljs-keyword">in</span> v’ <span class="hljs-keyword">order</span><br>There <span class="hljs-keyword">is</span> one Reduce <span class="hljs-keyword">function</span> <span class="hljs-keyword">call</span> per <span class="hljs-keyword">unique</span> <span class="hljs-keyword">key</span> k’<br><br></code></pre></td></tr></table></figure>


<h2 id="MapReduce-Environment"><a href="#MapReduce-Environment" class="headerlink" title="MapReduce Environment"></a>MapReduce Environment</h2><ol>
<li><code>Partitioning</code> the input data</li>
<li><code>Scheduling</code> the program’s execution across a set of machines</li>
<li>Performing the <code>group by key</code> step : In practice this is the bottleneck</li>
<li>Handling machine <code>failures</code></li>
<li>Managing required inter-machine: <code>Communication</code></li>
</ol>
<h2 id="Dataflow"><a href="#Dataflow" class="headerlink" title="Dataflow"></a>Dataflow</h2><ul>
<li>Input and final output are stored on a distributed file system (HDFS): Scheduler tries to schedule map tasks “close” to physical storage location of input data</li>
<li>Intermediate results are stored on local FS of Map and Reduce workers</li>
<li>Output is often input to another MapReduce task.</li>
</ul>
<h2 id="Grouping-by-Key"><a href="#Grouping-by-Key" class="headerlink" title="Grouping by Key"></a>Grouping by Key</h2><ul>
<li><p>After all Map tasks have all completed successfully:Key-value pairs are grouped by key(Values associated with each key are formed into a list of values)</p>
</li>
<li><p>Grouping is performed by the system, regardless of what the Map and Reduce tasks do</p>
</li>
<li><p>The master controller typically applies a hash function to keys and produces a bucket number from 0 to r-1</p>
</li>
<li><p>Each key that is output by a Map task is hashed and its key-value pair is put in one of r local files</p>
</li>
<li><p>Each file is sent to one of the Reduce tasks.</p>
</li>
</ul>
<h2 id="技巧"><a href="#技巧" class="headerlink" title="技巧"></a>技巧</h2><p>Why perform task in the Map task rather than the Reduce task? Reduce communication: send less data over network; Perform logic in the Map where possible without introducing errors.</p>
<h1 id="Reference-book"><a href="#Reference-book" class="headerlink" title="Reference book"></a>Reference book</h1><p>Mining of Massive Datasets: <a href="http://infolab.stanford.edu/~ullman/mmds/book.pdf">http://infolab.stanford.edu/~ullman/mmds/book.pdf</a></p>
]]></content>
      <tags>
        <tag>Data Mining</tag>
        <tag>Big Data</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱表格理解</title>
    <url>/2020/12/03/558-table-und/</url>
    <content><![CDATA[<h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><ul>
<li>Structured Data<ul>
<li>Knowledge Graphs</li>
<li>The Semantic Web</li>
<li>Databases</li>
<li>Structured files (CSV, XML, XLS)</li>
<li>Structures in HTML, e.g. tables  -&gt; 本章关注</li>
</ul>
</li>
<li>Unstructured Data</li>
</ul>
<h2 id="Web中的结构化表格-Structured-Tables-on-the-Web"><a href="#Web中的结构化表格-Structured-Tables-on-the-Web" class="headerlink" title="Web中的结构化表格 (Structured Tables on the Web)"></a>Web中的结构化表格 (Structured Tables on the Web)</h2><p>Tables contain really useful data<br>Vast Amount of Data in Tabular Form</p>
<h2 id="表格类型"><a href="#表格类型" class="headerlink" title="表格类型"></a>表格类型</h2><p><img src="/image/table_type.png"></p>
<h3 id="Listing"><a href="#Listing" class="headerlink" title="Listing"></a>Listing</h3><p>Horizontal Listings; -&gt; 类似于excel表格的形式，每一行表示某个item的信息<br>Vertical Listings; -&gt; 类似于Dell、Apple商品展现的形式，每一列表示某个item的信息</p>
<h3 id="Attribute-Value-Tables"><a href="#Attribute-Value-Tables" class="headerlink" title="Attribute/Value Tables"></a>Attribute/Value Tables</h3><p>类似于Wikipedia的展现形式，其本质就是一个k-v的字典</p>
<h3 id="Matrix"><a href="#Matrix" class="headerlink" title="Matrix"></a>Matrix</h3><p>二维表格，第一行和第一列都描述了元信息<br>same value type for any cell</p>
<h3 id="Enumeration"><a href="#Enumeration" class="headerlink" title="Enumeration"></a>Enumeration</h3><p>list series of objects with the same ontological relation;<br>major challenge: discovery of the predicate;<br>本质上就是一个list，然后其中包含了某一个属性下的列表信息。</p>
<h3 id="Form"><a href="#Form" class="headerlink" title="Form"></a>Form</h3><p>like attribute-value;<br>with missing values;<br>which are filled by a user</p>
<p>也就是日常见到的表单</p>
<h3 id="Navigational"><a href="#Navigational" class="headerlink" title="Navigational"></a>Navigational</h3><p>purpose: navigate within or outside of a site<br>no clear relations between the cells</p>
<p>也就是网站上常见到一系列可以点击的网页链接放在一块的展示<br><img src="/image/nva.png"></p>
<h3 id="Formatting"><a href="#Formatting" class="headerlink" title="Formatting"></a>Formatting</h3><p>purpose: organize data visually</p>
<p>主要是一些布局展示</p>
<h2 id="表格的属性"><a href="#表格的属性" class="headerlink" title="表格的属性"></a>表格的属性</h2><p>Global Layout features<br>• #rows, #cols, max cell length</p>
<p>Layout features (per row/column)<br>• cell length, length variance, ratio in col/rowspan</p>
<p>HTML features (content)<br>• unique tags, ratio of th/anchor/img/input/br/font…</p>
<p>Cell features (content)<br>• unique strings, end-with-colon, is/contains number, non- blank</p>
<h2 id="提取关系数据-Extracting-Relational-Data"><a href="#提取关系数据-Extracting-Relational-Data" class="headerlink" title="提取关系数据 (Extracting Relational Data)"></a>提取关系数据 (Extracting Relational Data)</h2><p>目标:<br>找到表格中的结构化关系<br>提取出表格下的RDF三元组</p>
<p>关键:<br>• What is the semantic type of each column? • Semantic Labeling  列的含义<br>• What is the subject of the table? • Entity Linking 表格中主体寻找<br>• What predicates describe the relationships in the table? • Semantic Modeling 单元格的关系建立</p>
<h2 id="提取定量数据-Extracting-Quantitative-Data"><a href="#提取定量数据-Extracting-Quantitative-Data" class="headerlink" title="提取定量数据 (Extracting Quantitative Data)"></a>提取定量数据 (Extracting Quantitative Data)</h2><p>• What is quantitative data?<br>• How is it harder than relational data?<br>• How is it easier than relational data?<br>• What is important to represent about this data?</p>
<h3 id="Quantitative-data"><a href="#Quantitative-data" class="headerlink" title="Quantitative data"></a>Quantitative data</h3><p>Quantitative data is defined as the value of data in the form of counts or numbers where each data-set has an unique numerical value associated with it.</p>
<p>Key idea:</p>
<p>• Measure: the thing we want to measure<br>• Attributes: help us understand what we’re measuring<br>• Dimensions: values that are varied across measurements<br>• Observation: The actual value we measured</p>
<p>Measure表示我们需要衡量什么，比如说温度<br>Attributes表示我们衡量的这个东西具有的属性，比如说那一天的温度、那个海拔高度的温度、温度的单位<br>Dimensions表示随着这个东西的改变，衡量的值会不同，比如说地点<br>Observation表示就是观测值，比如说30度</p>
<h3 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h3><p>Attributes有哪些？<br>Dimensions有哪些？<br>Value是什么？</p>
<p>需要做很多操作，例如<br>Metadata Detection, Separation between header and values, Detecting layouts, Detecting dependent rows/columns, Filling the implicit values, Recognizing null values ….</p>
<h3 id="核心任务"><a href="#核心任务" class="headerlink" title="核心任务"></a>核心任务</h3><p>Three core tasks across all table understanding tools:<br>Classify Cell Type;<br>Identify Blocks;<br>Detect Layout;<br><img src="/image/tbl_und.png"></p>
]]></content>
      <tags>
        <tag>Knowledge Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱Blocking&amp;关系型实体解析</title>
    <url>/2020/12/02/558blk-RER/</url>
    <content><![CDATA[<h2 id="实体解析回顾"><a href="#实体解析回顾" class="headerlink" title="实体解析回顾"></a>实体解析回顾</h2><p>Deduplication: Merging Ambiguous Entities (KG-&gt;KG)<br>Record Linkage: Combining KGs (KG1-&gt;KG2)<br>Entity Mapping/Linking: Integrating New Candidates (text -&gt; KG)</p>
<h2 id="Blocking"><a href="#Blocking" class="headerlink" title="Blocking"></a>Blocking</h2><p>简单匹配的问题: Comparing each entity with all other entities is too computationally demanding – O(N2)</p>
<p>解决:  Partition entities into ”blocks” –  O(N) blocks<br>Make only within block comparisons [so if largest block is log N in size – O(N log N2)]</p>
<p>Problem statment:<br>Input: Set of records R  Output: Set of blocks/canopies<br><img src="/image/blk.png"></p>
<p>Variants:<br>Disjoint Blocking: Each mention appears in one block. (=Set Partition)<br>Non-disjoint Blocking: Mentions can appear in more than one block.</p>
<h3 id="Blocking-场景"><a href="#Blocking-场景" class="headerlink" title="Blocking 场景"></a>Blocking 场景</h3><p>Ideal scenario<br><img src="/image/ideal_blk.png"></p>
<p>Worst scenario<br><img src="/image/worst_blk.png"></p>
<p>Practical Scenario<br><img src="/image/prac_blk.png"></p>
<h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><p>Efficiency  = num of pairs compared / total number of pairs in RxR<br>Recall  = num of true matches compared / num of true matches in RxR<br>Precision = um of true matches compared / num of matches compared<br>Max Canopy Size： max|Ci|</p>
<h3 id="Blocking-方式"><a href="#Blocking-方式" class="headerlink" title="Blocking 方式"></a>Blocking 方式</h3><ul>
<li>Feature-based blocking keys</li>
<li>Clustering or sorting</li>
<li>Hashing</li>
</ul>
<h4 id="Feature-based-blocking"><a href="#Feature-based-blocking" class="headerlink" title="Feature-based blocking"></a>Feature-based blocking</h4><p>思想:<br>Pick a key: an attribute or feature of each entity<br>Put all entities with that key in the same block<br>Perform entity resolution within each block</p>
<p>Key的选择考量:<br>Learn the keys, or use expert knowledge/heuristics<br>Schema awareness<br>Key type<br>Redundancy</p>
<p>Key选择扩展:<br>Frequency limits<br>Adaptive keys based on frequency<br>Learning keys based on data</p>
<h4 id="Sort-or-cluster-based-blocking"><a href="#Sort-or-cluster-based-blocking" class="headerlink" title="Sort or cluster-based blocking"></a>Sort or cluster-based blocking</h4><p>思想:<br>Pick an attribute<br>Sort all entities based on that attribute<br>Use a sliding window of |K| entities and compare all pairs.</p>
<p>Canopy Clustering [McCallum et al KDD’00]<br>Input:</p>
<ul>
<li>Mentions M (a set of entities that are needed to be cluster), x is an random chosen entity from M.</li>
<li>d(x,y), a distance metric</li>
<li>Two thresholds T1 and T2, where T1 &gt; T2</li>
</ul>
<p>Algorithm:</p>
<ul>
<li>从M中选择一个随机的x</li>
<li>为x生成一个block，其中的点满足d(x,y) &lt; T1</li>
<li>为x中这个block，删除符合d(x,y)&lt;T2的点 (it means that we are finishing assigning cluster labels to y) </li>
<li>返回第一步 (repeat until all entities in M are assigned to at least one cluster)</li>
</ul>
<h4 id="Hash-based-blocking"><a href="#Hash-based-blocking" class="headerlink" title="Hash-based blocking"></a>Hash-based blocking</h4><p>思想:<br>Each block Ci is associated with a hash key hi.<br>Mention x is hashed to Ci if hash(x) = hi.<br>Within a block, all pairs are compared.<br>Each hash function results in disjoint blocks.</p>
<p>minhash<br>…</p>
<h2 id="Collective-Relational-Entity-Resolution"><a href="#Collective-Relational-Entity-Resolution" class="headerlink" title="Collective Relational Entity Resolution"></a>Collective Relational Entity Resolution</h2><p>策略:<br>Using PSL for collective KG ER</p>
<p>Encode ER dependencies in a set of rules;<br>Use soft-logic values to capture similarities;<br>Use logic to capture the constraints;</p>
<p>Local vs. Collective<br><img src="/image/loca_colle.png"></p>
<h3 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h3><p>¬SAME(E1,E2)<br>CANDSAME(E1,E2) =&gt; SAME(E1,E2)<br>CANDSAME(E1,E2) ∧ SIM(E1,E2)  =&gt; SAME(E1,E2)</p>
<h3 id="Collective"><a href="#Collective" class="headerlink" title="Collective"></a>Collective</h3><p>Symmetry: If M1 matches with M2, then M2 must match with M1<br>Transitivity: If M1 and M2 match, M2 and M3 match, then M1 and M3 match<br>Sparsity: If M1 matches with M2, then M1 cannot match with M3</p>
<p>example:<br>SAME(E1,E2) =&gt; SAME(E2,E1)<br>CANDSAME(A,B) ∧ CANDSAME(B,C) ∧ CANDSAME(A,C) ∧ SAME(A,B) ∧ SAME(B,C) =&gt; SAME(A,C)<br>CANDSAME(A,B) ∧ CANDSAME(B,C) ∧ SAME(A,B) =&gt; ¬SAME(A,C)</p>
<h3 id="New-entity-rules"><a href="#New-entity-rules" class="headerlink" title="New entity rules"></a>New entity rules</h3><ol>
<li>Linking to new entities CANDSAME(E1,E2) ∧ NEWENTITY(E1) =&gt; SAME(E1,E2)</li>
<li>Avoiding new Entities   SAME(E1,E2) ∧ CANDSAME(E1,E3) ∧ NEWENTITY(E3) =&gt; ¬SAME(E1,E3)</li>
</ol>
]]></content>
      <tags>
        <tag>Knowledge Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱数据获取与知识产权</title>
    <url>/2020/12/01/558data-acq/</url>
    <content><![CDATA[<blockquote>
<p>Tim Berners-Lee published the first-ever website in August 6, 1991<br>Knowledge Graph的数据大部分来源于Web</p>
</blockquote>
<h1 id="Web基本信息"><a href="#Web基本信息" class="headerlink" title="Web基本信息"></a>Web基本信息</h1><h2 id="Web结构"><a href="#Web结构" class="headerlink" title="Web结构"></a>Web结构</h2><ul>
<li>Surface Web: Pages reachable by following links from static pages （表层网络，可以直接在万维网浏览的内容）</li>
<li>Deep Web: Pages reachable only via web forms （不能被www直接访问到，需要账号密码，访问权限等才能进入，例如邮箱、数据库）</li>
<li>Dark Web: Pages reachable only via Tor or equivalent （通过特定的浏览器、特殊授权或者特殊设置才能链接上的网络）</li>
</ul>
<p><img src="/image/web_struc.jpg"></p>
<h2 id="Web的Bowtie模型"><a href="#Web的Bowtie模型" class="headerlink" title="Web的Bowtie模型"></a>Web的Bowtie模型</h2><p>Strongly Connected Component – 27.5%<br>IN and OUT – 21.5%<br>Tendrils and tubes – 21.5%<br>Disconnected – 8%</p>
<p><img src="/image/bowtie.png"></p>
<h2 id="Web-数据获取途径-Crawler"><a href="#Web-数据获取途径-Crawler" class="headerlink" title="Web 数据获取途径 - Crawler"></a>Web 数据获取途径 - Crawler</h2><p>Web Crawler定义: Software that automatically browses the web and downloads content<br>Crawler 用途: Building search engines, Study the web/internet, Archive the web, Analyze web sites, <u><b>Download content(知识图谱数据获取)</b></u><br>Crawler 基本框架: 调度器调度爬取页面，下载器多线程下载。<br><img src="/image/crawler_stru.png"><br>Queue: one per web site<br>Scheduler: breath first or depth first ?<br>Multithreaded Downloader: take Politeness and Latency into account<br>After download: Parse -&gt; Extract Content -&gt; Extract URLs -&gt;Add To Queue</p>
<p>Crawler常用工具：Scrapy, bs4, import.io parsehub<br>Crawler 挑战: scale、deduplication、cost、error、freshness、counter-crawling/access …<br>Crawler 要求: robustness、 politeness、robots.txt</p>
<h1 id="Web下的知识产权-Intellectual-Property"><a href="#Web下的知识产权-Intellectual-Property" class="headerlink" title="Web下的知识产权(Intellectual Property)"></a>Web下的知识产权(Intellectual Property)</h1><p>Intellectual property (IP) is an intangible creative work. It is not the physical form on which it is stored or delivered. IP can be protected through the use of patents, copyrights, trademarks, and trade secret laws.</p>
<h2 id="专利-Patents"><a href="#专利-Patents" class="headerlink" title="专利(Patents)"></a>专利(Patents)</h2><p>PATENTS provide rights for up to 20 years for inventions in three broad categories:  </p>
<ol>
<li><p>Utility patents: protect useful processes, machines, articles of manufacture, and compositions of matter (实用专利)</p>
</li>
<li><p>Design patents guard the unauthorized use of new, original, and ornamental designs for articles of manufacture (设计专利)</p>
</li>
<li><p>Plant patents are the way we protect invented or discovered, asexually reproduced plant varieties (植物专利)</p>
</li>
</ol>
<h2 id="版权-Copyrights"><a href="#版权-Copyrights" class="headerlink" title="版权(Copyrights)"></a>版权(Copyrights)</h2><p>COPYRIGHTS protect works of authorship, such as writings, music, and works of art that have been tangibly expressed. （文学领域，版权也称为著作权）<br>The Library of Congress registers copyrights which last the life of the author plus 70 years. （去世前+70年）<br>Books, albums, movies are all copyrighted.<br>You cannot copyright facts, such as the information in the telephone book. But you can copyright the particular presentation of those facts</p>
<p>版权拥有者所拥有的权利:<br>Make copies of the work;<br>Produce derivative works;<br>Distribute copies;<br>Perform the work in public;<br>Display the work in public;<br>这也就意味着，非版权拥有者不能够享有这些权利。 =&gt; 需要经过许可</p>
<p>在某些情况下，不需要许可使用作品:<br>合理使用需要考虑的因素: purpose and character of the work, nature of the copyrighted work, amount and substantiality of the portion used in relation to the copyrighted work, effect on the potential market for or value of the copyrighted work<br>以下情况可以不需要经过版权许可：<br>Use of copyrighted material that contribute to the creation of new work(The new works cannot significantly affect sales of the source material, thus depriving copyright holders of their income)；<br>Use for some research and educational purposes;<br>Use for news reporting and critiquing;</p>
<p>不受版权法保护的作品类型:<br>Works in the public domain are not covered by IP rights;<br>Ideas (e.g., mathematical formulas);<br>Works pre-dating copyright law (e.g., Bible);<br>Expired copyright;<br>Government works;<br>Traditional knowledge, folklore.</p>
<p>知识共享(Creative Commons (CC))<br>Attribution<br>Noncommercial<br>No Derivative Works<br>ShareAlike</p>
<h2 id="商标-Trademarks"><a href="#商标-Trademarks" class="headerlink" title="商标(Trademarks)"></a>商标(Trademarks)</h2><p>TRADEMARKS protect words, names, symbols, sounds, or colors that distinguish goods and services from other manufacturer’s products (商品符号)<br>Trademarks, unlike patents, can be renewed forever as long as they are being used in business. (无限期)<br>No Fair-Use provision in Trademark law</p>
<p>商标种类:<br>™ : unregistered trade mark used to promote or brand goods<br>℠ : unregistered service mark used to promote or brand services<br>® : registered trademark</p>
<h2 id="商业秘密-Trade-Secrets"><a href="#商业秘密-Trade-Secrets" class="headerlink" title="商业秘密(Trade Secrets)"></a>商业秘密(Trade Secrets)</h2><p>TRADE SECRETS are information that companies keep secret to give them an advantage over their competitors. example: formula for Coca-Cola<br>Gives owner perpetual monopoly on secret information (无限期)</p>
<h2 id="数据汇总-Data-Aggregation"><a href="#数据汇总-Data-Aggregation" class="headerlink" title="数据汇总(Data Aggregation)"></a>数据汇总(Data Aggregation)</h2><p>Data aggregation, aka “screen scraping” or “wrapping” is conducted widely today<br>Some of it is on sound legal ground;<br>But some it is has more ambiguous legal standing</p>
<h2 id="软件许可证-Software-Licenses"><a href="#软件许可证-Software-Licenses" class="headerlink" title="软件许可证(Software Licenses)"></a>软件许可证(Software Licenses)</h2><p>Copyleft licenses<br>Permissive licenses</p>
]]></content>
      <tags>
        <tag>Knowledge Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱实体链接</title>
    <url>/2020/12/01/558ent-link/</url>
    <content><![CDATA[<h2 id="NLP中的实体任务"><a href="#NLP中的实体任务" class="headerlink" title="NLP中的实体任务"></a>NLP中的实体任务</h2><ul>
<li>NER (Recognition): detecting the phrase that is the name of an entity 实体发现</li>
<li>NEC (Classification): assigning an entity type to the phrase 实体类型分类</li>
<li>NEL (Linking): establishing the identity of the entity in a given reference<br>database (Wikipedia, DBpedia, YAGO) 实体链接</li>
<li>Coreference: any phrase that makes reference to an entity instance, including<br>pronouns, noun phrases, abbreviations, acronyms, etc… 实体指代</li>
</ul>
<h3 id="Named-Entity-Linking-（NEL）"><a href="#Named-Entity-Linking-（NEL）" class="headerlink" title="Named Entity Linking （NEL）"></a>Named Entity Linking （NEL）</h3><p>Problem statement:<br>Potentially ambiguous entity mention (“Paris”) needs to be linked to a canonical identifier/instance (<a href="http://dbpedia.org/resource/Paris">http://dbpedia.org/resource/Paris</a>) that fits the intended referent in the context of the text</p>
<p>entity linking (text -&gt; KG)</p>
<h3 id="Named-Entity-Recognition-and-Disambiguation-NERD"><a href="#Named-Entity-Recognition-and-Disambiguation-NERD" class="headerlink" title="Named Entity Recognition and Disambiguation (NERD)"></a>Named Entity Recognition and Disambiguation (NERD)</h3><p>假设: the mentions are already recognized in text</p>
<p>Combine recognition and disambiguation/linking -&gt; NERD<br>NERC + NED = NERD</p>
<h2 id="Knowledge-bases"><a href="#Knowledge-bases" class="headerlink" title="Knowledge bases"></a>Knowledge bases</h2><p>A catalog of things, usually entities. Each one has:<br>one or more names;<br>other attributes;<br>Connections to other entities<br>Textual description;</p>
<p>Knowledge bases are also connected to each other</p>
<p>Example:<br>Structured knowledge bases: DBpedia &amp; Wikidata<br>Unstructured knowledge bases: Wikipedia</p>
<h2 id="实体链接的好处"><a href="#实体链接的好处" class="headerlink" title="实体链接的好处"></a>实体链接的好处</h2><p>Benefits of connecting text and knowledge bases:<br>Automatic knowledge base construction (AKBC) / Knowledge base completion (KBC)帮助KB的完善</p>
<h2 id="实体链接的挑战"><a href="#实体链接的挑战" class="headerlink" title="实体链接的挑战"></a>实体链接的挑战</h2><ol>
<li>name ambiguity: Entities with the same name</li>
<li>name variation: Different names for the same entity</li>
<li>Missing (NIL) entities</li>
</ol>
<h2 id="实体链接框架"><a href="#实体链接框架" class="headerlink" title="实体链接框架"></a>实体链接框架</h2><p><img src="/image/EL.png"></p>
<h3 id="Mention-Detection-NER"><a href="#Mention-Detection-NER" class="headerlink" title="Mention Detection (NER)"></a>Mention Detection (NER)</h3><p>在文档中识别出潜在的实体</p>
<h3 id="Candidate-generation-selection"><a href="#Candidate-generation-selection" class="headerlink" title="Candidate generation/selection"></a>Candidate generation/selection</h3><p>Balance between generating too many candidates (too much ‘noise’) and generating too little candidates (missing the correct one)<br>Trade-off between precision and recall -&gt;  an art by itself!<br>In practice, something like 30 candidates per mention is usually enough</p>
<p>如何选择top30?<br>commonness: for a given mention, how relatively often it refers to some instance in Wikipedia.<br>Also, observe dominance within a form and topical bias</p>
<h3 id="Disambiguation"><a href="#Disambiguation" class="headerlink" title="Disambiguation"></a>Disambiguation</h3><p>Goal: decide which of the candidates (or none) is the correct referent.</p>
<p>方法:</p>
<ol>
<li><p>Word-based methods: DBpedia Spotlight<br>Compute cosine similarity between the text paragraph with an entity mention and Wikipedia descriptions of each candidate</p>
</li>
<li><p>Graph-based methods: AIDA and AGDISTIS<br>Construct a subgraph that contains all entity candidates with some facts from a KB, then find the best connected candidates per mention.</p>
</li>
</ol>
<h2 id="实体链接的评估"><a href="#实体链接的评估" class="headerlink" title="实体链接的评估"></a>实体链接的评估</h2><ol>
<li>Assign a true positive (TP), false positive (FP), and/or false negative (FN) per mention occurrence</li>
<li>Count the TPs, FPs, and FNs across all mentions</li>
<li>Compute precision, recall, and F1-scores once on top of these</li>
</ol>
]]></content>
      <tags>
        <tag>Knowledge Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱实体解析</title>
    <url>/2020/12/01/558ent-res/</url>
    <content><![CDATA[<h2 id="为什么需要实体解析？"><a href="#为什么需要实体解析？" class="headerlink" title="为什么需要实体解析？"></a>为什么需要实体解析？</h2><p>Ambiguity: Entities with the same name<br>Variance: Different names for the same entity</p>
<p>这里我们使用record/mention来表示数字世界中的一个实体</p>
<h2 id="什么是实体解析？"><a href="#什么是实体解析？" class="headerlink" title="什么是实体解析？"></a>什么是实体解析？</h2><p>The task of disambiguating records that correspond to real world entities across and within datasets.</p>
<h2 id="实体解析的四个方面"><a href="#实体解析的四个方面" class="headerlink" title="实体解析的四个方面"></a>实体解析的四个方面</h2><p>Coreference: Unifying mentions of the same entity in a single document (text-&gt;text)<br>Entity Linking: Mapping mentions from text to the canonical entity in a KG (text-&gt;KG)<br>Deduplication: Unifying mentions within a KG into a single view (KG-&gt;KG)<br>Record Linkage: Matching enentites across two different KGs (KG1 -&gt; KG2)</p>
<h3 id="Coreference"><a href="#Coreference" class="headerlink" title="Coreference"></a>Coreference</h3><p>举个例子，一篇文档中有许多的代词，需要将这些代词与之前提到的名词进行指代</p>
<h3 id="Entity-Linking"><a href="#Entity-Linking" class="headerlink" title="Entity Linking"></a>Entity Linking</h3><p>举个例子，一篇文档出现诸多实体，需要将这些实体与已有的知识图谱中的实体进行关联</p>
<h3 id="Deduplication"><a href="#Deduplication" class="headerlink" title="Deduplication"></a>Deduplication</h3><p>举个例子，一个知识图谱中存在的实体可能是相同或者很相似的，这个时候可以对其进行聚类，使得一类的mention表示同一个实体。</p>
<h3 id="Record-Linkage"><a href="#Record-Linkage" class="headerlink" title="Record Linkage"></a>Record Linkage</h3><p>举个例子，对多个不同的知识图谱之间的相同的实体进行关联。</p>
<h2 id="如何进行实体解析？"><a href="#如何进行实体解析？" class="headerlink" title="如何进行实体解析？"></a>如何进行实体解析？</h2><p>Assumption:<br>Each record/mention is associated with a single real world entity.<br>If two records/mentions are identical, then they are true matches.</p>
<h3 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h3><p>从一系列records/mentions中计算实体是一个聚类问题<br>ER vs (Multi-relational) Clustering</p>
<ul>
<li>In typical clustering algorithms (k-means, LDA, etc.) number of clusters is a constant or sublinear in R.</li>
<li>In ER: number of clusters is linear in R, and average cluster size is a constant. Significant fraction of clusters are singletons.</li>
</ul>
<h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><p>对一对实体确定匹配或非匹配是一个分类问题</p>
<p>Imbalanced: typically O(R) matches, O(R^2) non-matches</p>
<h2 id="实体解析相关模型"><a href="#实体解析相关模型" class="headerlink" title="实体解析相关模型"></a>实体解析相关模型</h2><h3 id="Blocking"><a href="#Blocking" class="headerlink" title="Blocking"></a>Blocking</h3><p>目的: 为了减少匹配次数<br>Naive pairwise: |R|2 pairwise comparisons<br>example:1000 business listings each from 1,000 different cities across the<br>world =&gt; 1M listings in total =&gt; 1 trillion comparisons<br>If we use Blocking Criterion: City =&gt; 1 billion comparisons</p>
<p>Problem Statement:<br>Input: Set of records R<br>Output: Set of blocks/canopies</p>
<p>Variants:<br>Disjoint Blocking<br>Non-disjoint Blocking</p>
<p>Evalution Metrics:<br>Efficiency  = num of pairs compared / total number of pairs in RxR<br>Recall  = num of true matches compared / num of true matches in RxR<br>Precision = um of true matches compared / num of matches compared<br>Max canopy size: the size of the largest block </p>
<p>Blocking方式:<br>Blocking predicates (keys), e.g. First three characters of last name, City + State + Zip ;<br>该方式存在的问题: Using one or more blocking predicates may be insufficient, 也就是说根据key来blocking的话，很有可能出现两种结果，1是有些key下的block还是有很多的record, 2是NULL values may create large blocks.<br>解决方式： Construct blocking predicates by combining simple predicates</p>
<h3 id="Matching"><a href="#Matching" class="headerlink" title="Matching"></a>Matching</h3><p>Use pairwise predictor to resolve co-referent mentions</p>
<p>Problem Statement: Given a vector of component-wise similarities for a pair of records (x,y), compute P(x and y match)</p>
<p>Type I Error =&gt; False Positive<br>Type II Error =&gt; False Negative</p>
<p>ML Pairwise Approaches:<br>Supervised machine learning algorithms:<br>Decision trees<br>Support vector machines<br>Ensembles of classifiers<br>Conditional Random Fields (CRF)<br>…</p>
<p>存在的问题:<br>Training set generation<br>Imbalanced classes – many more negatives than positives<br>Misclassification cost</p>
<h3 id="Validation"><a href="#Validation" class="headerlink" title="Validation"></a>Validation</h3><p>Apply constraints like 1-1 matching, identify canonical form</p>
<p>Important forms of constraints:<br>• Exclusivity: If M1 matches with M2, then M3 cannot match with M2<br>• Transitivity: If M1 and M2 match, M2 and M3 match, then M1 and M3 match<br>• Functional Dependency: If M1 and M2 match, then M3 and M4 must match</p>
<h2 id="String-matching-problem"><a href="#String-matching-problem" class="headerlink" title="String matching problem"></a>String matching problem</h2><p>Problem Statement:<br>Given X and Y sets of strings, Find pairs (x, y) such that both x and y refer to the same real world entity</p>
<h3 id="为什么字符串不能完美匹配？"><a href="#为什么字符串不能完美匹配？" class="headerlink" title="为什么字符串不能完美匹配？"></a>为什么字符串不能完美匹配？</h3><p>typos, OCR errors, formatting conventions, abbreviations, nick names, word order</p>
<h3 id="评价字符串相似的指标"><a href="#评价字符串相似的指标" class="headerlink" title="评价字符串相似的指标"></a>评价字符串相似的指标</h3><h4 id="Sequence-based-Measures"><a href="#Sequence-based-Measures" class="headerlink" title="Sequence based Measures"></a>Sequence based Measures</h4><ul>
<li>Edit Distance/Levenshtein Distance<br>lev(x, y) is the minimum cost to transform x to y<br>Online calculator: <a href="http://planetcalc.com/1721/">http://planetcalc.com/1721/</a></li>
</ul>
<h4 id="Set-Based-Measures"><a href="#Set-Based-Measures" class="headerlink" title="Set-Based Measures"></a>Set-Based Measures</h4><p>方式: Generate set of tokens from the strings and measure similarity between the sets of tokens</p>
<p>tokenize a string =&gt; “david smith” 3-grams </p>
<ul>
<li>Jaccard Measure</li>
<li>TF/IDF Measure: distinguishing term should carry more weight</li>
</ul>
<h4 id="Hybrid-Measures"><a href="#Hybrid-Measures" class="headerlink" title="Hybrid Measures"></a>Hybrid Measures</h4><p>Do the set-based thing but use a similiarity metric for each element of the set</p>
<h4 id="Phonetic-Similarity-Measures"><a href="#Phonetic-Similarity-Measures" class="headerlink" title="Phonetic Similarity Measures"></a>Phonetic Similarity Measures</h4>]]></content>
      <tags>
        <tag>Knowledge Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱数据提取</title>
    <url>/2020/12/01/558info-extra/</url>
    <content><![CDATA[<blockquote>
<p>NLP是数据提取中不可缺少的元素</p>
</blockquote>
<h1 id="NLP-基础"><a href="#NLP-基础" class="headerlink" title="NLP 基础"></a>NLP 基础</h1><p>什么是NLP？<br>Program computers to process and analyze large amounts of natural language data.</p>
<p>Unstructured、Ambiguous、Lots and lots of data<br>=&gt; Structured、Precise、Actionable Specific to the task data<br>By  Information Extraction</p>
<p>什么是信息提取？<br>Information extraction is the process of extracting information from unstructured textual sources to enable finding entities as well as classifying and storing them. (<a href="https://www.ontotext.com/knowledgehub/fundamentals/information-extraction/">https://www.ontotext.com/knowledgehub/fundamentals/information-extraction/</a>)</p>
<h2 id="Information-Extraction-总览"><a href="#Information-Extraction-总览" class="headerlink" title="Information Extraction 总览"></a>Information Extraction 总览</h2><p>信息提取的三个阶段：</p>
<ol>
<li>从sentence中提取<ul>
<li>Part of speech tagging 词性标注</li>
<li>Dependency Parsing 依存分析    </li>
<li>Named entity recognition 命名实体识别</li>
</ul>
</li>
<li>从document中提取<ul>
<li>Coreference Resolution 指代消解</li>
</ul>
</li>
<li>多个document提取<ul>
<li>Entity resolution 实体解析</li>
<li>Entity linking 实体链接</li>
<li>Relation extraction 关系提取</li>
</ul>
</li>
</ol>
<h3 id="从sentence说起"><a href="#从sentence说起" class="headerlink" title="从sentence说起"></a>从sentence说起</h3><p>信息从句子中提取之前么，首先需要进行Tokenization &amp; Sentence Splitting<br><img src="/image/token_ss.png"></p>
<p>对于Knowledge Graph造成的影响:</p>
<ul>
<li>Strictly constrains other NLP tasks<ul>
<li>Parts of Speech</li>
<li>Dependency Parsing</li>
</ul>
</li>
<li>Directly effects KG nodes/edges<ul>
<li>Mention boundaries</li>
<li>Relations within sentences</li>
</ul>
</li>
</ul>
<h4 id="词性标注-Tagging-Parts-of-Speech"><a href="#词性标注-Tagging-Parts-of-Speech" class="headerlink" title="词性标注(Tagging Parts of Speech)"></a>词性标注(Tagging Parts of Speech)</h4><p><img src="/image/POS.png"></p>
<p>Context matters.<br>Label whole sentence together.<br>Using Conditional Random Fields, CNN, LSTM to do such task.</p>
<p>对于Knowledge Graph造成的影响:</p>
<ul>
<li>Entities appear as nouns</li>
<li>Verbs are very useful<ul>
<li>For identifying relations</li>
<li>For identifying entity types</li>
</ul>
</li>
<li>Important for downstream NLP (NER, Dependency Parsing…)</li>
</ul>
<h4 id="实体识别-Detecting-Named-Entities"><a href="#实体识别-Detecting-Named-Entities" class="headerlink" title="实体识别(Detecting Named Entities)"></a>实体识别(Detecting Named Entities)</h4><p><img src="/image/NER.png"></p>
<p>Context matters.<br>Label whole sentence together</p>
<p>对于Knowledge Graph造成的影响:</p>
<ul>
<li>Mentions describes the nodes</li>
<li>Types of entities are incredibly important(Often restrict relations).</li>
</ul>
<h4 id="依存分析-Dependency-Parsing"><a href="#依存分析-Dependency-Parsing" class="headerlink" title="依存分析(Dependency Parsing)"></a>依存分析(Dependency Parsing)</h4><p><img src="/image/DEP.png"></p>
<p>通过使用模型来对预测的依存关系进行打分来预测。</p>
<p>对于Knowledge Graph造成的影响:</p>
<ul>
<li>Incredibly useful for relations!</li>
<li>Incredibly useful for attributes!</li>
<li>Paths are used as surface relations </li>
</ul>
<p>Surface relation： 仅仅根据句子的结构而不借助语义分析来判定关系模式<br><img src="/image/DEP_PATH.png"></p>
<h3 id="一个Document中的多个sentence"><a href="#一个Document中的多个sentence" class="headerlink" title="一个Document中的多个sentence"></a>一个Document中的多个sentence</h3><h4 id="文档间指代-Within-document-Coreference"><a href="#文档间指代-Within-document-Coreference" class="headerlink" title="文档间指代(Within-document Coreference)"></a>文档间指代(Within-document Coreference)</h4><p><img src="/image/Correference.png"></p>
<p>实现方式:<br>Model: score pairwise links, exmaple: dep path, similarity, representative mention …<br>Prediction: Search over clusterings, example: greedy (left to right), ILP,<br>belief propagation, MCMC …</p>
<p>对于Knowledge Graph造成的影响:<br>More context for each entity!<br>Many relations occur on pronouns.<br>Coref can be used for types.<br>Difficult, so often ignored.</p>
<h3 id="文档间、不同源间的信息提取"><a href="#文档间、不同源间的信息提取" class="headerlink" title="文档间、不同源间的信息提取"></a>文档间、不同源间的信息提取</h3><p>=&gt; Information Extraction</p>
<ul>
<li>Entity resolution 实体解析</li>
<li>Entity linking 实体链接</li>
<li>Relation extraction 关系提取</li>
</ul>
<p><img src="/image/IE.png"></p>
<h2 id="Information-Extraction-细节"><a href="#Information-Extraction-细节" class="headerlink" title="Information Extraction 细节"></a>Information Extraction 细节</h2><h3 id="表面模式-Surface-Patterns"><a href="#表面模式-Surface-Patterns" class="headerlink" title="表面模式 Surface Patterns"></a>表面模式 Surface Patterns</h3><p>Combine tokens, dependency paths, and entity types to define rules.<br>简单的说，就是根据句子、文档的语法结构来判断分析出实体间的关系模式，而忽略了句子的语义信息。</p>
<h3 id="Rule-Based-Extraction"><a href="#Rule-Based-Extraction" class="headerlink" title="Rule-Based Extraction"></a>Rule-Based Extraction</h3><p>通过人为的规定某些语法模式，根据句子的结构来信息提取。</p>
<p>来源:<br>Manually specified, Learned from Data</p>
<p>效果:<br>High precision: when it fires, it’s correct Easy to explain predictions. Easy to fix mistakes.<br>Poor recall: Do not generalize! Only work when the rules fire </p>
<h3 id="Supervised-Extraction"><a href="#Supervised-Extraction" class="headerlink" title="Supervised Extraction"></a>Supervised Extraction</h3><p>通过使用机器学习算法来进行信息的提取</p>
<p>Machine Learning: hopefully, generalizes the labels in the right way<br>Use all of NLP as features: words, POS, NER, dependencies, embeddings</p>
<p>缺陷:<br>Requires a lot of labeled data is needed, which is expensive &amp; time consuming.<br>Requires a lot of feature engineering!</p>
<h3 id="Entity-Resolution-amp-Linking"><a href="#Entity-Resolution-amp-Linking" class="headerlink" title="Entity Resolution &amp; Linking"></a>Entity Resolution &amp; Linking</h3><p>命名实体存在的问题:</p>
<ol>
<li>Different Entities with Same Name</li>
<li>Different Names for same Entities</li>
</ol>
<p>一般步骤：<br>Candidate Generation =&gt; Entity Types =&gt; Coreference =&gt; Coherence</p>
<h2 id="Information-Extraction-三个子问题及解决方式"><a href="#Information-Extraction-三个子问题及解决方式" class="headerlink" title="Information Extraction 三个子问题及解决方式"></a>Information Extraction 三个子问题及解决方式</h2><p>3*3 = 9</p>
<p><img src="/image/IEpro.png"></p>
<h3 id="Defining-Domain"><a href="#Defining-Domain" class="headerlink" title="Defining Domain"></a>Defining Domain</h3><h4 id="Manual"><a href="#Manual" class="headerlink" title="Manual"></a>Manual</h4><p>Highly semantic ontology;<br>Leads to high precision extraction;<br>Expensive to create;<br>Requires domain experts;</p>
<h4 id="Semi-automatic"><a href="#Semi-automatic" class="headerlink" title="Semi-automatic"></a>Semi-automatic</h4><p>Subset of types are manually defined;<br>SSL methods discover new types from unlabeled data;</p>
<p>Easier to derive types using existing resources;<br>Relations are discovered from the corpus;<br>Leads to moderate precision extractions;<br>Partially semantic ontology;</p>
<p>Example:<br>Assumption:<br>Types and type hierarchy is manually defined (E.g. River, City, Food, Chemical, Disease, Bacteria);<br>Relations are automatically discovered using clustering methods</p>
<h4 id="Automatic"><a href="#Automatic" class="headerlink" title="Automatic"></a>Automatic</h4><p>Any noun phrase is a candidate entity<br>Any verb phrase is a candidate relation</p>
<p>Cheapest way to induce types/ relations from corpus;<br>Little expert annotations needed;<br>Limited semantics;<br>Leads to noisy extractions;</p>
<h3 id="Learning-extractors"><a href="#Learning-extractors" class="headerlink" title="Learning extractors"></a>Learning extractors</h3><h4 id="Manual-1"><a href="#Manual-1" class="headerlink" title="Manual"></a>Manual</h4><p>Human defined high-precision extraction patterns for each relation</p>
<h4 id="Semi-supervised"><a href="#Semi-supervised" class="headerlink" title="Semi-supervised"></a>Semi-supervised</h4><p>Bootstrapping</p>
<p><img src="/image/semi-sup.png"><br>简单的说就是，人为给一些seed instances给机器，让机器去提取符合这些instance的pattern，然后根据提取到的pattern去提取符合的instance，重复操作，获取越来越多的instance和pattern.</p>
<p>缺陷:Semantic Drift, 语义漂移<br>解决方式: topk, interactive<br><img src="/image/LE-semi.png"></p>
<h4 id="Unsupervised"><a href="#Unsupervised" class="headerlink" title="Unsupervised"></a>Unsupervised</h4><p>Identify candidate relations: 为每个动词找到最长的单词序列<br>Identify arguments for each relation: 对于每个关系，找其左侧和右侧找到最接近的名词短语</p>
<h3 id="Scoring-candidate-facts"><a href="#Scoring-candidate-facts" class="headerlink" title="Scoring candidate facts"></a>Scoring candidate facts</h3><h4 id="Manual-2"><a href="#Manual-2" class="headerlink" title="Manual"></a>Manual</h4><p>Human defined scoring function or Scoring function learnt using supervised ML with large amount of training data</p>
<h4 id="Semi-supervised-1"><a href="#Semi-supervised-1" class="headerlink" title="Semi-supervised"></a>Semi-supervised</h4><p>Small amount of training data is available. Scoring refined over multiple iterations<br>Using both labeled and unlabeled data</p>
<h4 id="Unsupervised-1"><a href="#Unsupervised-1" class="headerlink" title="Unsupervised"></a>Unsupervised</h4><p>Confidence(extraction pattern) ∝ (#unique instances it could extract)<br>Score(candidate fact) ∝ (#distinct extraction patterns that support it)</p>
<h2 id="Information-Extraction-技术类别"><a href="#Information-Extraction-技术类别" class="headerlink" title="Information Extraction 技术类别"></a>Information Extraction 技术类别</h2><h3 id="Narrow-domain-patterns"><a href="#Narrow-domain-patterns" class="headerlink" title="Narrow domain patterns"></a>Narrow domain patterns</h3><p>Defining domain: Manual<br>Learning extractors:  Manual<br>Scoring candidate facts: Manual</p>
<h3 id="Ontology-based-extraction"><a href="#Ontology-based-extraction" class="headerlink" title="Ontology based extraction"></a>Ontology based extraction</h3><p>Defining domain: Manual<br>Learning extractors:  Semi-supervised<br>Scoring candidate facts: Unsupervised</p>
<h3 id="Interactive-extraction"><a href="#Interactive-extraction" class="headerlink" title="Interactive extraction"></a>Interactive extraction</h3><p>Defining domain: Manual<br>Learning extractors:  Semi-supervised<br>Scoring candidate facts: Semi-supervised</p>
<h3 id="Open-domain-IE"><a href="#Open-domain-IE" class="headerlink" title="Open domain IE"></a>Open domain IE</h3><p>Defining domain: Unsupervised<br>Learning extractors:  Unsupervised<br>Scoring candidate facts: Semi-supervised</p>
<h3 id="Hybrid-approach-Adding-structure-to-OpenIE-KB"><a href="#Hybrid-approach-Adding-structure-to-OpenIE-KB" class="headerlink" title="Hybrid approach (Adding structure to OpenIE KB)"></a>Hybrid approach (Adding structure to OpenIE KB)</h3><p>Defining domain: Semi-supervised =&gt; Distant supervision to add structure<br>Learning extractors:  Unsupervised<br>Scoring candidate facts: Semi-supervised</p>
<h2 id="Knowledge-fusion"><a href="#Knowledge-fusion" class="headerlink" title="Knowledge fusion"></a>Knowledge fusion</h2><p>融合多种Learning extractors来提高提取准确度</p>
<p>Knowledge fusion schemes:</p>
<ul>
<li>Voting (AND vs OR of extractors)</li>
<li>Co-training (multiple extraction methods)</li>
<li>Multi-view learning (multiple data sources)</li>
<li>Classification</li>
</ul>
<h2 id="IE-systems-in-practice"><a href="#IE-systems-in-practice" class="headerlink" title="IE systems in practice"></a>IE systems in practice</h2><ul>
<li>Conceptnet</li>
<li>NELL (Never Ending Language Learning )</li>
<li>Knowledge vault</li>
<li>Open IE</li>
</ul>
<p><img src="/image/IE_sys.png"></p>
]]></content>
      <tags>
        <tag>Knowledge Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱简介</title>
    <url>/2020/12/01/558intro/</url>
    <content><![CDATA[<h2 id="什么是知识图谱？"><a href="#什么是知识图谱？" class="headerlink" title="什么是知识图谱？"></a>什么是知识图谱？</h2><ul>
<li>Knowledge in graph form!</li>
<li>Captures entities, attributes, and relationships</li>
<li>Nodes are entities</li>
<li>Nodes are labeled with attributes (e.g., types)</li>
<li>Typed edges between two nodes capture a relationship between entities</li>
</ul>
<p><img src="/image/kg_overview.png"></p>
<p>简单的说，知识图谱就是一张图，用来描述节点之间的关系。</p>
<h2 id="知识图谱的来源？"><a href="#知识图谱的来源？" class="headerlink" title="知识图谱的来源？"></a>知识图谱的来源？</h2><ul>
<li>Structured Text (Wikipedia Infoboxes, tables, databases, social nets)</li>
<li>Unstructured Text (WWW, news, social media, reference articles)</li>
<li>Images</li>
<li>Video (YouTube, video feeds)</li>
</ul>
<h2 id="知识图谱如何被使用？"><a href="#知识图谱如何被使用？" class="headerlink" title="知识图谱如何被使用？"></a>知识图谱如何被使用？</h2><p>Human perspective:</p>
<ul>
<li>Combat information overload</li>
<li>Explore via intuitive structure</li>
<li>Tool for supporting knowledge-driven tasks</li>
</ul>
<p>AI perspective:</p>
<ul>
<li>Key ingredient for many AI tasks</li>
<li>Bridge from data to human semantics</li>
<li>Use decades of work on graph analysis</li>
</ul>
<p>常见的应用:</p>
<ul>
<li>QA/Agents</li>
<li>Decision Support</li>
<li>Fueling Discovery</li>
</ul>
<p>工业界的使用包括: Google Knowledge Graph(Google Knowledge Vault), Amazon Product Graph, Facebook Graph API, IBM Watson, Microsoft Satori, LinkedIn Knowledge Graph, Yandex Object Answer, Diffbot, GraphIQ, Maana, ParseHub, Reactor Labs, SpazioDati</p>
<h2 id="典型的知识图谱架构？"><a href="#典型的知识图谱架构？" class="headerlink" title="典型的知识图谱架构？"></a>典型的知识图谱架构？</h2><p>基本流程如下：</p>
<ol>
<li>数据获取</li>
<li>信息提取</li>
<li>本体映射</li>
<li>实体解析</li>
<li>知识图谱的部署</li>
<li>图谱具体应用</li>
</ol>
<p><img src="/image/kg_arc.png"></p>
]]></content>
      <tags>
        <tag>Knowledge Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱图嵌入</title>
    <url>/2020/12/03/558kg-emb/</url>
    <content><![CDATA[<h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="数据的表示"><a href="#数据的表示" class="headerlink" title="数据的表示"></a>数据的表示</h3><p>Scalar: a  标量，0维<br>Vector: a[i]， 向量，1维<br>Matrix: a[i,j]， 矩阵，2维<br>Tensor: a[i,j,k]， 张量，多维</p>
<p>Tensor和 Graph的关系:<br>Graphs over time -&gt; 3rd order tensors</p>
<p>Knowledge Graphs:  a directed heterogeneous multigraph whose node and relation types have domain-specific semantics （Vertices: Entities, Edges: Predicates）<br>有向、异源、对关系图</p>
<h3 id="为什么要进行嵌入？"><a href="#为什么要进行嵌入？" class="headerlink" title="为什么要进行嵌入？"></a>为什么要进行嵌入？</h3><p>Meaningful non-numerical representations are highly important, e.g., languages with discrete representations, graphs, shapes</p>
<p>But, Most inference models, e.g., SVM, NN, Reg., etc., are designed for vector-space representable quantities<br>=&gt;<br>To benefits from these models, we need to convert non-numeric representations to numeric representations</p>
<p>简单的说，就是将高维的离散信息转化为低维的连续数据，并且保持其信息的完整性，用于后续的建模分析。</p>
<h3 id="嵌入"><a href="#嵌入" class="headerlink" title="嵌入"></a>嵌入</h3><p>定义:<br>Mapping of a discrete variable to a vector of continuous numbers such that semantic/abstract similarities are encoded in terms of geometric distances</p>
<p>• Low-dimensional -&gt; Mitigate redundancy, curse of dimensionality(避免维度灾难)<br>• Embeddings provide a generalizable context about the overall<br>graph that can be used for interference, e.g., relations.</p>
<h3 id="图嵌入"><a href="#图嵌入" class="headerlink" title="图嵌入"></a>图嵌入</h3><p>Given entities &amp; predicates, find mappings for entities and links (predicate (s,r,t))<br>就是将实体或者关系以向量的形式表达。</p>
<h2 id="图嵌入方法"><a href="#图嵌入方法" class="headerlink" title="图嵌入方法"></a>图嵌入方法</h2><h3 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h3><ul>
<li>Decomposition in terms of singular values</li>
<li>V, U: orthogonal matrices</li>
</ul>
<h3 id="Deep-Graph-Embeddings"><a href="#Deep-Graph-Embeddings" class="headerlink" title="Deep Graph Embeddings"></a>Deep Graph Embeddings</h3><p>Skip-gram: DeepWalk, Node2Vec<br>思想:<br>Sample a set of paths with random walk from node 𝑣i<br>Adjacent Nodes are Similar and should have similar embeddings. The frequency of co-occurrence in random walks is an indicator of node similarity.</p>
<ul>
<li>Hierarchical Softmax (DeepWalk)</li>
<li>Negative Sampling (Node2Vec)</li>
</ul>
<p>其他算法</p>
<p>Metapath2Vec: heterogenous graph<br>LINE: 1st order + 2nd order proximity<br>UltimateWalk: closed form, unifies DeepWalk and Node2Vec reconstruct W,<br>AutoEncoder: similar to SVD<br>Struc2Vec: focuses on structural similarity<br>GCN: interesting! borrowed the idea from CNN</p>
<h2 id="图嵌入应用"><a href="#图嵌入应用" class="headerlink" title="图嵌入应用"></a>图嵌入应用</h2><p>• Reconstruction / Fact checking: Triples completion: (s,?,t)<br>• Classification: Triples classification: (s,r,t)<br>• Featurizing: Link prediction, Recommendation</p>
<h2 id="Tensor与图嵌入"><a href="#Tensor与图嵌入" class="headerlink" title="Tensor与图嵌入"></a>Tensor与图嵌入</h2><p><img src="/image/kg_tensor.png"></p>
<h3 id="PARAFAC"><a href="#PARAFAC" class="headerlink" title="PARAFAC"></a>PARAFAC</h3><p>针对图张量的分解<br><img src="/image/PARAFAC.png"></p>
<h2 id="知识图谱嵌入"><a href="#知识图谱嵌入" class="headerlink" title="知识图谱嵌入"></a>知识图谱嵌入</h2><p>Problem statement:<br>How to enforce KG structures on the embedding representations?</p>
<p>Solution: Triple scoring</p>
<p>Loss: what shall we optimize?</p>
<h3 id="Triple-Scoring"><a href="#Triple-Scoring" class="headerlink" title="Triple Scoring"></a>Triple Scoring</h3><p>What is the relationship among sub (h), pred (r), and obj (t)? • Addition: h + r =?= t<br>表示实体关系向量间的关系。</p>
<ol>
<li>Addition: h + r =?= t</li>
<li>Multiplication: h ⚬ r =?= t</li>
</ol>
<h4 id="Addition"><a href="#Addition" class="headerlink" title="Addition"></a>Addition</h4><p>查看 h + r 是否 = t</p>
<p>TransE算法:<br>score(h,r,t) = – ||h+r-t||   返回结果二范式的负值</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><code class="hljs angelscript">‘Merkel’: h=(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>)<br>‘Beatles’: t =(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>‘plays_bass’: r =(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>) <br>score(h,r,t)=- ||h+ t− r|| = <span class="hljs-number">-1</span><br></code></pre></td></tr></table></figure>
<p>缺陷:<br>从公式就可以看出，该算法不适用于一对多的关系</p>
<p>TransH:<br>Project to relation-specific hyperplanes</p>
<p>TransR:<br>Translate to relation-specific space</p>
<h4 id="Multiplication"><a href="#Multiplication" class="headerlink" title="Multiplication"></a>Multiplication</h4><p>查看 h ⚬ r 是否 =  t</p>
<p>RESCAL算法, score(h,r,t) = h.T ⚬ Wr ⚬ t<br>缺陷: Too many parameters </p>
<p>DistMult算法, score(h,r,t) = h.T ⚬ diag(Wr) ⚬ t<br>好处: Simplify RESCAL by using a diagonal matrix<br>缺陷: Cannot deal with asymmetric relations!</p>
<p>ComplEx算法: score(h,r,t) = Re(h.T ⚬ diag(Wr) ⚬ t)<br>Extend DistMult by introducing complex value embedding, so can handle asymmetric relations</p>
<h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>Closed world assumption: square loss</p>
]]></content>
      <tags>
        <tag>Knowledge Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱图分析&amp;问题回答</title>
    <url>/2020/12/03/558kg-ga-qs/</url>
    <content><![CDATA[<h2 id="图分析"><a href="#图分析" class="headerlink" title="图分析"></a>图分析</h2><p>图分析算法</p>
<p>Pathfinding and Search: Djikstra’s algorithm<br>Matching and coloring: k-coloring, graph equivalence<br>Centrality: Degree centrality, page-rank, betweenness centrality, closeness centrality<br>Clustering and Community Detection: Connected components, clustering coefficient</p>
<p>Matching: a set of edges that do not have a set of common vertices.</p>
<p>Coloring: Assignment of labels traditionally called “colors” to elements of a graph subject to certain constraints.</p>
<h3 id="Centrality"><a href="#Centrality" class="headerlink" title="Centrality"></a>Centrality</h3><p>Eccentricity: Maximum of minimum distances of a node to other nodes in the graph （某个点的偏心率也就是说连接该点到图中其他点的最大距离）<br>Central point: node with lowest eccentricity<br>Radius: distance: central point’s eccentricity (lowest max-distance)<br>Diameter: largest eccentricity (highest max-distance)</p>
<p>Degree Centrality: Nodes with maximum in-degree or out-degree<br>Betweenness Centrality: Number of all-pairs shortest paths a node participates<br>in<br>Closeness (Harmonic) Centrality: Inverse average of distance (or average inverse distance) to all other nodes</p>
<h3 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h3><p>Connected components: Set of nodes where each node is reachable from all other nodes<br>Clustering Coefficient</p>
<h2 id="问答系统"><a href="#问答系统" class="headerlink" title="问答系统"></a>问答系统</h2><p>子任务:<br>• Entity Linking<br>• Relation Linking<br>• Query Structure Discovery<br>• Identifying Logical Operators</p>
<h3 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h3><ul>
<li><p>Lexical Gap between ontology and language:</p>
<ul>
<li>String normalization – different forms of the same word</li>
<li>Query expansion – adding additional terms</li>
<li>Pattern libraries – translating complex phrasal structure into properties</li>
<li>Entailment – using previous answers + reasoning to fill in missing facts</li>
</ul>
</li>
<li><p>Ambiguity of questions and concepts</p>
<ul>
<li>Context-based or corpus-based filtering</li>
<li>Constraints between concepts in the question</li>
<li>Coherence models across ontologies and concept hierarchies</li>
</ul>
</li>
<li><p>Multilingualism</p>
</li>
<li><p>Complexity of queries</p>
<ul>
<li>Multiple levels of indirection</li>
<li>Aggregation, logical comparisons, quantifiers</li>
<li>Indirect references to prior answers, nested queries</li>
<li>Constraints on the answers</li>
</ul>
</li>
<li><p>Distributed Knowledge across several KGs</p>
<ul>
<li>Identifying the right KG with the answer</li>
<li>Understanding entity links between KGs</li>
<li>Predicting missing links</li>
</ul>
</li>
<li><p>Procedural, Spatial, &amp; Temporal QA</p>
<ul>
<li>Answering tasks may require different types of knowledge</li>
<li>Answer may not be an entity (e.g., “How do you make a sandwich?”</li>
<li>Event-based questions require knowledge of “before” “after” “causes”</li>
<li>Questions may use geo-coodinates, spatial relationships, containment</li>
</ul>
</li>
<li><p>Template(-free) questions</p>
<ul>
<li>Question may not match existing structural patterns</li>
<li>Question might require multiple structural schemas</li>
<li>May require aggregation of answers from different query types and knowledge sources</li>
</ul>
</li>
</ul>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p><img src="/image/sol_qa.png"></p>
]]></content>
      <tags>
        <tag>Knowledge Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱表示</title>
    <url>/2020/12/01/558kg-rep/</url>
    <content><![CDATA[<p>本章关注:<br><img src="/image/semantic_web.png"></p>
<h2 id="Web信息表达"><a href="#Web信息表达" class="headerlink" title="Web信息表达"></a>Web信息表达</h2><h3 id="编码规范"><a href="#编码规范" class="headerlink" title="编码规范"></a>编码规范</h3><p>Unicode: a computing industry standard for the consistent encoding, representation and handling of text expressed in most of the world’s writing systems.</p>
<h3 id="Web资源地址"><a href="#Web资源地址" class="headerlink" title="Web资源地址"></a>Web资源地址</h3><p>URL(Uniform Resource Locator):A reference to an Internet resource<br>URL vs URI<br>URI = URL + URN</p>
<p>为什么使用URI?<br>URIs look cool;<br>Precisely identify resources(Avoid confusion among different subjects);<br>Precisely identify properties;<br>Provide information about properties;<br>Look them up on the web</p>
<h2 id="RDF-Resource-Description-Framework"><a href="#RDF-Resource-Description-Framework" class="headerlink" title="RDF (Resource Description Framework)"></a>RDF (Resource Description Framework)</h2><p>The Resource Description Framework (RDF) is a language for representing information about resources in the World Wide Web</p>
<p>Intention: Intended for representing metadata about Web resources, such as the title, author, and modification date of a Web document（为了描述web中资源的元数据信息）<br>=&gt; also be used to represent information about things that can be identified on the Web, even when they cannot be directly retrieved on the Web</p>
<h3 id="Triples"><a href="#Triples" class="headerlink" title="Triples"></a>Triples</h3><p>Represent Information as Triples, (Subject,Predicate,Object)<br>Subject: The resource being described<br>Predicate: A property of the resource<br>Object: The value of the property</p>
<p>通过三元组的形式，很自然的形成了一条有向边的表达方式。</p>
<h3 id="Namespaces"><a href="#Namespaces" class="headerlink" title="Namespaces"></a>Namespaces</h3><p>命名空间，用于简化URI的书写，其中存储了一系列subject或者predicate的信息<br><a href="http://xmlns.com/foaf/0.1/firstName">http://xmlns.com/foaf/0.1/firstName</a> =&gt;  foaf:firstName</p>
<h3 id="RDF-Syntaxes"><a href="#RDF-Syntaxes" class="headerlink" title="RDF Syntaxes"></a>RDF Syntaxes</h3><p>XML, N3 Turtle, N-Triples, RDFa, JSON-LD</p>
<h3 id="Represent-nested-structures"><a href="#Represent-nested-structures" class="headerlink" title="Represent nested structures"></a>Represent nested structures</h3><p><code>usc:isi   schema:address  “4676 Admiralty Way, Marina del Rey, CA 90292” .</code><br>问题:  In what city is USC/ISI located?</p>
<p>使用嵌套节点</p>
<figure class="highlight css"><table><tr><td class="code"><pre><code class="hljs css"><span class="hljs-selector-tag">usc</span><span class="hljs-selector-pseudo">:isi</span> <span class="hljs-selector-tag">schema</span><span class="hljs-selector-pseudo">:address</span> <span class="hljs-selector-tag">usc</span><span class="hljs-selector-pseudo">:isi-address</span> .<br><br><span class="hljs-selector-tag">usc</span><span class="hljs-selector-pseudo">:isi-address</span><br>  <span class="hljs-selector-tag">schema</span><span class="hljs-selector-pseudo">:addressCountry</span> 	“<span class="hljs-selector-tag">USA</span>” ;<br>  <span class="hljs-selector-tag">schema</span><span class="hljs-selector-pseudo">:addressRegion</span> 		“<span class="hljs-selector-tag">CA</span>”;<br>  <span class="hljs-selector-tag">schema</span><span class="hljs-selector-pseudo">:addressLocality</span> 	“<span class="hljs-selector-tag">Marina</span> <span class="hljs-selector-tag">del</span> <span class="hljs-selector-tag">Rey</span>” ; <br>  <span class="hljs-selector-tag">schema</span><span class="hljs-selector-pseudo">:postalCode</span> 		“90292” ;<br>  <span class="hljs-selector-tag">schema</span><span class="hljs-selector-pseudo">:streetAddress</span> 		“4676 <span class="hljs-selector-tag">Admiralty</span> <span class="hljs-selector-tag">Way</span>” . <br></code></pre></td></tr></table></figure>
<p>We minted a URI for USC/ISI’s address, but sometimes we don’t want to mint URIs =&gt; BNode<br>在这里可以发现嵌套节点usc:isi-address也有具体的URI来表示，但是该节点的URI没有什么其他的作用，所以使用空节点来表示, 前缀为_</p>
<figure class="highlight css"><table><tr><td class="code"><pre><code class="hljs css"><span class="hljs-selector-tag">usc</span><span class="hljs-selector-pseudo">:isi</span> <span class="hljs-selector-tag">schema</span><span class="hljs-selector-pseudo">:address</span> _<span class="hljs-selector-pseudo">:isi-address</span> .<br><br>_<span class="hljs-selector-pseudo">:isi-address</span> .<br>  <span class="hljs-selector-tag">schema</span><span class="hljs-selector-pseudo">:addressCountry</span> 	“<span class="hljs-selector-tag">USA</span>” ;<br>  <span class="hljs-selector-tag">schema</span><span class="hljs-selector-pseudo">:addressRegion</span> 		“<span class="hljs-selector-tag">CA</span>”;<br>  <span class="hljs-selector-tag">schema</span><span class="hljs-selector-pseudo">:addressLocality</span> 	“<span class="hljs-selector-tag">Marina</span> <span class="hljs-selector-tag">del</span> <span class="hljs-selector-tag">Rey</span>” ; <br>  <span class="hljs-selector-tag">schema</span><span class="hljs-selector-pseudo">:postalCode</span> 		“90292” ;<br>  <span class="hljs-selector-tag">schema</span><span class="hljs-selector-pseudo">:streetAddress</span> 		“4676 <span class="hljs-selector-tag">Admiralty</span> <span class="hljs-selector-tag">Way</span>” . <br></code></pre></td></tr></table></figure>

<h3 id="字面值的类型（Typed-Literals）"><a href="#字面值的类型（Typed-Literals）" class="headerlink" title="字面值的类型（Typed Literals）"></a>字面值的类型（Typed Literals）</h3><p>在object后面添加类型信息。<br>example: weather:date ”2012-06-18”^^xsd:date ;</p>
<h3 id="Reification"><a href="#Reification" class="headerlink" title="Reification"></a>Reification</h3><p>RDF applications sometimes need to describe other RDF statements using RDF, for instance, to record information about when statements were made, who made them, or other similar information (this is sometimes referred to as “provenance” information). </p>
<p>举个例子： 他说了xxx<br>“On June 19 2012, Claudia said that Sam’s email address is <a href="mailto:&#x53;&#97;&#109;&#64;&#x67;&#x6d;&#x61;&#105;&#x6c;&#46;&#x63;&#111;&#x6d;">&#x53;&#97;&#109;&#64;&#x67;&#x6d;&#x61;&#105;&#x6c;&#46;&#x63;&#111;&#x6d;</a>”</p>
<figure class="highlight groovy"><table><tr><td class="code"><pre><code class="hljs groovy"><span class="hljs-attr">_:</span>s <span class="hljs-attr">rdf:</span>type        <span class="hljs-attr">rdf:</span>Statement .<br><span class="hljs-attr">_:</span>s <span class="hljs-attr">rdf:</span>subject     &lt;<span class="hljs-attr">http:</span><span class="hljs-comment">//szekelys.com/family#sam&gt; .    </span><br><span class="hljs-attr">_:</span>s <span class="hljs-attr">rdf:</span>predicate   <span class="hljs-attr">foaf:</span>mbox .<br><span class="hljs-attr">_:</span>s <span class="hljs-attr">rdf:</span>object      &lt;Sam<span class="hljs-meta">@gmail</span>.com&gt;<br><br><span class="hljs-attr">_:</span>s <span class="hljs-attr">dcterms:</span>date     “<span class="hljs-number">2012</span><span class="hljs-number">-06</span><span class="hljs-number">-19</span>”^^<span class="hljs-attr">xsd:</span>date .<br><span class="hljs-attr">_:</span>s <span class="hljs-attr">dcterms:</span>creator  &lt;<span class="hljs-attr">http:</span><span class="hljs-comment">//uniandes.edu.co/faculty#claudiaj&gt; .</span><br></code></pre></td></tr></table></figure>


<h3 id="RDF-syntax-in-turtle"><a href="#RDF-syntax-in-turtle" class="headerlink" title="RDF syntax in turtle"></a>RDF syntax in turtle</h3><ul>
<li><p>URIs are in &lt;&gt;</p>
</li>
<li><p>Prefix:<br>  @prefix foo: <a href="http://example.org/ns#">http://example.org/ns#</a> .<br>  @prefix : <a href="http://other.example.org/ns#">http://other.example.org/ns#</a> . &lt;- 空命名空间<br>  foo:bar foo: : . 等价于<br>  <a href="http://example.org/ns#bar">http://example.org/ns#bar</a> <a href="http://example.org/ns">http://example.org/ns</a> <a href="http://other.example.org/ns">http://other.example.org/ns</a>.</p>
</li>
<li><p>Turtle Literals</p>
<figure class="highlight lasso"><table><tr><td class="code"><pre><code class="hljs lasso"><span class="hljs-keyword">Type</span> of literals: <br>  untyped (“London” equivalent <span class="hljs-keyword">to</span> “London”^^xsd:<span class="hljs-built_in">string</span>)<br>  language <span class="hljs-built_in">tag</span><br>  <span class="hljs-built_in">data</span> <span class="hljs-keyword">type</span> (<span class="hljs-keyword">with</span> a URI)<br>Example:<br>  Strings <span class="hljs-keyword">in</span> “”<br>  Language <span class="hljs-built_in">tag</span>: <span class="hljs-string">&quot;That Seventies Show&quot;</span>@en<br>  <span class="hljs-built_in">Data</span> <span class="hljs-keyword">type</span>: <span class="hljs-string">&quot;10&quot;</span>^^xsd:<span class="hljs-built_in">decimal</span><br></code></pre></td></tr></table></figure></li>
<li><p>Blank Node<br>  _:a<br>  []</p>
</li>
<li><p>Turtle Base URI ????</p>
</li>
<li><p>常用缩写<br>  rdf:type  = a<br>  “-5”^^xsd:integer<br>  “true”^^xds:boolean<br>  “1.3e2”^^xsd:double</p>
</li>
</ul>
<h2 id="RDF-Schema"><a href="#RDF-Schema" class="headerlink" title="RDF Schema"></a>RDF Schema</h2><p>RDF Schema is the language for defining RDF vocabularies.<br>It specifies the RDF inference rules: the triples that are implied by the triples you have.</p>
<p>常用rdfs:<br><img src="/image/rdfs.png"></p>
<p>All things described by RDF are called resources, and are instances of the class rdfs:Resource</p>
]]></content>
      <tags>
        <tag>Knowledge Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱键连数据与语义网络</title>
    <url>/2020/12/03/558ld-sw/</url>
    <content><![CDATA[<h2 id="Web-of-documents-vs-Web-of-data"><a href="#Web-of-documents-vs-Web-of-data" class="headerlink" title="Web of documents vs. Web of data"></a>Web of documents vs. Web of data</h2><p>Web of Document: 数据之间关联少<br>Analogy: a global filesystem<br>Primary objects: documents<br>Links between: documents (or sub-parts of)<br>Degree of structure in objects: fairly low<br>Semantics of content and links: implicit<br>Designed for: human consumption</p>
<p>Web of Data<br>Analogy:  a global database<br>Primary objects: things (or descriptions of things)<br>Links between: things (including documents)<br>Degree of structure in objects: high<br>Semantics of content and links: explicit<br>Designed for: machines first, humans later</p>
<h2 id="键连数据-Linked-data"><a href="#键连数据-Linked-data" class="headerlink" title="键连数据 (Linked data)"></a>键连数据 (Linked data)</h2><p>定义: A method of publishing structured data so that it can be<br>interlinked and become more useful</p>
<p>键连资料是语义网的主题之一，描述了通过可链接的URI方式来发布、分享、连接Web中各类资源的方法。是一系列利用Web 在不同数据源之间创建语义关联的方法。</p>
<p>Web technologies: HTTP,URIs,RDF to share information processed automatically by computers</p>
<h3 id="Linked-Data-Principles"><a href="#Linked-Data-Principles" class="headerlink" title="Linked Data Principles"></a>Linked Data Principles</h3><ol>
<li>Use URIs as names for things</li>
<li>Use HTTP URIs so that people can look up those names</li>
<li>When someone looks up a URI, provide useful RDF information</li>
<li>Include RDF statements that link to other URIs so that they can discover more things</li>
</ol>
<h3 id="存在问题"><a href="#存在问题" class="headerlink" title="存在问题"></a>存在问题</h3><ul>
<li>Different URIs For the Same Thing</li>
<li>Timeliness （not up to date）</li>
<li>Provenance</li>
<li>Tools</li>
</ul>
<h3 id="Linked-data-评级"><a href="#Linked-data-评级" class="headerlink" title="Linked data 评级"></a>Linked data 评级</h3><p><img src="/image/linkdata.png"></p>
<h2 id="Semantic-Web"><a href="#Semantic-Web" class="headerlink" title="Semantic Web"></a>Semantic Web</h2><p>核心:通过给万维网上的文档添加能够被计算机所理解的语义，从而使整个互联网成为一个通用的信息交换介质</p>
<h2 id="Web中的Thing与Document"><a href="#Web中的Thing与Document" class="headerlink" title="Web中的Thing与Document"></a>Web中的Thing与Document</h2><h3 id="Web-document"><a href="#Web-document" class="headerlink" title="Web document"></a>Web document</h3><p>定义: A Web document is defined as something that has a URI and can return representations (responses in a format such as HTML or JPEG or RDF) of the identified resource in response to HTTP requests.</p>
<blockquote>
<p>An HTML document is not a representation of a person. It is a representation of a Web document.</p>
</blockquote>
<h3 id="内容协商-Content-Negotiation"><a href="#内容协商-Content-Negotiation" class="headerlink" title="内容协商(Content Negotiation)"></a>内容协商(Content Negotiation)</h3><p>定义: 超文本传输协议中定义的一个机制，它使同一个统一资源标志符上的文档可以根据用户代理中指定的适用信息提供不同的版本<br><img src="/image/cn.png"></p>
<h3 id="Web-Thing"><a href="#Web-Thing" class="headerlink" title="Web Thing"></a>Web Thing</h3><blockquote>
<p>We use  need different URIs for Things and Web documents<br>这里的Thing其实也就是知识图谱中的entity</p>
</blockquote>
<p><img src="/image/thing&web.png"><br>从这个例子中也可以发现，某一个实体的web document可以是该实体的一个属性</p>
<h3 id="303-重定向"><a href="#303-重定向" class="headerlink" title="303 重定向"></a>303 重定向</h3><p>解决问题: two documents don’t contain the same content</p>
<p><img src="/image/303.png"></p>
<h3 id="三个URI关系"><a href="#三个URI关系" class="headerlink" title="三个URI关系"></a>三个URI关系</h3><figure class="highlight crystal"><table><tr><td class="code"><pre><code class="hljs crystal">&lt;<span class="hljs-symbol">http:</span>/<span class="hljs-regexp">/ex.com/id</span><span class="hljs-regexp">/alice&gt;</span><br><span class="hljs-regexp">  foaf:page &lt;http:/</span><span class="hljs-regexp">/ex.com/people</span><span class="hljs-regexp">/alice&gt; ; </span><br><span class="hljs-regexp">  rdfs:isDefinedBy &lt;http:/</span><span class="hljs-regexp">/ex.com/data</span><span class="hljs-regexp">/alice&gt; ;</span><br><span class="hljs-regexp">  a foaf:Person ;</span><br><span class="hljs-regexp">  foaf:name &quot;Alice” ;</span><br><span class="hljs-regexp">  foaf:mbox &lt;mailto:alice@example.com&gt; ; ...</span><br></code></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>Knowledge Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱本体与推理</title>
    <url>/2020/12/03/558ont-rea/</url>
    <content><![CDATA[<h2 id="本体-Ontology"><a href="#本体-Ontology" class="headerlink" title="本体 Ontology"></a>本体 Ontology</h2><h3 id="从一个例子讲起"><a href="#从一个例子讲起" class="headerlink" title="从一个例子讲起"></a>从一个例子讲起</h3><figure class="highlight properties"><table><tr><td class="code"><pre><code class="hljs properties"><span class="hljs-attr">name</span>  <span class="hljs-string">rank</span><br><span class="hljs-attr">Bob</span>   <span class="hljs-string">Captain</span><br><span class="hljs-attr">Sue</span>   <span class="hljs-string">Sargent</span><br><span class="hljs-attr">Mary</span>  <span class="hljs-string">Admiral</span><br><span class="hljs-attr">Joe</span>   <span class="hljs-string">Sargent</span><br></code></pre></td></tr></table></figure>
<p>select ?x  { ?x rank “Sargent” }<br>select ?x  { ?x a Person }<br>select ?x  { ?x a MilitaryPerson }</p>
<p>如何进行inference?<br>Tell the computer some triples it infers more triples  + RDFS辅助</p>
<p>=&gt; RDFSchema OWL</p>
<h3 id="OWL-Web-Ontology-Language"><a href="#OWL-Web-Ontology-Language" class="headerlink" title="OWL (Web Ontology Language)"></a>OWL (Web Ontology Language)</h3><p>基本思想:<br>I state a few OWL axioms;<br>I load lots of triples;<br>The system infers lots of new triples.</p>
<p>构建模块:<br>Classes   – 类<br>Properties – 属性<br>Individuals – 单个个体，类似于instance</p>
<h3 id="OWL中的假设"><a href="#OWL中的假设" class="headerlink" title="OWL中的假设"></a>OWL中的假设</h3><ol>
<li>Unique Name Assumption <figure class="highlight css"><table><tr><td class="code"><pre><code class="hljs css"><span class="hljs-selector-tag">bob</span> <span class="hljs-selector-tag">a</span> <span class="hljs-selector-tag">foaf</span><span class="hljs-selector-pseudo">:Person</span>; <br>	<span class="hljs-selector-tag">foaf</span><span class="hljs-selector-pseudo">:name</span> “<span class="hljs-selector-tag">Bob</span>”;<br>	<span class="hljs-selector-tag">ex</span><span class="hljs-selector-pseudo">:rank</span> “<span class="hljs-selector-tag">Captain</span>” .<br><br><br><span class="hljs-selector-tag">sue</span> <span class="hljs-selector-tag">a</span> <span class="hljs-selector-tag">foaf</span><span class="hljs-selector-pseudo">:Person</span>; <br>	<span class="hljs-selector-tag">foaf</span><span class="hljs-selector-pseudo">:name</span> &quot;<span class="hljs-selector-tag">Sue</span>&quot;;<br>	<span class="hljs-selector-tag">ex</span><span class="hljs-selector-pseudo">:rank</span> <span class="hljs-selector-tag">Sargent</span> .<br></code></pre></td></tr></table></figure>
Unique Name Assumption = TRUE:<br>=&gt; bob and sue refer to different individuals in the world</li>
</ol>
<p>Unique Name Assumption = FALSE:<br>=&gt; bob and sue may refer to the same individual in the world</p>
<p>OWL使用的假设: Unique Name Assumption = FALSE</p>
<ol start="2">
<li>Open World or Closed World</li>
</ol>
<p>Closed World: if I don’t tell it something, assume it’s false, 没提到就是false<br>Open World: if I don’t tell it something, don’t assume anything I might tell it later that it is true, 没提到可能是对的</p>
<h3 id="Class"><a href="#Class" class="headerlink" title="Class"></a>Class</h3><p>owl: Thing 所有的invidual均属于owl:Thing类</p>
]]></content>
      <tags>
        <tag>Knowledge Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱复习大纲</title>
    <url>/2020/12/03/558overview/</url>
    <content><![CDATA[<h1 id="Week-1"><a href="#Week-1" class="headerlink" title="Week 1"></a>Week 1</h1><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><h3 id="What-is-a-KG"><a href="#What-is-a-KG" class="headerlink" title="What is a KG?"></a>What is a KG?</h3><p>KG is the Knowledge in graph form, it captures entities, attributes, and relationships. In KG, nodes are entities and they are labeled with attributes. Edges between two nodes capture a relationship between entities</p>
<p>Knowledge Graphs: is a directed heterogeneous multigraph whose node and relation types have domain-specific semantics</p>
<h3 id="Why-KGs"><a href="#Why-KGs" class="headerlink" title="Why KGs?"></a>Why KGs?</h3><p>From human’s perspective, KG helps combat information overload. It also helps explore via intuitive structure. It is also a tool for supporting knowledge-driven tasks.<br>From AI’s perspective, KG’s content is the key ingredient for many AI tasks. It bridges from data to human semantics and people can use decades of work on graph analysis via KG.</p>
<h3 id="How-are-KGs-used"><a href="#How-are-KGs-used" class="headerlink" title="How are KGs used?"></a>How are KGs used?</h3><p>QA/Agents: By extracting suitable candidates (entities, relations or literals) from the KG<br>Decision Support: By reasoning over the KG<br>Fueling Discovery: By reasoning over the KG, By extracting and suggesting suitable candidates in the KG, By matching and analyzing patterns in the KG</p>
<h3 id="Where-do-KGs-come-from"><a href="#Where-do-KGs-come-from" class="headerlink" title="Where do KGs come from?"></a>Where do KGs come from?</h3><p>Structured Text (Wikipedia Infoboxes, tables, databases, social nets)<br>Unstructured Text (WWW, news, social media, reference articles)<br>Images<br>Video (YouTube, video feeds)</p>
<h2 id="Crawling-the-Web-amp-Intellectual-Property"><a href="#Crawling-the-Web-amp-Intellectual-Property" class="headerlink" title="Crawling the Web &amp; Intellectual Property"></a>Crawling the Web &amp; Intellectual Property</h2><h3 id="Surface-web-vs-deep-web-vs-dark-web"><a href="#Surface-web-vs-deep-web-vs-dark-web" class="headerlink" title="Surface web vs. deep web vs. dark web"></a>Surface web vs. deep web vs. dark web</h3><p>Surface web:  Pages reachable by following links from static pages, people can access these pages directly<br>Deep Web: Pages reachable only via web forms , for example: email information, databases<br>Dark Web: Pages reachable only via Tor or equivalent or other specific tools.</p>
<h3 id="What-are-the-challenges"><a href="#What-are-the-challenges" class="headerlink" title="What are the challenges?"></a>What are the challenges?</h3><p>scale<br>deduplication<br>cost (fetching, parsing/extracting, memory/disk * speed)<br>errors, redirects<br>freshness<br>deep web, forms<br>counter-crawling/access (login, captchas, traps, fake errors, banning)<br>localization<br>dynamic pages<br>infinite scrolling<br>archiving</p>
<h3 id="What-is-the-basic-architecture-of-a-crawler"><a href="#What-is-the-basic-architecture-of-a-crawler" class="headerlink" title="What is the basic architecture of a crawler?"></a>What is the basic architecture of a crawler?</h3><p>Basically, a crawler can be split into four parts: Scheduler, Queue, Multi-thread downloader, Storage. The procedure of the crawler is : first a user providers a list of web pages(URLS) to Scheduler. Then Scheduler shedules these URLs by putting them into Queue accoring to certain rules. Then Scheduler assigns the different URLs in Queue to  Multi-thread downloader to let the downloader automatically crawls the information. After information is downloaded, content will be parsed and content will be extracted. There might be some urls extracted, the Scheduler will review thses urls and and the worthy ones to Queue again. Target data(Text and metadata) will be storeed into Storages. The crawler will end if there is no url to crawl of the user terminate it. </p>
<h3 id="What-mechanisms-exist-to-protect-intellectual-property"><a href="#What-mechanisms-exist-to-protect-intellectual-property" class="headerlink" title="What mechanisms exist to protect intellectual property?"></a>What mechanisms exist to protect intellectual property?</h3><p>Patents<br>Copyrights<br>Trademarks<br>Trade Secrets<br>Software Licenses</p>
<h3 id="What-are-the-requirements-for-a-patent"><a href="#What-are-the-requirements-for-a-patent" class="headerlink" title="What are the requirements for a patent?"></a>What are the requirements for a patent?</h3><p>Patent can be split into 3 main types.<br>Utility patents protect useful processes, machines, articles of manufacture, and compositions of matter.<br>Design patents guard the unauthorized use of new, original, and ornamental designs for articles of manufacture.<br>Plant patents are the way we protect invented or discovered, asexually reproduced plant varieties<br>Patents provide rights for up to 20 years for inventions</p>
<h3 id="What-can-be-copyrighted-and-what-is-the-difference-in-CC-licenses"><a href="#What-can-be-copyrighted-and-what-is-the-difference-in-CC-licenses" class="headerlink" title="What can be copyrighted, and what is the difference in CC licenses?"></a>What can be copyrighted, and what is the difference in CC licenses?</h3><p>COPYRIGHTS protect works of authorship, such as writings, music, and works of art that have been tangibly expressed. So basically, these creative following works can be copyrighted:<br>Literary, musical and dramatic works.<br>Pantomimes and choreographic works.<br>Pictorial, graphic and sculptural works.<br>Sound recordings.<br>Motion pictures and other AV works.<br>Computer programs.<br>Compilations of works and derivative works.<br>Architectural works. </p>
<p>Attribution: Licensees may copy, distribute, display and perform the work only if they give owner the credits<br>Noncommercial: Licensees may copy, distribute, etc. the work only for noncommercial purpose<br>No Derivative Works: Licensees may copy, distribute, etc. the work, not derivatives based on it.<br>ShareAlike: Licensees may distribute derivative works only under a license identical to the license that governs the original work.</p>
<h1 id="Week-2"><a href="#Week-2" class="headerlink" title="Week 2"></a>Week 2</h1><h2 id="Information-Extraction"><a href="#Information-Extraction" class="headerlink" title="Information Extraction"></a>Information Extraction</h2><h3 id="What-are-some-typical-IE-tasks"><a href="#What-are-some-typical-IE-tasks" class="headerlink" title="What are some typical IE tasks?"></a>What are some typical IE tasks?</h3><p>sentence:<br>Part of speech tagging<br>Dependency Parsing<br>Named entity recognition </p>
<p>document:<br>Coreference Resolution </p>
<p>documents:<br>Entity resolution<br>Entity linking<br>Relation extraction </p>
<h3 id="What-are-the-three-components-to-IE"><a href="#What-are-the-three-components-to-IE" class="headerlink" title="What are the three components to IE?"></a>What are the three components to IE?</h3><p>Defining domain<br>Learning extractors<br>Scoring the facts</p>
<h3 id="What-are-the-possible-levels-of-supervision-for-each-component"><a href="#What-are-the-possible-levels-of-supervision-for-each-component" class="headerlink" title="What are the possible levels of supervision for each component?"></a>What are the possible levels of supervision for each component?</h3><p>Supervised<br>Semi-supervised<br>Unsupervised</p>
<h3 id="Real-world-O-IE-systems"><a href="#Real-world-O-IE-systems" class="headerlink" title="Real-world (O)IE systems"></a>Real-world (O)IE systems</h3><p>Open domain IE:<br>Defining domain: Unsupervised<br>Learning extractors: Unsupervised<br>Scoring candidate facts: Semi-supervised</p>
<h2 id="Knowledge-Representation"><a href="#Knowledge-Representation" class="headerlink" title="Knowledge Representation"></a>Knowledge Representation</h2><h3 id="What-are-the-basic-elements-of-RDF"><a href="#What-are-the-basic-elements-of-RDF" class="headerlink" title="What are the basic elements of RDF?"></a>What are the basic elements of RDF?</h3><p>S(subject)P(predicate)O(object)</p>
<h3 id="What-are-the-different-syntaxes-for-RDF"><a href="#What-are-the-different-syntaxes-for-RDF" class="headerlink" title="What are the different syntaxes for RDF?"></a>What are the different syntaxes for RDF?</h3><p>XML, N3 Turtle, N-Triples, RDFa, JSON-LD</p>
<h3 id="What-is-RDF-Schema"><a href="#What-is-RDF-Schema" class="headerlink" title="What is RDF Schema?"></a>What is RDF Schema?</h3><p>RDF Schema is the language for defining RDF vocabularies.<br>It specifies the RDF inference rules: the triples that are implied by the triples you have.</p>
<h3 id="Degree-of-semantics-in-graphs"><a href="#Degree-of-semantics-in-graphs" class="headerlink" title="Degree of semantics in graphs"></a>Degree of semantics in graphs</h3><p>????</p>
<h1 id="Week-3"><a href="#Week-3" class="headerlink" title="Week 3"></a>Week 3</h1><h2 id="Entity-Resolution"><a href="#Entity-Resolution" class="headerlink" title="Entity Resolution"></a>Entity Resolution</h2><h3 id="Which-ER-variants-exist"><a href="#Which-ER-variants-exist" class="headerlink" title="Which ER variants exist?"></a>Which ER variants exist?</h3><p>Ambiguity: Entities with the same name<br>Variance: Different names for the same entity</p>
<h3 id="What-are-the-three-basic-steps-of-ER"><a href="#What-are-the-three-basic-steps-of-ER" class="headerlink" title="What are the three basic steps of ER?"></a>What are the three basic steps of ER?</h3><p>？？？</p>
<h3 id="How-do-we-evaluate-blocking"><a href="#How-do-we-evaluate-blocking" class="headerlink" title="How do we evaluate blocking?"></a>How do we evaluate blocking?</h3><p>Efficiency = num of pairs compared / total number of pairs in RxR<br>Recall = num of true matches compared / num of true matches in RxR<br>Precision = um of true matches compared / num of matches compared<br>Max canopy size: the size of the largest block</p>
<h3 id="What-are-real-world-examples-of-these-ER-settings"><a href="#What-are-real-world-examples-of-these-ER-settings" class="headerlink" title="What are real-world examples of these ER settings?"></a>What are real-world examples of these ER settings?</h3><p>text -&gt; text ?<br>text -&gt; KG ?<br>KG -&gt; KG ?<br>KG1 -&gt; KG2 ?</p>
<h1 id="Week-4"><a href="#Week-4" class="headerlink" title="Week 4"></a>Week 4</h1><h2 id="Queries-amp-KGs"><a href="#Queries-amp-KGs" class="headerlink" title="Queries &amp; KGs"></a>Queries &amp; KGs</h2><h3 id="RDF-vs-Property-Graphs"><a href="#RDF-vs-Property-Graphs" class="headerlink" title="RDF vs. Property Graphs"></a>RDF vs. Property Graphs</h3><p>Similarity:<br>Both represent directed graphs as a basic data structure;<br>Both have associated graph-oriented query languages;<br>In practice, both are used as “graph stores”, accessible via HTTP and/or various API-s;</p>
<p>Differences:<br>RDF has an emphasis on OWA, and is rooted in the Web via URL-s. Not the case for PG, PG node is oblivious to what it “contains”: can be a URL, can be a literal; (node不一定是uri)<br>PG includes the possibility to add simple key/value pairs to “relationships” (i.e., RDF predicates) （关系中可以带属性）</p>
<h3 id="RDF-triple-stores-vs-Graph-DBs"><a href="#RDF-triple-stores-vs-Graph-DBs" class="headerlink" title="RDF triple-stores vs. Graph DBs"></a>RDF triple-stores vs. Graph DBs</h3><p>In RDF triple-stores everything is expressed in terms of SPO and its predicates can’t have attributes;<br>In Graph DBs predicates can have attributes;<br>Fair to say that RDF triple-stores are a kind of Graph DB;</p>
<h3 id="SPARQL-syntax-Graph-Patterns-Aggregation-etc…"><a href="#SPARQL-syntax-Graph-Patterns-Aggregation-etc…" class="headerlink" title="SPARQL: syntax, Graph Patterns, Aggregation, etc…"></a>SPARQL: syntax, Graph Patterns, Aggregation, etc…</h3><h3 id="SPARQL-vs-Cypher"><a href="#SPARQL-vs-Cypher" class="headerlink" title="SPARQL vs. Cypher"></a>SPARQL vs. Cypher</h3><p>SPARQL Protocol and RDF Query Language , this is RDF’s QUERY LANGUAGE<br>Neo4j’s QUERY LANGUAGE</p>
<h2 id="Special-Topics-KG-Use-Cases"><a href="#Special-Topics-KG-Use-Cases" class="headerlink" title="Special Topics: KG Use Cases"></a>Special Topics: KG Use Cases</h2><h3 id="Wikidata-data-model"><a href="#Wikidata-data-model" class="headerlink" title="Wikidata data model"></a>Wikidata data model</h3><p>Model using statements Subject, property, value, Qualifiers, references</p>
<h3 id="Challenges-and-methods-for-building-KGs-in-real-applications"><a href="#Challenges-and-methods-for-building-KGs-in-real-applications" class="headerlink" title="Challenges and methods for building KGs in real applications"></a>Challenges and methods for building KGs in real applications</h3><p>challenge: make the annotations easy to use<br>hundreds of millions of sex ads on the open internet…many trafficking related<br>Data obfuscation makes even simple questions hard to automatically answer</p>
<p>solution:<br>USE KG in tsv format<br>Knowledge Graph Is A TSV File<br>easy to process </p>
<h1 id="Week-5"><a href="#Week-5" class="headerlink" title="Week 5"></a>Week 5</h1><h2 id="Large-KGs-amp-Entity-Linking"><a href="#Large-KGs-amp-Entity-Linking" class="headerlink" title="Large KGs &amp; Entity Linking"></a>Large KGs &amp; Entity Linking</h2><h3 id="Which-kinds-of-large-KGs-exist"><a href="#Which-kinds-of-large-KGs-exist" class="headerlink" title="Which (kinds of) large KGs exist?"></a>Which (kinds of) large KGs exist?</h3><p>DBpedia, YAGO, Wikidata, Freebase, ConceptNet, NELL, OpenIE</p>
<h3 id="How-is-Wikidata-different-from-other-large-KGs"><a href="#How-is-Wikidata-different-from-other-large-KGs" class="headerlink" title="How is Wikidata different from other large KGs?"></a>How is Wikidata different from other large KGs?</h3><p>???</p>
<h3 id="What-are-the-different-methods-to-get-data-from-large-KGs"><a href="#What-are-the-different-methods-to-get-data-from-large-KGs" class="headerlink" title="What are the different methods to get data from large KGs?"></a>What are the different methods to get data from large KGs?</h3><p>??? </p>
<h3 id="What-is-entity-linking-from-text-and-why-is-it-hard"><a href="#What-is-entity-linking-from-text-and-why-is-it-hard" class="headerlink" title="What is entity linking (from text) and why is it hard?"></a>What is entity linking (from text) and why is it hard?</h3><p>Establishing the identity of the entity in a given reference<br>database (text-&gt;KG).<br>name ambiguity: Entities with the same name<br>name variation: Different names for the same entity<br>Missing (NIL) entities</p>
<h3 id="What-is-the-basic-architecture-of-entity-linkers"><a href="#What-is-the-basic-architecture-of-entity-linkers" class="headerlink" title="What is the basic architecture of entity linkers?"></a>What is the basic architecture of entity linkers?</h3><p>mention detection -&gt; candidate selection -&gt; Disambiguation -&gt; entity annotatiton</p>
<h3 id="What-are-some-methods-for-disambiguation"><a href="#What-are-some-methods-for-disambiguation" class="headerlink" title="What are some methods for disambiguation?"></a>What are some methods for disambiguation?</h3><p>Word-based methods: DBpedia Spotlight: Compute cosine similarity between the text paragraph with an entity mention and Wikipedia descriptions of each candidate.Decide for one mention at a time. The linking can be restricted to certain types or even to a custom set of entities.<br>Graph-based methods: AIDA and AGDISTIS: 1. construct a subgraph that contains all entity candidates with some facts from a KB. 2. find the best connected candidates per mention.</p>
<h2 id="String-Similarity"><a href="#String-Similarity" class="headerlink" title="String Similarity"></a>String Similarity</h2><h3 id="Which-similarity-families-exist-and-what-is-the-main-idea-of-each"><a href="#Which-similarity-families-exist-and-what-is-the-main-idea-of-each" class="headerlink" title="Which similarity families exist and what is the main idea of each?"></a>Which similarity families exist and what is the main idea of each?</h3><h3 id="What-are-the-strengths-and-weaknesses-of-each-method"><a href="#What-are-the-strengths-and-weaknesses-of-each-method" class="headerlink" title="What are the strengths and weaknesses of each method?"></a>What are the strengths and weaknesses of each method?</h3><h3 id="How-do-hybrid-methods-work"><a href="#How-do-hybrid-methods-work" class="headerlink" title="How do hybrid methods work?"></a>How do hybrid methods work?</h3><h1 id="Week-6"><a href="#Week-6" class="headerlink" title="Week 6"></a>Week 6</h1><h2 id="Information-Extraction-1"><a href="#Information-Extraction-1" class="headerlink" title="Information Extraction"></a>Information Extraction</h2><h3 id="What-are-labeling-functions-Snorkel-What-makes-them-good"><a href="#What-are-labeling-functions-Snorkel-What-makes-them-good" class="headerlink" title="What are labeling functions (Snorkel)? What makes them good?"></a>What are labeling functions (Snorkel)? What makes them good?</h3><p>Labeling functions (LFs) help users encode domain knowledge and other supervision sources programmatically, it is a function desgined by human to help label data points.<br>Instead of poeple manually labeling the points, people can write heuristics to noisily label data! Human can leverage real-world knowledge, context, and common-sense heuristics to make labeling decisions. This help quick and low cost labeling.</p>
<h3 id="What-does-the-generative-model-do"><a href="#What-does-the-generative-model-do" class="headerlink" title="What does the generative model do?"></a>What does the generative model do?</h3><p>LFs have different latent accuracies, Snorkel wants to learn these latent accuracies without labeled data by leveraging overlap and conflict of LFs.</p>
<p>Snorkel creates a generative model to maximize the marginal likelihood of the LFs to learn parameters Intuitively, compares their agreements and disagreements, and detect correlations and other dependencies among LFs to correct their accuracies.</p>
<h3 id="What-is-the-purpose-of-the-discriminative-model"><a href="#What-is-the-purpose-of-the-discriminative-model" class="headerlink" title="What is the purpose of the discriminative model?"></a>What is the purpose of the discriminative model?</h3><p>Compiling Rules into Features.</p>
<p>The output of the generative model is a set of probabilistic training labels<br>Snorkel wants to use these labels to train final discriminative model. The discriminative model learns a feature representation of our LFs. </p>
<p>The discriminative model aims to be able to generalize beyond the noisy LFs. The more unlabeled data we train with Snorkel, the better is the predictive performance of the discriminatory model.</p>
<h1 id="Week-7"><a href="#Week-7" class="headerlink" title="Week 7"></a>Week 7</h1>]]></content>
      <tags>
        <tag>Knowledge Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱查询</title>
    <url>/2020/12/01/558query-kg/</url>
    <content><![CDATA[<h2 id="Property-Graphs"><a href="#Property-Graphs" class="headerlink" title="Property Graphs"></a>Property Graphs</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>Also called Labeled Property Graphs (LPG)</p>
<p>It is a framework for representing data and metadata with a graph of nodes and links</p>
<ul>
<li>both nodes and links may have additional key/value pairs (“properties”) </li>
<li>nodes are “just” nodes, not necessarily URLs</li>
</ul>
<p>Link annotations are very useful to assign temporal, spatial, provenance, etc.</p>
<p><img src="/image/LPG.png"></p>
<h3 id="LPG-vs-RDF"><a href="#LPG-vs-RDF" class="headerlink" title="LPG vs. RDF"></a>LPG vs. RDF</h3><p>Similarity:<br>Both represent directed graphs as a basic data structure;<br>Both have associated graph-oriented query languages;<br>In practice, both are used as “graph stores”, accessible via HTTP and/or various API-s;</p>
<p>Differences:<br>RDF has an emphasis on OWA, and is rooted in the Web via URL-s. Not the case for PG, PG node is oblivious to what it “contains”: can be a URL, can be a literal;  (node不一定是uri)<br>PG includes the possibility to add simple key/value pairs to “relationships” (i.e., RDF predicates) （关系中可以带属性）</p>
<h3 id="RDF-triple-stores-vs-Graph-DBs"><a href="#RDF-triple-stores-vs-Graph-DBs" class="headerlink" title="RDF triple-stores vs. Graph DBs"></a>RDF triple-stores vs. Graph DBs</h3><p>In RDF triple-stores everything is expressed in terms of SPO and its predicates can’t have attributes;<br>In Graph DBs predicates can have attributes;<br>Fair to say that RDF triple-stores are a kind of Graph DB;</p>
<p><img src="/image/rdf-gdb.png"></p>
<h2 id="SPARQL-SPARQL-Protocol-and-RDF-Query-Language"><a href="#SPARQL-SPARQL-Protocol-and-RDF-Query-Language" class="headerlink" title="SPARQL (SPARQL Protocol and RDF Query Language)"></a>SPARQL (SPARQL Protocol and RDF Query Language)</h2><p>SELECT/ASK/CONSTRUCT/DESCRIBE</p>
<p>Main idea:  Pattern matching (想像成对图进行搜索)</p>
<p>Queries describe sub-graphs of the queried graph;<br>Graph patterns are RDF graphs specified in Turtle syntax, which contain variables (prefixed by either “?” or “$”);<br>Sub-graphs that match the graph patterns yield a result</p>
<h3 id="图模式-Graph-Pattern"><a href="#图模式-Graph-Pattern" class="headerlink" title="图模式 Graph Pattern"></a>图模式 Graph Pattern</h3><p>基本匹配: Where a set of triple patterns must match<br>集合匹配 {}: Where a set of graph patterns must all match<br>可选匹配 OPTIONAL: Where additional patterns may extend the solution =&gt; 简单理解就是这个节点有属性则加，无则省略<br>附加匹配 UNION: Where two or more possible patterns are tried<br>命名图 GRAPH: Where patterns are matched against named graphs</p>
<h3 id="FILTER-NOT-EXISTS-vs-MINUS"><a href="#FILTER-NOT-EXISTS-vs-MINUS" class="headerlink" title="FILTER NOT EXISTS vs. MINUS"></a>FILTER NOT EXISTS vs. MINUS</h3><figure class="highlight asciidoc"><table><tr><td class="code"><pre><code class="hljs asciidoc">Data<br>@prefix  :       &lt;http://example/&gt; .<br>@prefix  rdf:    &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .<br>@prefix  foaf:   &lt;http://xmlns.com/foaf/0.1/&gt; .<br><br><span class="hljs-meta">:alice  rdf:type</span>   foaf:Person .<br><span class="hljs-meta">:alice  foaf:name</span>  &quot;Alice&quot; .<br><span class="hljs-meta">:bob    rdf:type</span>   foaf:Person . <br><br>Query:<br>PREFIX  rdf:    &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; <br>PREFIX  foaf:   &lt;http://xmlns.com/foaf/0.1/&gt; <br><br>SELECT ?person<br>WHERE <br>&#123;<br><span class="hljs-code">    ?person rdf:type  foaf:Person .</span><br><span class="hljs-code">    FILTER NOT EXISTS &#123; ?person foaf:name ?name &#125; # =&gt; 筛选出没有这条边的点</span><br>&#125; <br><br>Result<br>person<br>&lt;http://example/bob&gt;<br><br><br>Data:<br>@prefix :       &lt;http://example/&gt; .<br>@prefix foaf:   &lt;http://xmlns.com/foaf/0.1/&gt; .<br><br><span class="hljs-meta">:alice  foaf:givenName</span> &quot;Alice&quot; ;<br><span class="hljs-code">        foaf:familyName &quot;Smith&quot; .</span><br><span class="hljs-meta">:bob    foaf:givenName</span> &quot;Bob&quot; ;<br><span class="hljs-code">        foaf:familyName &quot;Jones&quot; .</span><br><span class="hljs-meta">:carol  foaf:givenName</span> &quot;Carol&quot; ;<br><span class="hljs-code">        foaf:familyName &quot;Smith&quot; </span><br><br>Query:<br>PREFIX :     &lt;http://example/&gt;<br>PREFIX foaf: &lt;http://xmlns.com/foaf/0.1/&gt;<br><br>SELECT DISTINCT ?s<br>WHERE &#123;<br><span class="hljs-code">   ?s ?p ?o .</span><br><span class="hljs-code">   MINUS &#123;</span><br><span class="hljs-code">      ?s foaf:givenName &quot;Bob&quot; .</span><br><span class="hljs-code">   &#125;</span><br>&#125;<br><br>Result:<br>s<br>&lt;http://example/carol&gt;<br>&lt;http://example/alice&gt;<br></code></pre></td></tr></table></figure>
<p>简单的说, FILTER 共享对象引用 MINUS 不共享, 例子</p>
<figure class="highlight ruby"><table><tr><td class="code"><pre><code class="hljs ruby"><span class="hljs-symbol">Data:</span> <br>@prefix : &lt;<span class="hljs-symbol">http:</span>/<span class="hljs-regexp">/example/</span>&gt; .<br><span class="hljs-symbol">:a</span> <span class="hljs-symbol">:b</span> <span class="hljs-symbol">:c</span> .<br><br><span class="hljs-symbol">Query:</span><br>SELECT *<br>&#123; <br>  <span class="hljs-string">?s</span> <span class="hljs-string">?p</span> <span class="hljs-string">?o</span><br>  FILTER NOT EXISTS &#123; <span class="hljs-string">?x</span> <span class="hljs-string">?y</span> <span class="hljs-string">?z</span> &#125; <span class="hljs-comment"># =&gt; 这里  ?s = ?x</span><br>&#125;<br><br><span class="hljs-symbol">Result:</span><br>Null<br><br><br><span class="hljs-symbol">Data:</span> <br>@prefix : &lt;<span class="hljs-symbol">http:</span>/<span class="hljs-regexp">/example/</span>&gt; .<br><span class="hljs-symbol">:a</span> <span class="hljs-symbol">:b</span> <span class="hljs-symbol">:c</span> .<br><br><span class="hljs-symbol">Query:</span><br>SELECT *<br>&#123; <br>   <span class="hljs-string">?s</span> <span class="hljs-string">?p</span> <span class="hljs-string">?o</span> <br>   MINUS <br>     &#123; <span class="hljs-string">?x</span> <span class="hljs-string">?y</span> <span class="hljs-string">?z</span> &#125;  <span class="hljs-comment"># =&gt; 这里 ?s != ?x</span><br>&#125;<br><br>Result<br>s 	                       p                  o<br>&lt;<span class="hljs-symbol">http:</span>/<span class="hljs-regexp">/example/a</span>&gt; &lt;<span class="hljs-symbol">http:</span>/<span class="hljs-regexp">/example/b</span>&gt; &lt;<span class="hljs-symbol">http:</span>/<span class="hljs-regexp">/example/c</span>&gt;<br><br><br></code></pre></td></tr></table></figure>

<h3 id="Aggregation"><a href="#Aggregation" class="headerlink" title="Aggregation"></a>Aggregation</h3><p>SELECT (SUM(?lprice) AS ?totalPrice)<br>GROUP BY ?org<br>GROUP BY ?a ?b ?c<br>HAVING (SUM(?lprice) &gt; 10)</p>
<h3 id="Subqueries"><a href="#Subqueries" class="headerlink" title="Subqueries"></a>Subqueries</h3><p>原则: 先计算内部，在算外部的</p>
<h3 id="RDF-Dataset"><a href="#RDF-Dataset" class="headerlink" title="RDF Dataset"></a>RDF Dataset</h3><p>RDF Dataset =<br>    default graph<br>    + named graph 1<br>    + named graph 2<br>    + …</p>
<p>Separate graphs enable you to reason about who said what and when</p>
<h3 id="常用SPARQL语句"><a href="#常用SPARQL语句" class="headerlink" title="常用SPARQL语句"></a>常用SPARQL语句</h3><figure class="highlight groovy"><table><tr><td class="code"><pre><code class="hljs groovy"><span class="hljs-attr">Data:</span><br><span class="hljs-meta">@prefix</span> <span class="hljs-attr">foaf:</span>  &lt;<span class="hljs-attr">http:</span><span class="hljs-comment">//xmlns.com/foaf/0.1/&gt; .</span><br>          <br><span class="hljs-attr">_:</span>a  <span class="hljs-attr">foaf:</span>givenName   <span class="hljs-string">&quot;John&quot;</span> .<br><span class="hljs-attr">_:</span>a  <span class="hljs-attr">foaf:</span>surname  <span class="hljs-string">&quot;Doe&quot;</span> .<br><br><span class="hljs-attr">Query:</span><br>PREFIX <span class="hljs-attr">foaf:</span>   &lt;<span class="hljs-attr">http:</span><span class="hljs-comment">//xmlns.com/foaf/0.1/&gt;</span><br>SELECT ?name<br>WHERE  &#123; <br>   ?P foaf:givenName ?G ; <br>      foaf:surname ?S <br>   BIND(CONCAT(?G, <span class="hljs-string">&quot; &quot;</span>, ?S) AS ?name)  # =&gt; Creating Values with Expressions<br><br>&#125;<br><br>Result:<br>name<br>“John Doe”<br><br><br>Data:<br>@prefix dc:   &lt;http:<span class="hljs-comment">//purl.org/dc/elements/1.1/&gt; .</span><br>@prefix :     &lt;<span class="hljs-attr">http:</span><span class="hljs-comment">//example.org/book/&gt; .</span><br><span class="hljs-meta">@prefix</span> <span class="hljs-attr">ns:</span>   &lt;<span class="hljs-attr">http:</span><span class="hljs-comment">//example.org/ns#&gt; .</span><br><br>:book1  <span class="hljs-attr">dc:</span>title  <span class="hljs-string">&quot;SPARQL Tutorial&quot;</span> .<br>:book1  <span class="hljs-attr">ns:</span>price  <span class="hljs-number">42</span> .<br>:book2  <span class="hljs-attr">dc:</span>title  <span class="hljs-string">&quot;The Semantic Web&quot;</span> .<br>:book2  <span class="hljs-attr">ns:</span>price  <span class="hljs-number">23</span> .<br><br><span class="hljs-attr">Query:</span><br>PREFIX <span class="hljs-attr">foaf:</span>   &lt;<span class="hljs-attr">http:</span><span class="hljs-comment">//xmlns.com/foaf/0.1/&gt;</span><br>PREFIX  <span class="hljs-attr">dc:</span>  &lt;<span class="hljs-attr">http:</span><span class="hljs-comment">//purl.org/dc/elements/1.1/&gt;</span><br>SELECT  ?title<br>WHERE   &#123; ?x dc:title ?title <br>          FILTER regex(?title, <span class="hljs-string">&quot;^SPARQL&quot;</span>)  # =&gt; Restricting the Value of <br>          Strings<br>          # FILTER (?price &lt; <span class="hljs-number">30.5</span>)<br>        &#125;<br><br>Result:<br>title<br><span class="hljs-string">&quot;SPARQL Tutorial&quot;</span><br><br><br><br></code></pre></td></tr></table></figure>




]]></content>
      <tags>
        <tag>Knowledge Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱RDFa</title>
    <url>/2020/12/03/558rdfa/</url>
    <content><![CDATA[<h2 id="RDFa"><a href="#RDFa" class="headerlink" title="RDFa"></a>RDFa</h2><p>Resource Description Framework  in  attributes</p>
<p>定义:<br>RDFa是一个W3C推荐标准。它扩展了XHTML的几个属性，网页制作者可以利用这些属性在网页中添加可供机器读取的后设资料。与RDF资料模型的对应关系使得RDFa可以将RDF的三元组嵌入在XHTML文档中，它也使得符合标准的使用端可以从RDFa文件中提取出这些RDF三元组来。</p>
<p>简单的说：RDFa就是在html中嵌入rdf信息。</p>
<h2 id="RDFa-嵌入"><a href="#RDFa-嵌入" class="headerlink" title="RDFa 嵌入"></a>RDFa 嵌入</h2><figure class="highlight vim"><table><tr><td class="code"><pre><code class="hljs vim">&lt;<span class="hljs-keyword">p</span> xmln<span class="hljs-variable">s:dc</span>=<span class="hljs-comment">&quot;http://purl.org/dc/elements/1.1/”   -- Namespace 声明</span><br>about=<span class="hljs-string">&quot;http://www.example.com/books/wikinomics&quot;</span>&gt; -- Resource 声明<br>In <span class="hljs-keyword">his</span> latest book<br>&lt;cite property=<span class="hljs-string">&quot;dc:title&quot;</span> &gt;Wikinomics&lt;/cite&gt;,   -- 属性声明<br><span class="hljs-symbol">&lt;span&gt;</span>Don Tapscott&lt;/span&gt;<br>explains deep <span class="hljs-keyword">changes</span> in technology, demographics <span class="hljs-built_in">and</span> business.<br>The book <span class="hljs-keyword">is</span> due <span class="hljs-keyword">to</span> <span class="hljs-keyword">be</span> published in <span class="hljs-symbol">&lt;span&gt;</span>October <span class="hljs-number">2006</span>&lt;/span&gt;.<br>&lt;/<span class="hljs-keyword">p</span>&gt;<br></code></pre></td></tr></table></figure>

<p>SPO =&gt;<br><a href="http://www.example.com/books/wikinomics">http://www.example.com/books/wikinomics</a>  dc:title  “Wikinomics”</p>
<p>嵌入RDFa的好处:<br>Not only human can Look Up the Meaning on the Web, but computers Can Look It Up Too.机器也可以去理解语义了。</p>
<p>例子:<br><img src="/image/rdfa.png"></p>
<h2 id="RDFa-问题与解决"><a href="#RDFa-问题与解决" class="headerlink" title="RDFa 问题与解决"></a>RDFa 问题与解决</h2><ol>
<li>格式问题<br><code>&lt;span property=&quot;dc:date”&gt;October 2006&lt;/span&gt;. 在这里的话，October 2006 is not a computer understandable date e.g., an xsd:date</code><br>解决: 使用 Content Attribute<figure class="highlight xml"><table><tr><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">span</span> </span><br><span class="hljs-tag"><span class="hljs-attr">property</span>=<span class="hljs-string">&quot;dc:date”</span></span><br><span class="hljs-tag"><span class="hljs-string">content=&quot;</span><span class="hljs-attr">2006-10-01</span>” </span><br><span class="hljs-tag"><span class="hljs-attr">datatype</span>=<span class="hljs-string">“xsd:date”</span>&gt;</span> October 2006<span class="hljs-tag">&lt;/<span class="hljs-name">span</span>&gt;</span>.<br></code></pre></td></tr></table></figure>
只要html标签中存在content的属性，那么RDF客户端在读取数据时自动会将content的内容来代替实际文本内容</li>
</ol>
<h2 id="RDFa-例子"><a href="#RDFa-例子" class="headerlink" title="RDFa 例子"></a>RDFa 例子</h2><h3 id="RDFa-vocab-and-typeof"><a href="#RDFa-vocab-and-typeof" class="headerlink" title="RDFa: vocab and typeof"></a>RDFa: vocab and typeof</h3><p>vocab: Define default vocabulary for an HTML element 相当于一个词汇表<br>typeof: Declare the type of this property</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">vocab</span>=<span class="hljs-string">&quot;http://xmlns.com/foaf/0.1/&quot;</span>&gt;</span>   -&gt; 这里声明了一个vocab，后续的标签中可以使用该词汇表中的术语<br><br><span class="hljs-tag">&lt;<span class="hljs-name">li</span> <span class="hljs-attr">typeof</span>=<span class="hljs-string">&quot;Person&quot;</span>&gt;</span>。 --&gt; 这里的Person表示的是foaf:person的含义，不需要使用uri显示表示<br><span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;http://example.com/eve/&quot;</span>&gt;</span>Eve<span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span> <br><span class="hljs-tag">&lt;/<span class="hljs-name">li</span>&gt;</span><br></code></pre></td></tr></table></figure>


<h3 id="RDFa-property-and-href"><a href="#RDFa-property-and-href" class="headerlink" title="RDFa: property and href"></a>RDFa: property and href</h3><p>When href is present, it supplies the value for the property.  也就是说，这里的href的值表示了SPO中的O</p>
<p><code>&lt;a property=“homepage” href=&quot;http://example.com/bob/&quot;&gt;Bob&lt;/a&gt;</code></p>
<p><img src="/image/ex_rdfa.png"></p>
]]></content>
      <tags>
        <tag>Knowledge Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱信息提取-Snorkel</title>
    <url>/2020/12/02/558snorkel/</url>
    <content><![CDATA[<h2 id="Dark-Data-Extraction"><a href="#Dark-Data-Extraction" class="headerlink" title="Dark Data Extraction"></a>Dark Data Extraction</h2><p>Structured Data(4%): Data easy to process by machines<br>Dark Data(96%): Valuable &amp; hard to process. (占据的web信息的绝大部分)<br>example: Scientific articles &amp; government reports，Medical Images</p>
<p>DDE:  Dark Data(Text, Tables, Images, Diagrams, etc)  =&gt; Structured Data (Enables analyses, interfaces etc.)</p>
<h3 id="DDE-传统流程"><a href="#DDE-传统流程" class="headerlink" title="DDE 传统流程"></a>DDE 传统流程</h3><p>1.Candidate Extraction =&gt; 2.Training Set  =&gt; 3.Feature Extraction =&gt; 4.Learning &amp; Inference</p>
<p>Example: Chemical-Disease Relation Extraction from Text<br>方式1: Human defines candidate entity mentions to populate a relational schema with relation mentions （费事费力，需要domain expertis）<br>方式2: Relation Extraction with Machine Learning (Feature engineering is the bottleneck,however this bottleneck is solved by deep learning) 解决3，但是2没被解决</p>
<blockquote>
<p> Training data is THE ML dev bottleneck today.</p>
</blockquote>
<h3 id="DDE挑战"><a href="#DDE挑战" class="headerlink" title="DDE挑战"></a>DDE挑战</h3><p>Dark data extraction systems still take months or years to build using state-of-the-art machine learning (ML) systems<br>时间太久，成本太高。</p>
<p>Training Data Creation: $$$, Slow, Static 成本高，耗时长，灵活性差</p>
<ul>
<li>Expensive &amp; Slow: Especially when domain expertise needed</li>
<li>Static: Real-world problems change but hand-labeled training data does not.</li>
</ul>
<h2 id="Snorkel"><a href="#Snorkel" class="headerlink" title="Snorkel"></a>Snorkel</h2><blockquote>
<p>Motiviton: Can we use noisier training data and still train high-performance models?</p>
</blockquote>
<p>Snorkel: A System for Rapidly Generating Training Data with Weak Supervision<br>简单的说，这是一个生成训练集的框架。</p>
<p><img src="/image/snorkel.png"></p>
<h2 id="Snorkel-Pipeline-概括"><a href="#Snorkel-Pipeline-概括" class="headerlink" title="Snorkel Pipeline 概括"></a>Snorkel Pipeline 概括</h2><ul>
<li>Users write labeling functions to generate noisy labels</li>
<li>Snorkel models and combine these labels</li>
<li>We use the resulting probabilistic training labels to train a model<br><img src="/image/pipeline_snorkel.png"></li>
</ul>
<h3 id="Labeling-Functions"><a href="#Labeling-Functions" class="headerlink" title="Labeling Functions"></a>Labeling Functions</h3><p>Users give Snorkel labeling functions that label data points<br>简答的说，这个Labeling function的起到的作用和human expertise类似，用于规定一些规则来让snorkel后续过程中发现text里与其匹配的模式来对提取的信息信息标注。</p>
<p>因此，用户可以书写尽可能多的Labeling Functions来提高准确性，并且Labeling Functions之间可以有冲突，因为后续用到的是概率来确定结果。</p>
<h3 id="Model-Combine-Iterate"><a href="#Model-Combine-Iterate" class="headerlink" title="Model, Combine, Iterate"></a>Model, Combine, Iterate</h3><p>Snorkel models and combine these labels</p>
<p>这里snorkl根据用户书写的Labeling functions进行建模，并迭代运行模型，得到一个probabilistic training labels </p>
<h3 id="Train-End-Model"><a href="#Train-End-Model" class="headerlink" title="Train End Model"></a>Train End Model</h3><p>Use the resulting probabilistic training labels to train a model</p>
<h2 id="Snorkel-challenge-Step2"><a href="#Snorkel-challenge-Step2" class="headerlink" title="Snorkel challenge - Step2:"></a>Snorkel challenge - Step2:</h2><blockquote>
<p>How do we model and combine LFs? How to best reweight and combine the noisy supervision signal?</p>
</blockquote>
<p>解决方式: A Generative Model of the Training Data Labeling Process<br>使用生成式模型反映各个LF得到的标注结果<br><img src="/image/GM_s.png"></p>
<p><img src="/image/example_snorkel.png"></p>
<p>从中也可以发现，LF可以存在冲突，并且对他存在相关联的LF建模分析也能够提高精度。（ We can learn dependency structure using statistical and/or static analysis techniques [ICML ’17, NIPS ‘17]）</p>
<h2 id="Snorkel-Pipeline-细节"><a href="#Snorkel-Pipeline-细节" class="headerlink" title="Snorkel Pipeline 细节"></a>Snorkel Pipeline 细节</h2><h3 id="Designing-Labeling-Functions-LFs"><a href="#Designing-Labeling-Functions-LFs" class="headerlink" title="Designing Labeling Functions (LFs)"></a>Designing Labeling Functions (LFs)</h3><p>Write heuristics to noisily label data! 区别于人工手动标记，这里snorkel写label function的目的是为了后续的Programmatically generate training data</p>
<p>如何设计Labeling function:<br>Human annotators leverage real-world knowledge, context, and common-sense heuristics to make labeling decisions</p>
<p>Labeling function 结果:<br>{-1, 0, 1} =&gt; {Negative, Abstain, Positive}</p>
<p>流程:<br>text中提取candidates，candidates中包含true和false的instances，使用设计的Labeling function来判断candidate为true或false</p>
<p>目标:<br>Apply labeling functions to all candidates to predict both positive and negative labels</p>
<p>Tip:<br>Labeling functions can be noisy，毕竟无法涵盖所有范围，所有语境。</p>
<p>设计策略:</p>
<ol>
<li><p>Pattern-based Labeling Functions<br> Common sense patterns or keywords<br> String matching via regular expressions and other heuristics</p>
</li>
<li><p>Distant Supervision Labeling Functions<br> Use an existing database of known facts to generate noisy labels</p>
</li>
</ol>
<p>LF评价指标:<br>Accuracy: percentage of candidates a labeling function labels correctly<br>Coverage: percentage of all candidates that are labeled by &gt;= 1 LFs<br>Conflict: percentage of candidates with &gt;1 labels that disagree</p>
<p>LF选择标准:<br>We want high-coverage, high-accuracy LFs<br>LFs need to label with probability better than random chance<br>Conflict is actually good — it allows our algorithm to learn information about the LF</p>
<h3 id="Build-Generative-Model-Unifying-Supervision"><a href="#Build-Generative-Model-Unifying-Supervision" class="headerlink" title="Build Generative Model: Unifying Supervision"></a>Build Generative Model: Unifying Supervision</h3><p>Simple Baseline: Majority Vote<br>Automatically Learning LF Accuracies<br>LF Dependency Learning</p>
<h3 id="Build-Discriminative-Model-“Compiling”-Rules-into-Features"><a href="#Build-Discriminative-Model-“Compiling”-Rules-into-Features" class="headerlink" title="Build Discriminative Model: “Compiling” Rules into Features"></a>Build Discriminative Model: “Compiling” Rules into Features</h3><p>Training with Probabilistic Labels<br>The Death of Manual Feature Engineering<br>Why Do We Need the Discriminative Model?</p>
]]></content>
      <tags>
        <tag>Knowledge Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱字符串匹配</title>
    <url>/2020/12/01/558str-match/</url>
    <content><![CDATA[<h2 id="字符串匹配问题"><a href="#字符串匹配问题" class="headerlink" title="字符串匹配问题"></a>字符串匹配问题</h2><h3 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement:"></a>Problem Statement:</h3><p>Given X and Y sets of strings, Find pairs (x, y) such that both x and y refer to the same real world entity. </p>
<p>Then use precision(fraction of pairs found that are correct) and recall(fraction of pairs found) to evaluate algorithms</p>
<h3 id="Why-can’t-match-perfectly"><a href="#Why-can’t-match-perfectly" class="headerlink" title="Why can’t match perfectly"></a>Why can’t match perfectly</h3><p>Typos    “Joh” vs. “Jhon”<br>OCR errors “J0hn” vs “John”<br>Formatting conventions ”03/17” vs “March 17”<br>Abbreviations “J. S. Sargent” vs “John Singer Sargent”<br>Nick name ”John” vs “Jock”<br>Word order “Sargent, John S.” vs “John S. Sargent”</p>
<p>因此，转而计算字符串的相似度</p>
<h2 id="Similiarity-Measure"><a href="#Similiarity-Measure" class="headerlink" title="Similiarity Measure"></a>Similiarity Measure</h2><p>Similarity(x, y) in [0, 1] is better than distance(x, y) in [0, ∞)</p>
<p>Types of Similarity Metrics:</p>
<ul>
<li>Sequence based</li>
<li>Set based</li>
<li>Hybrid</li>
<li>Phonetic</li>
</ul>
<h2 id="Sequence-Based-Metrics"><a href="#Sequence-Based-Metrics" class="headerlink" title="Sequence Based Metrics"></a>Sequence Based Metrics</h2><h3 id="Edit-Distance-Levenshtein-Distance"><a href="#Edit-Distance-Levenshtein-Distance" class="headerlink" title="Edit Distance/Levenshtein Distance"></a>Edit Distance/Levenshtein Distance</h3><p>Online calculator: <a href="http://planetcalc.com/1721/">http://planetcalc.com/1721/</a></p>
<p>lev(x, y) is the minimum cost to transform x to y(insert,delete,substitute)</p>
<p>原理： 动态规划 d(i,j)<br>case1: xi  = yj  =&gt; d(i,j) = d(i-1,j-1)<br>case2: xi != yj  =&gt; d(i,j) = min(  d(i-1,j) + 1 , d(i,j-1) + 1 , d(i-1,j-1) + 1)</p>
<p>缺陷:<br>Too high a cost for deleting a sequence of characters, so it is not suitable for abbreviation cases (example: lev(John Singer Sargent,John S. Sargent ) = 5, lev(John Singer Sargent,John Klinger Sargent ) = 5 )</p>
<h3 id="Needleman-Wunch-Measure"><a href="#Needleman-Wunch-Measure" class="headerlink" title="Needleman-Wunch Measure"></a>Needleman-Wunch Measure</h3><p>Generalization of levenstein(x, y), 为了解决lev带来的缩写问题的一种新的度量方式<br>得到的结果是两个string相似的评分，而不是距离</p>
<p><img src="/image/NW.png"></p>
<p>缺陷:<br>Longer gaps are penalized more, bad for names (example:nw(John Singer Sargent,John S.     Sargent) = 25, nw(John Stanislaus Sargent,John S.         Sargent)</p>
<h3 id="Affine-Gap-Measure"><a href="#Affine-Gap-Measure" class="headerlink" title="Affine Gap Measure"></a>Affine Gap Measure</h3><p>Generalization of needleman-wunch(x, y),为了解决Needleman-Wunch带来的长gap问题<br>Intuition: two small gaps are worse than one large one<br><img src="/image/Afm.png"></p>
<h3 id="Smith-Waterman"><a href="#Smith-Waterman" class="headerlink" title="Smith-Waterman"></a>Smith-Waterman</h3><p>可以说是Needleman-Wunch的local版本</p>
<p>Needleman-Wunch: Fully align sequences, opening gaps as needed,也就是说两边空格都被考虑进去了的<br><img src="/image/smith-waterman.png"></p>
<h3 id="Jaro-Similarity-Measure"><a href="#Jaro-Similarity-Measure" class="headerlink" title="Jaro Similarity Measure"></a>Jaro Similarity Measure</h3><p>Get points for having characters in common<br>• but only if they are “close by”<br>Get points for common characters in the same order<br>• lose points for transpositions<br><img src="/image/Jaro-sim.png"><br>？？？</p>
<h3 id="Jaro-Winkler-Measure"><a href="#Jaro-Winkler-Measure" class="headerlink" title="Jaro-Winkler Measure"></a>Jaro-Winkler Measure</h3><p>？？？？</p>
<h2 id="Set-Based-Metrics"><a href="#Set-Based-Metrics" class="headerlink" title="Set-Based Metrics"></a>Set-Based Metrics</h2><p>Generate set of tokens from the strings and then measure similarity between the sets of tokens</p>
<h3 id="Jaccard-Measure"><a href="#Jaccard-Measure" class="headerlink" title="Jaccard Measure"></a>Jaccard Measure</h3><h3 id="TF-IDF-Measure"><a href="#TF-IDF-Measure" class="headerlink" title="TF-IDF Measure"></a>TF-IDF Measure</h3><h2 id="Hybrid-Similarity-Measures"><a href="#Hybrid-Similarity-Measures" class="headerlink" title="Hybrid Similarity Measures"></a>Hybrid Similarity Measures</h2><p>Do the set-based thing but use a similiarity metric for each element of the set</p>
<h3 id="Generalized-Jaccard-Measure"><a href="#Generalized-Jaccard-Measure" class="headerlink" title="Generalized Jaccard Measure"></a>Generalized Jaccard Measure</h3><h3 id="The-Soft-TF-IDF-Measure"><a href="#The-Soft-TF-IDF-Measure" class="headerlink" title="The Soft TF/IDF Measure"></a>The Soft TF/IDF Measure</h3><h3 id="Monge-Elkan-Measure"><a href="#Monge-Elkan-Measure" class="headerlink" title="Monge-Elkan Measure"></a>Monge-Elkan Measure</h3><h2 id="Phonetic-Similarity"><a href="#Phonetic-Similarity" class="headerlink" title="Phonetic Similarity"></a>Phonetic Similarity</h2><h3 id="Soundex"><a href="#Soundex" class="headerlink" title="Soundex"></a>Soundex</h3>]]></content>
      <tags>
        <tag>Knowledge Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>Leetcode-二叉树</title>
    <url>/2020/09/17/Algorithm-binarytree/</url>
    <content><![CDATA[<h2 id="二叉树的一些思想"><a href="#二叉树的一些思想" class="headerlink" title="二叉树的一些思想"></a>二叉树的一些思想</h2><ul>
<li>前序遍历(preOrder)  root-&gt;left-&gt;right </li>
<li>中序遍历(inOrder)    left-&gt; root-&gt;right 二叉搜索树中，其遍历结果为有序数组</li>
<li>后序遍历(postOrder) left-&gt;right-&gt;root </li>
<li>广度优先搜索(BFS)</li>
<li>深度优先搜索(DFS)</li>
<li>递归(Recursion)，根据要求选择使用前序，中序，后序的递归框架来解决问题</li>
</ul>
<h4 id="897-递增顺序查找树"><a href="#897-递增顺序查找树" class="headerlink" title="897. 递增顺序查找树"></a><a href="https://leetcode-cn.com/problems/increasing-order-search-tree/">897. 递增顺序查找树</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 思路1: 中序遍历形成有序列表 + 重新生成树</span><br><span class="hljs-comment"># 思路2: 使用递归,改变树形状，不生成新的树</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">increasingBST</span>(<span class="hljs-params">self, root: TreeNode</span>) -&gt; TreeNode:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dfs</span>(<span class="hljs-params">root</span>):</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root: <span class="hljs-keyword">return</span> <br>        dfs(root.left)<br>        <span class="hljs-comment">### 对根节点的操作 ###</span><br>        root.left = <span class="hljs-literal">None</span><br>        self.cur.right = root<br>        self.cur = root<br>        <span class="hljs-comment">###################</span><br>        dfs(root.right)<br>    res = self.cur = TreeNode(<span class="hljs-literal">None</span>)<br>    dfs(root)<br>    <span class="hljs-keyword">return</span> res.right<br></code></pre></td></tr></table></figure>

<h4 id="226-翻转二叉树"><a href="#226-翻转二叉树" class="headerlink" title="226. 翻转二叉树"></a><a href="https://leetcode-cn.com/problems/invert-binary-tree/">226. 翻转二叉树</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">invertTree</span>(<span class="hljs-params">self, root</span>):</span><br>    <span class="hljs-comment"># 先序遍历  （后序遍历也可）</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root: <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br>    <span class="hljs-comment">### 对根节点的操作 ###</span><br>    root.left,root.right = root.right,root.left<br>    <span class="hljs-comment">### 对根节点的操作 ###</span><br>    root.left = self.invertTree(root.left)  <br>    root.right = self.invertTree(root.right)<br>    <span class="hljs-comment"># 函数返回时就表示当前这个节点，以及它的左右子树都已经交换完了       </span><br>    <span class="hljs-keyword">return</span> root<br></code></pre></td></tr></table></figure>

<h4 id="617-合并二叉树"><a href="#617-合并二叉树" class="headerlink" title="617. 合并二叉树"></a><a href="https://leetcode-cn.com/problems/merge-two-binary-trees/">617. 合并二叉树</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mergeTrees</span>(<span class="hljs-params">self, t1: TreeNode, t2: TreeNode</span>) -&gt; TreeNode:</span><br>    <span class="hljs-comment">###### 对根节点的操作 #####</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> (t1 <span class="hljs-keyword">and</span> t2): <span class="hljs-comment"># 不能用 if not t1 and not t2</span><br>        <span class="hljs-keyword">return</span> t1 <span class="hljs-keyword">if</span> t1 <span class="hljs-keyword">else</span> t2<br>    t1.val+=t2.val<br>    <span class="hljs-comment">#########################</span><br>    t1.left = self.mergeTrees(t1.left,t2.left) <br>    t1.right = self.mergeTrees(t1.right,t2.right)<br><br>    <span class="hljs-keyword">return</span> t1<br></code></pre></td></tr></table></figure>

<h4 id="938-二叉搜索树的范围和"><a href="#938-二叉搜索树的范围和" class="headerlink" title="938. 二叉搜索树的范围和"></a><a href="https://leetcode-cn.com/problems/range-sum-of-bst/">938. 二叉搜索树的范围和</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">rangeSumBST</span>(<span class="hljs-params">self, root: TreeNode, L: int, R: int</span>) -&gt; int:</span><br>    <br>    self.res = <span class="hljs-number">0</span> <span class="hljs-comment"># 需要使用一个全局变量进行结果的保存</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dfs</span>(<span class="hljs-params">root,L,R</span>):</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root: <span class="hljs-keyword">return</span> <br>        <span class="hljs-comment">###### 对根节点的操作 #####</span><br>        <span class="hljs-keyword">if</span> L&lt;= root.val &lt;=R:<br>            self.res+=root.val<br>        <span class="hljs-comment">#########################</span><br>        dfs(root.left,L,R)<br>        dfs(root.right,L,R)<br><br>    dfs(root,L,R)<br>    <span class="hljs-keyword">return</span> self.res<br><br></code></pre></td></tr></table></figure>

<h4 id="104-二叉树的最大深度"><a href="#104-二叉树的最大深度" class="headerlink" title="104. 二叉树的最大深度"></a><a href="https://leetcode-cn.com/problems/maximum-depth-of-binary-tree/">104. 二叉树的最大深度</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">maxDepth</span>(<span class="hljs-params">self, root: TreeNode</span>) -&gt; int:</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root:<span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>    <span class="hljs-comment"># 后序遍历</span><br>    <span class="hljs-keyword">return</span> max(self.maxDepth(root.left),self.maxDepth(root.right)) +<span class="hljs-number">1</span><br>    <span class="hljs-comment"># 也可以使用BFS得到高度</span><br></code></pre></td></tr></table></figure>

<h4 id="590-N叉树的后序遍历-前序遍历"><a href="#590-N叉树的后序遍历-前序遍历" class="headerlink" title="590. N叉树的后序遍历/前序遍历"></a><a href="https://leetcode-cn.com/problems/n-ary-tree-postorder-traversal/">590. N叉树的后序遍历</a>/<a href="https://leetcode-cn.com/problems/n-ary-tree-preorder-traversal/">前序遍历</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">postorder</span>(<span class="hljs-params">self, root: <span class="hljs-string">&#x27;Node&#x27;</span></span>) -&gt; List[int]:</span><br>    self.res = []<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">helper</span>(<span class="hljs-params">root</span>):</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root:<span class="hljs-keyword">return</span><br>        <span class="hljs-keyword">for</span> child <span class="hljs-keyword">in</span> root.children:<br>            helper(child)<br>        <span class="hljs-comment">###### 对根节点的操作 #####</span><br>        self.res.append(root.val)<br>        <span class="hljs-comment">###### 对根节点的操作 #####</span><br>    helper(root)<br>    <span class="hljs-keyword">return</span> self.res<br></code></pre></td></tr></table></figure>

<h4 id="700-二叉搜索树中的搜索"><a href="#700-二叉搜索树中的搜索" class="headerlink" title="700. 二叉搜索树中的搜索"></a><a href="https://leetcode-cn.com/problems/search-in-a-binary-search-tree/">700. 二叉搜索树中的搜索</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">searchBST</span>(<span class="hljs-params">self, root: TreeNode, val: int</span>) -&gt; TreeNode:</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root:<span class="hljs-keyword">return</span> <br>    <span class="hljs-comment">###### 对根节点的操作 #####</span><br>    <span class="hljs-keyword">if</span> root.val == val:<br>        <span class="hljs-keyword">return</span> root<br>    <span class="hljs-comment">#########################</span><br>    <span class="hljs-keyword">if</span> root.val &lt; val:<br>        <span class="hljs-keyword">return</span> self.searchBST(root.right,val) <span class="hljs-comment"># 记住需要return</span><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> self.searchBST(root.left,val)<br></code></pre></td></tr></table></figure>

<h4 id="108-将有序数组转换为二叉搜索树"><a href="#108-将有序数组转换为二叉搜索树" class="headerlink" title="108. 将有序数组转换为二叉搜索树"></a><a href="https://leetcode-cn.com/problems/convert-sorted-array-to-binary-search-tree/">108. 将有序数组转换为二叉搜索树</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sortedArrayToBST</span>(<span class="hljs-params">self, nums: List[int]</span>) -&gt; TreeNode:</span><br>    <span class="hljs-comment">###### 对根节点的操作 #####</span><br>    <span class="hljs-keyword">if</span> len(nums) == <span class="hljs-number">0</span> : <span class="hljs-keyword">return</span> <br>    mid = len(nums)//<span class="hljs-number">2</span><br>    root = TreeNode(nums[mid])<br>    <span class="hljs-comment">##########################</span><br>    root.left = self.sortedArrayToBST(nums[:mid])<br>    root.right = self.sortedArrayToBST(nums[mid+<span class="hljs-number">1</span>:])<br>    <br>    <span class="hljs-keyword">return</span> root<br></code></pre></td></tr></table></figure>

<h4 id="897-递增顺序查找树-1"><a href="#897-递增顺序查找树-1" class="headerlink" title="897. 递增顺序查找树"></a><a href="https://leetcode-cn.com/problems/increasing-order-search-tree/">897. 递增顺序查找树</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">increasingBST</span>(<span class="hljs-params">self, root: TreeNode</span>) -&gt; TreeNode:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dfs</span>(<span class="hljs-params">root</span>):</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root: <span class="hljs-keyword">return</span> <br>        dfs(root.left)<br>        <span class="hljs-comment">### 对根节点的操作 #####</span><br>        root.left = <span class="hljs-literal">None</span><br>        self.cur.right = root<br>        self.cur = root<br>        <span class="hljs-comment">#####################</span><br>        dfs(root.right)<br>    res = self.cur = TreeNode(<span class="hljs-literal">None</span>)<br>    dfs(root)<br>    <span class="hljs-keyword">return</span> res.right<br></code></pre></td></tr></table></figure>

<h4 id="559-N叉树的最大深度"><a href="#559-N叉树的最大深度" class="headerlink" title="559. N叉树的最大深度"></a><a href="https://leetcode-cn.com/problems/maximum-depth-of-n-ary-tree/">559. N叉树的最大深度</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">maxDepth</span>(<span class="hljs-params">self, root: <span class="hljs-string">&#x27;Node&#x27;</span></span>) -&gt; int:</span><br>    <br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root:<span class="hljs-keyword">return</span>  <span class="hljs-number">0</span>   <br>    <span class="hljs-keyword">return</span> max([self.maxDepth(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span>  root.children]+[<span class="hljs-number">0</span>])+<span class="hljs-number">1</span> <span class="hljs-comment"># 注意需要 +[0] =&gt; 没有0的话会报错: max() arg is an empty sequence</span><br></code></pre></td></tr></table></figure>

<h4 id="剑指-Offer-68-II-二叉树的最近公共祖先"><a href="#剑指-Offer-68-II-二叉树的最近公共祖先" class="headerlink" title="剑指 Offer 68 - II. 二叉树的最近公共祖先"></a><a href="https://leetcode-cn.com/problems/er-cha-shu-de-zui-jin-gong-gong-zu-xian-lcof/">剑指 Offer 68 - II. 二叉树的最近公共祖先</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">lowestCommonAncestor</span>(<span class="hljs-params">self, root: TreeNode, p: TreeNode, q: TreeNode</span>) -&gt; TreeNode:</span><br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root <span class="hljs-keyword">or</span> p == root <span class="hljs-keyword">or</span> q == root: <span class="hljs-keyword">return</span> root<br><br>    left = self.lowestCommonAncestor(root.left,p,q)<br>    right = self.lowestCommonAncestor(root.right,p,q)<br>    <span class="hljs-comment">### 对根节点的操作 #####</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> left <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> right: <span class="hljs-keyword">return</span>  <span class="hljs-comment"># 左右子树均为空</span><br>    <span class="hljs-keyword">elif</span> <span class="hljs-keyword">not</span> left: <span class="hljs-keyword">return</span> right        <span class="hljs-comment"># 右空，左不空</span><br>    <span class="hljs-keyword">elif</span> <span class="hljs-keyword">not</span> right:<span class="hljs-keyword">return</span> left         <span class="hljs-comment"># 左空，右不空</span><br>    <span class="hljs-keyword">else</span>: <span class="hljs-keyword">return</span> root                  <span class="hljs-comment"># 左右均不空</span><br>    <span class="hljs-comment">### 对根节点的操作 #####</span><br></code></pre></td></tr></table></figure>

<h4 id="637-二叉树的层平均值"><a href="#637-二叉树的层平均值" class="headerlink" title="637. 二叉树的层平均值"></a><a href="https://leetcode-cn.com/problems/average-of-levels-in-binary-tree/">637. 二叉树的层平均值</a></h4>]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Leetcode-剑指Offer</title>
    <url>/2020/09/05/Algorithm-jianzhioffer/</url>
    <content><![CDATA[<h4 id="左旋转字符串"><a href="#左旋转字符串" class="headerlink" title="左旋转字符串"></a><a href="https://leetcode-cn.com/problems/zuo-xuan-zhuan-zi-fu-chuan-lcof/">左旋转字符串</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reverseLeftWords</span>(<span class="hljs-params">s,n</span>):</span><br>    <span class="hljs-keyword">return</span> s[n:] + s[:n]<br></code></pre></td></tr></table></figure>

<h4 id="求1-2-…-n"><a href="#求1-2-…-n" class="headerlink" title="求1+2+…+n"></a><a href="https://leetcode-cn.com/problems/qiu-12n-lcof/">求1+2+…+n</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python">res = <span class="hljs-number">0</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sumNums</span>(<span class="hljs-params">n</span>):</span><br>    <span class="hljs-keyword">global</span> res<br>    n &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> sumNums(n<span class="hljs-number">-1</span>) <span class="hljs-comment"># n=1 终止递归的需求，可通过短路效应实现</span><br>    res += n <br>    <span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure>

<h4 id="数组中重复的数字"><a href="#数组中重复的数字" class="headerlink" title="数组中重复的数字"></a><a href="https://leetcode-cn.com/problems/shu-zu-zhong-zhong-fu-de-shu-zi-lcof/">数组中重复的数字</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">findRepeatNumber</span>(<span class="hljs-params">nums</span>):</span><br>    fre_dict=&#123;&#125; <span class="hljs-comment"># 使用字典</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> nums:<br>        fre_dict[i] = fre_dict.get(i,<span class="hljs-number">0</span>)+<span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> fre_dict[i]&gt;<span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">return</span> i<br>    <span class="hljs-comment"># 另一种思路 ，排序 找nums[i] == nums[i-1]</span><br></code></pre></td></tr></table></figure>

<h4 id="数组中数字出现的次数"><a href="#数组中数字出现的次数" class="headerlink" title="数组中数字出现的次数"></a><a href="https://leetcode-cn.com/problems/shu-zu-zhong-shu-zi-chu-xian-de-ci-shu-lcof/">数组中数字出现的次数</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">singleNumbers</span>(<span class="hljs-params">nums</span>):</span> <br>    tmp_res = <span class="hljs-number">0</span>     <br>    a,b = <span class="hljs-number">0</span>,<span class="hljs-number">0</span> <br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> nums:<br>        tmp_res = tmp_res ^ i  <span class="hljs-comment"># 假设需要的结果为为a和b 那么tmp_res = a^b  note: a^a = 0 </span><br>    <span class="hljs-comment"># 将数组分成两部分 一部分只包含a  另一部分只包含b</span><br>    pivot = <span class="hljs-number">1</span><br>    <span class="hljs-keyword">while</span> tmp_res&amp;pivot ==<span class="hljs-number">0</span>:<br>        pivot = pivot&lt;&lt;<span class="hljs-number">1</span><br>    <br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> nums:<br>        <span class="hljs-keyword">if</span> i &amp;pivot:<br>            a = a^i<br>        <span class="hljs-keyword">else</span>:<br>            b = b^i<br>    <span class="hljs-keyword">return</span> [a,b]<br></code></pre></td></tr></table></figure>

<h4 id="数组中数字出现的次数-II"><a href="#数组中数字出现的次数-II" class="headerlink" title="数组中数字出现的次数 II"></a><a href="https://leetcode-cn.com/problems/shu-zu-zhong-shu-zi-chu-xian-de-ci-shu-ii-lcof/">数组中数字出现的次数 II</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">singleNumber</span>(<span class="hljs-params">nums</span>):</span><br>  <span class="hljs-comment"># 1.数学 (sum(set(nums))*3-sum(nums))//2  2.字典思路 3. 位运算</span><br>  <span class="hljs-comment"># a^a = 0  a^0 = a 位运算</span><br>    res = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">32</span>): <span class="hljs-comment"># 1 &lt;= nums[i] &lt; 2^31限制</span><br>        idx = <span class="hljs-number">1</span> &lt;&lt; i  <span class="hljs-comment"># 0001   0010   0010</span><br>        count = <span class="hljs-number">0</span> <br>        <span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> nums:<br>            <span class="hljs-keyword">if</span> num &amp; idx !=<span class="hljs-number">0</span>:<br>                count+=<span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> count%<span class="hljs-number">3</span> ==<span class="hljs-number">1</span>: <span class="hljs-comment"># 表明唯一的那个数在这一位上为1</span><br>            res = res|idx<br>    <span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure>

<h4 id="二叉树的深度"><a href="#二叉树的深度" class="headerlink" title="二叉树的深度"></a><a href="https://leetcode-cn.com/problems/er-cha-shu-de-shen-du-lcof/">二叉树的深度</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">maxDepth</span>(<span class="hljs-params">root</span>):</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root:<span class="hljs-keyword">return</span> <span class="hljs-number">0</span> <span class="hljs-comment"># 递归出口</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> + max(self.maxDepth(root.left),self.maxDepth(root.right))<br><span class="hljs-comment"># 非递归方法： BFS 获得层数即二叉树的深度</span><br></code></pre></td></tr></table></figure>

<h4 id="链表中倒数第k个节点"><a href="#链表中倒数第k个节点" class="headerlink" title="链表中倒数第k个节点"></a><a href="https://leetcode-cn.com/problems/lian-biao-zhong-dao-shu-di-kge-jie-dian-lcof/">链表中倒数第k个节点</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getKthFromEnd</span>(<span class="hljs-params">head, k</span>):</span><br>    <span class="hljs-comment"># 思路1: 遍历统计链表长度，倒是第k个节点便是顺数n-k个节点</span><br>    <span class="hljs-comment"># 快慢指针思路</span><br>    slow,fast = head,head<br>    index = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(k): <span class="hljs-comment"># 快指针先走k步</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> fast: <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br>        fast = fast.next<br>    <span class="hljs-keyword">while</span> fast:<br>        slow = slow.next<br>        fast = fast.next<br>    <span class="hljs-keyword">return</span> slow<br></code></pre></td></tr></table></figure>

<h4 id="二叉树的镜像"><a href="#二叉树的镜像" class="headerlink" title="二叉树的镜像"></a><a href="https://leetcode-cn.com/problems/er-cha-shu-de-jing-xiang-lcof/">二叉树的镜像</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mirrorTree</span>(<span class="hljs-params">root</span>):</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root: <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span> <span class="hljs-comment"># 递归出口</span><br>    root.left, root.right = self.mirrorTree(root.right),self.mirrorTree(root.left)<br>    <span class="hljs-keyword">return</span> root<br><span class="hljs-comment"># 非递归方法： BFS 对每一层进行迭代 </span><br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> deque <br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mirrorTree</span>(<span class="hljs-params">root</span>):</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root: <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span> <br>    queue = deque()<br>    queue.append(root)<br>    <span class="hljs-keyword">while</span> queue: <span class="hljs-comment"># 对每一层的每一个节点进行遍历</span><br>        node = queue.popleft()<br>        tmp_left, tmp_right =node.left, node.right<br>        <span class="hljs-keyword">if</span> tmp_left:<br>            queue.append(tmp_left)<br>            node.right = tmp_left<br>        <span class="hljs-keyword">else</span>:<br>            node.right = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> tmp_right:<br>            queue.append(tmp_right)<br>            node.left = tmp_right  <br>        <span class="hljs-keyword">else</span>:<br>            node.left = <span class="hljs-literal">None</span>              <br>    <span class="hljs-keyword">return</span> root<br></code></pre></td></tr></table></figure>

<h4 id="打印从1到最大的n位数"><a href="#打印从1到最大的n位数" class="headerlink" title="打印从1到最大的n位数"></a><a href="https://leetcode-cn.com/problems/da-yin-cong-1dao-zui-da-de-nwei-shu-lcof/">打印从1到最大的n位数</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">printNumbers</span>(<span class="hljs-params">n</span>):</span><br>    <span class="hljs-keyword">return</span> [ i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,<span class="hljs-number">10</span>**n) ]<br></code></pre></td></tr></table></figure>

<h4 id="替换空格"><a href="#替换空格" class="headerlink" title="替换空格"></a><a href="https://leetcode-cn.com/problems/ti-huan-kong-ge-lcof/">替换空格</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">replaceSpace</span>(<span class="hljs-params">self, s</span>):</span><br>    <span class="hljs-keyword">return</span> s.replace(<span class="hljs-string">&#x27; &#x27;</span>,<span class="hljs-string">&#x27;%20&#x27;</span>)<br></code></pre></td></tr></table></figure>

<h4 id="从尾到头打印链表"><a href="#从尾到头打印链表" class="headerlink" title="从尾到头打印链表"></a><a href="https://leetcode-cn.com/problems/cong-wei-dao-tou-da-yin-lian-biao-lcof/">从尾到头打印链表</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reversePrint</span>(<span class="hljs-params">head</span>):</span><br>    res = []<br>    <span class="hljs-keyword">while</span> head:<br>        res.append(head.val)<br>        head = head.next<br>    <span class="hljs-keyword">return</span> res[::<span class="hljs-number">-1</span>]    <br><span class="hljs-comment"># 递归解法 return reversePrint(head.next) + [head.val] if head else []</span><br></code></pre></td></tr></table></figure>

<h4 id="反转链表"><a href="#反转链表" class="headerlink" title="反转链表"></a><a href="https://leetcode-cn.com/problems/fan-zhuan-lian-biao-lcof/">反转链表</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reverseList</span>(<span class="hljs-params">head</span>):</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> head: <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br>    cur,pre = head,<span class="hljs-literal">None</span><br>    <span class="hljs-keyword">while</span> cur:<br>        tmp = cur.next<br>        cur.next = pre<br>        pre = cur<br>        cur = tmp <br>    <span class="hljs-keyword">return</span> pre <br></code></pre></td></tr></table></figure>

<h4 id="二叉搜索树的第k大节点"><a href="#二叉搜索树的第k大节点" class="headerlink" title="二叉搜索树的第k大节点"></a><a href="https://leetcode-cn.com/problems/er-cha-sou-suo-shu-de-di-kda-jie-dian-lcof/">二叉搜索树的第k大节点</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">kthLargest</span>(<span class="hljs-params">root, k</span>):</span><br>    <span class="hljs-comment"># 中序遍历 获得有序数组后返回第k个大的数字。 中序遍历： 左子树 + root + 右子树</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">traverse</span>(<span class="hljs-params">root</span>):</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root: <span class="hljs-keyword">return</span> <br>        res = []<br>        <span class="hljs-keyword">if</span> root.left:<br>            res+=(traverse(root.left)) <span class="hljs-comment"># 注意不能使用append  否则会形成一个嵌套列表 </span><br>        res.append(root.val)<br>        <span class="hljs-keyword">if</span> root.right:<br>            res+=(traverse(root.right))<br>        <span class="hljs-keyword">return</span> res<br>    res = traverse(root)<br>    <span class="hljs-keyword">return</span> res[-k]<br></code></pre></td></tr></table></figure>

<h4 id="合并两个排序的链表"><a href="#合并两个排序的链表" class="headerlink" title="合并两个排序的链表####"></a><a href="https://leetcode-cn.com/problems/he-bing-liang-ge-pai-xu-de-lian-biao-lcof/">合并两个排序的链表</a>####</h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mergeTwoLists</span>(<span class="hljs-params">l1,l2</span>):</span><br>    <span class="hljs-comment"># 思路1: 新建一个链表，依次插入合适的节点</span><br>    cur = dummy = ListNode(<span class="hljs-number">0</span>) <br>    <span class="hljs-keyword">while</span> l1 <span class="hljs-keyword">and</span> l2:<br>        <span class="hljs-keyword">if</span> l1.val &lt; l2.val:<br>            cur.next, l1 = l1, l1.next<br>        <span class="hljs-keyword">else</span>:<br>            cur.next, l2 = l2, l2.next<br>        cur = cur.next<br>    cur.next = l1 <span class="hljs-keyword">if</span> l1 <span class="hljs-keyword">else</span> l2<br>    <span class="hljs-keyword">return</span> dummy.next<br><br>    <span class="hljs-comment"># 思路2: 使用递归进行</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> l1:<span class="hljs-keyword">return</span> l2<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> l2:<span class="hljs-keyword">return</span> l1<br>    <span class="hljs-keyword">if</span> l1.val &lt;= l2.val:<br>        l1.next = self.mergeTwoLists(l1.next,l2)<br>        <span class="hljs-keyword">return</span> l1<br>    <span class="hljs-keyword">else</span>:<br>        l2.next = self.mergeTwoLists(l1,l2.next)<br>        <span class="hljs-keyword">return</span> l2<br></code></pre></td></tr></table></figure>

<h4 id="二进制中1的个数"><a href="#二进制中1的个数" class="headerlink" title="二进制中1的个数"></a><a href="https://leetcode-cn.com/problems/er-jin-zhi-zhong-1de-ge-shu-lcof/">二进制中1的个数</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hammingWeight</span>(<span class="hljs-params">n</span>):</span><br>    <span class="hljs-comment"># 位运算 n&amp;(n−1)  二进制数字n最右边的 1 变成 0 ，其余不变。</span><br>    res = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">while</span> n:<br>        res += <span class="hljs-number">1</span><br>        n &amp;= n - <span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> res<br>    <span class="hljs-comment"># n&amp;1 =1 =&gt; n 二进制 最右一位 为 1 ；n&amp;1 = 0 =&gt; n 二进制 最右一位 为 0</span><br>    res = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">while</span> n:<br>        res += n&amp;<span class="hljs-number">1</span><br>        n = n&gt;&gt;<span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure>

<h4 id="用两个栈实现队列"><a href="#用两个栈实现队列" class="headerlink" title="用两个栈实现队列"></a><a href="https://leetcode-cn.com/problems/yong-liang-ge-zhan-shi-xian-dui-lie-lcof/">用两个栈实现队列</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CQueue</span>:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>       self.A,self.B = [],[]<br>       <span class="hljs-comment"># A 负责入队  B负责出队</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">appendTail</span>(<span class="hljs-params">self, value</span>):</span><br>        self.A.append(value)<br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">deleteHead</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">if</span> self.B: <span class="hljs-keyword">return</span> self.B.pop()<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.A: <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span><br>        <span class="hljs-keyword">while</span> self.A:<br>            self.B.append(self.A.pop())<br>        <span class="hljs-keyword">return</span> self.B.pop()<br></code></pre></td></tr></table></figure>

<h4 id="复杂链表的复制-待定"><a href="#复杂链表的复制-待定" class="headerlink" title="复杂链表的复制 ##待定"></a><a href="https://leetcode-cn.com/problems/fu-za-lian-biao-de-fu-zhi-lcof/">复杂链表的复制</a> ##待定</h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">copyRandomList</span>(<span class="hljs-params">head</span>):</span><br><br></code></pre></td></tr></table></figure>

<h4 id="二叉树的最近公共祖先-待定"><a href="#二叉树的最近公共祖先-待定" class="headerlink" title="二叉树的最近公共祖先##待定"></a><a href="https://leetcode-cn.com/problems/er-cha-shu-de-zui-jin-gong-gong-zu-xian-lcof/">二叉树的最近公共祖先</a>##待定</h4><h4 id="和为s的连续正数序列"><a href="#和为s的连续正数序列" class="headerlink" title="和为s的连续正数序列"></a><a href="https://leetcode-cn.com/problems/he-wei-sde-lian-xu-zheng-shu-xu-lie-lcof/">和为s的连续正数序列</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">findContinuousSequence</span>(<span class="hljs-params">target</span>):</span><br>    <span class="hljs-comment"># 滑动窗口解决</span><br>    nums = list(range(<span class="hljs-number">1</span>,(target//<span class="hljs-number">2</span>+<span class="hljs-number">2</span>))) <span class="hljs-comment"># 候选的最大值不会超高target/2 +1</span><br>    res = []<br>    start_window,total = <span class="hljs-number">0</span>,nums[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">for</span> end_window <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,len(nums)):<br>        total += nums[end_window]<br>        <span class="hljs-keyword">while</span> total&gt;=target:<br>            <span class="hljs-keyword">if</span> total == target:  <span class="hljs-comment"># 窗口和 = target =&gt;当前窗口加入结果 窗口起点右移</span><br>                res.append(nums[start_window:end_window+<span class="hljs-number">1</span>])<br>            total -= nums[start_window]<br>            start_window+=<span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure>

<h4 id="重建二叉树"><a href="#重建二叉树" class="headerlink" title="重建二叉树"></a><a href="https://leetcode-cn.com/problems/zhong-jian-er-cha-shu-lcof/">重建二叉树</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">buildTree</span>(<span class="hljs-params">preorder, inorder</span>):</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> preorder:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span> <br>    root = preorder[<span class="hljs-number">0</span>]<br>    pos = inorder.index(root)<br><br>    binary_tree = TreeNode(root)<br>    binary_tree.left = self.buildTree(preorder[<span class="hljs-number">1</span>:pos+<span class="hljs-number">1</span>],inorder[:pos])<br>    binary_tree.right = self.buildTree(preorder[pos+<span class="hljs-number">1</span>:],inorder[pos+<span class="hljs-number">1</span>:])<br>    <span class="hljs-keyword">return</span> binary_tree<br><br></code></pre></td></tr></table></figure>

<h4 id="从上到下打印二叉树-I"><a href="#从上到下打印二叉树-I" class="headerlink" title="从上到下打印二叉树 I"></a><a href="https://leetcode-cn.com/problems/cong-shang-dao-xia-da-yin-er-cha-shu-lcof/">从上到下打印二叉树 I</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">levelOrder</span>(<span class="hljs-params">root</span>):</span><br>    <span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> deque<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root:<span class="hljs-keyword">return</span> []<br>    queue = deque()<br>    queue.append(root)<br>    res = []<br>    <span class="hljs-keyword">while</span> queue:<br>        level_size = len(queue)<br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(level_size):<br>            node = queue.popleft()<br>            <span class="hljs-keyword">if</span> node.left:<br>                queue.append(node.left)<br>            <span class="hljs-keyword">if</span> node.right:<br>                queue.append(node.right)<br>            res.append(node.val)<br>    <span class="hljs-keyword">return</span> res <br></code></pre></td></tr></table></figure>


<h4 id="从上到下打印二叉树-II"><a href="#从上到下打印二叉树-II" class="headerlink" title="从上到下打印二叉树 II"></a><a href="https://leetcode-cn.com/problems/cong-shang-dao-xia-da-yin-er-cha-shu-ii-lcof/">从上到下打印二叉树 II</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">levelOrder</span>(<span class="hljs-params">root</span>):</span><br>    <span class="hljs-comment"># BFS 层序遍历 使用队列实现</span><br>    <span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> deque<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root: <span class="hljs-keyword">return</span> []<br>    queue = deque()<br>    queue.append(root)<br>    res = []<br>    <span class="hljs-keyword">while</span> queue:<br>        level_size = len(queue)<br>        cur_level = []<br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(level_size):<br>            node = queue.popleft()<br>            cur_level.append(node.val)<br>            <span class="hljs-keyword">if</span> node.left:<br>                queue.append(node.left)<br>            <span class="hljs-keyword">if</span> node.right:<br>                queue.append(node.right)<br>        res.append(cur_level)<br>    <span class="hljs-keyword">return</span> res <br></code></pre></td></tr></table></figure>

<h4 id="礼物的最大价值"><a href="#礼物的最大价值" class="headerlink" title="礼物的最大价值"></a><a href="https://leetcode-cn.com/problems/li-wu-de-zui-da-jie-zhi-lcof/">礼物的最大价值</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">maxValue</span>(<span class="hljs-params">grid</span>):</span> <br>    <span class="hljs-comment"># 二维dp  grid[i][j] = max(grid[i][j - 1], grid[i - 1][j]) +  grid[i][j]</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(grid)):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(len(grid[<span class="hljs-number">0</span>])):<br>            <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> j == <span class="hljs-number">0</span>: <span class="hljs-keyword">continue</span><br>            <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span>: grid[i][j] += grid[i][j - <span class="hljs-number">1</span>]<br>            <span class="hljs-keyword">elif</span> j == <span class="hljs-number">0</span>: grid[i][j] += grid[i - <span class="hljs-number">1</span>][j]<br>            <span class="hljs-keyword">else</span>: grid[i][j] += max(grid[i][j - <span class="hljs-number">1</span>], grid[i - <span class="hljs-number">1</span>][j])<br>    <span class="hljs-keyword">return</span> grid[<span class="hljs-number">-1</span>][<span class="hljs-number">-1</span>]<br></code></pre></td></tr></table></figure>

<h4 id="数组中出现次数超过一半的数字"><a href="#数组中出现次数超过一半的数字" class="headerlink" title="数组中出现次数超过一半的数字"></a><a href="https://leetcode-cn.com/problems/shu-zu-zhong-chu-xian-ci-shu-chao-guo-yi-ban-de-shu-zi-lcof/">数组中出现次数超过一半的数字</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">majorityElement</span>(<span class="hljs-params">nums</span>):</span><br>    <span class="hljs-keyword">return</span> sorted(nums)[len(nums)//<span class="hljs-number">2</span>]  <span class="hljs-comment">#  排序后取中间的数</span><br><br>    <span class="hljs-comment">#Boyer-Moore 投票法 O(n)</span><br>    res = nums[<span class="hljs-number">0</span>]<br>    cal = <span class="hljs-number">1</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> nums[<span class="hljs-number">1</span>:]:<br>        <span class="hljs-keyword">if</span> cal ==<span class="hljs-number">0</span>: <br>            res = i<br>        <span class="hljs-keyword">if</span> i == res:<br>            cal+=<span class="hljs-number">1</span><br>        <span class="hljs-keyword">else</span>:<br>            cal-=<span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure>

<h4 id="和为s的两个数字"><a href="#和为s的两个数字" class="headerlink" title="和为s的两个数字"></a><a href="https://leetcode-cn.com/problems/he-wei-sde-liang-ge-shu-zi-lcof/">和为s的两个数字</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">twoSum</span>(<span class="hljs-params">nums,target</span>):</span>  <br>    left, right = <span class="hljs-number">0</span>,len(nums)<span class="hljs-number">-1</span> <span class="hljs-comment"># 使用双指针 空间复杂度O(1)</span><br>    <span class="hljs-keyword">while</span> left&lt;right:<br>        sums = nums[left]+nums[right]<br>        <span class="hljs-keyword">if</span> sums&lt;target:<br>            left+=<span class="hljs-number">1</span><br>        <span class="hljs-keyword">elif</span> sums&gt;target:<br>            right-=<span class="hljs-number">1</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> [nums[left],nums[right]]<br>    <span class="hljs-keyword">return</span> []<br></code></pre></td></tr></table></figure>

<h4 id="丑数"><a href="#丑数" class="headerlink" title="丑数"></a><a href="https://leetcode-cn.com/problems/chou-shu-lcof/">丑数</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">nthUglyNumber</span>(<span class="hljs-params">n</span>):</span><br>    <span class="hljs-comment"># 带条件的动态规划</span><br>    dp, a, b, c = [<span class="hljs-number">1</span>] * n, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, n):<br>        n2, n3, n5 = dp[a] * <span class="hljs-number">2</span>, dp[b] * <span class="hljs-number">3</span>, dp[c] * <span class="hljs-number">5</span><br>        dp[i] = min(n2, n3, n5)<br>        <span class="hljs-keyword">if</span> dp[i] == n2: a += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> dp[i] == n3: b += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> dp[i] == n5: c += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> dp[<span class="hljs-number">-1</span>]<br></code></pre></td></tr></table></figure>

<h4 id="调整数组顺序使奇数位于偶数前面"><a href="#调整数组顺序使奇数位于偶数前面" class="headerlink" title="调整数组顺序使奇数位于偶数前面"></a><a href="https://leetcode-cn.com/problems/diao-zheng-shu-zu-shun-xu-shi-qi-shu-wei-yu-ou-shu-qian-mian-lcof/">调整数组顺序使奇数位于偶数前面</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">exchange</span>(<span class="hljs-params">self, nums: List[int]</span>) -&gt; List[int]:</span><br>    left,right = <span class="hljs-number">0</span>,len(nums)<span class="hljs-number">-1</span><br>    <span class="hljs-keyword">while</span> left&lt;right:<br>    <span class="hljs-comment"># 保证nums[left]奇数,nums[right]为偶数</span><br>        <span class="hljs-keyword">if</span> nums[left]%<span class="hljs-number">2</span>==<span class="hljs-number">1</span>:  <br>            left+=<span class="hljs-number">1</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> nums[right]%<span class="hljs-number">2</span> ==<span class="hljs-number">1</span>:<br>                nums[left],nums[right]=nums[right],nums[left]<br>                left+=<span class="hljs-number">1</span><br>                right-=<span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>:<br>                right-=<span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> nums<br></code></pre></td></tr></table></figure>

<h4 id="股票的最大利润"><a href="#股票的最大利润" class="headerlink" title="股票的最大利润"></a><a href="https://leetcode-cn.com/problems/gu-piao-de-zui-da-li-run-lcof/">股票的最大利润</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">maxProfit</span>(<span class="hljs-params">prices</span>):</span><br>    <span class="hljs-comment"># opt[i] = max(opt[i-1],prices[i]-min(prices[:i])) 动态规划转移方程</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> prices: <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>    min_price = prices[<span class="hljs-number">0</span>] <span class="hljs-comment"># 存储当天前最低的股票</span><br>    opt = [<span class="hljs-number">0</span>]*len(prices)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,len(prices)):<br>        <span class="hljs-keyword">if</span> prices[i]&lt;min_price:<br>            min_price = prices[i]<br>        opt[i] = max(opt[i<span class="hljs-number">-1</span>],prices[i]-min_price)<br>    <br>    <span class="hljs-keyword">return</span> opt[<span class="hljs-number">-1</span>]<br></code></pre></td></tr></table></figure>

<h4 id="圆圈中最后剩下的数字"><a href="#圆圈中最后剩下的数字" class="headerlink" title="圆圈中最后剩下的数字"></a><a href="https://leetcode-cn.com/problems/yuan-quan-zhong-zui-hou-sheng-xia-de-shu-zi-lcof/">圆圈中最后剩下的数字</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><br></code></pre></td></tr></table></figure>

<h4 id="把字符串转换成整数"><a href="#把字符串转换成整数" class="headerlink" title="把字符串转换成整数"></a><a href="https://leetcode-cn.com/problems/ba-zi-fu-chuan-zhuan-huan-cheng-zheng-shu-lcof/">把字符串转换成整数</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">strToInt</span>(<span class="hljs-params">self, str: str</span>) -&gt; int:</span><br>    sign,res = <span class="hljs-number">1</span>,<span class="hljs-number">0</span><br>    str = str.strip()<br>    <br>    <span class="hljs-comment"># 情况1: 字符串只有空格</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> str: <span class="hljs-keyword">return</span> <span class="hljs-number">0</span> <br>    <span class="hljs-comment"># 首先判断第一个字符</span><br>    <span class="hljs-keyword">if</span> str[<span class="hljs-number">0</span>] == <span class="hljs-string">&#x27;-&#x27;</span> : sign = <span class="hljs-number">-1</span><br>    <span class="hljs-keyword">elif</span> str[<span class="hljs-number">0</span>] == <span class="hljs-string">&#x27;+&#x27;</span>: sign = <span class="hljs-number">1</span><br>    <span class="hljs-keyword">elif</span> <span class="hljs-string">&#x27;0&#x27;</span>&lt;=str[<span class="hljs-number">0</span>]&lt;=<span class="hljs-string">&#x27;9&#x27;</span>:res = res*<span class="hljs-number">10</span> + int(str[<span class="hljs-number">0</span>])<br>    <span class="hljs-keyword">else</span>: <span class="hljs-keyword">return</span> res <br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> str[<span class="hljs-number">1</span>:]:<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;0&#x27;</span>&lt;=i&lt;=<span class="hljs-string">&#x27;9&#x27;</span>:<br>            res = res*<span class="hljs-number">10</span> + ord(i) - ord(<span class="hljs-string">&#x27;0&#x27;</span>)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">break</span><br>    <span class="hljs-keyword">if</span> sign&gt; <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">return</span> res <span class="hljs-keyword">if</span> res &lt; <span class="hljs-number">2</span>**<span class="hljs-number">31</span><span class="hljs-number">-1</span> <span class="hljs-keyword">else</span> <span class="hljs-number">2</span>**<span class="hljs-number">31</span><span class="hljs-number">-1</span><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> res*sign <span class="hljs-keyword">if</span> res*sign &gt; <span class="hljs-number">-2</span>**<span class="hljs-number">31</span> <span class="hljs-keyword">else</span>  <span class="hljs-number">-2</span>**<span class="hljs-number">31</span><br></code></pre></td></tr></table></figure>

<h4 id="数值的整数次方"><a href="#数值的整数次方" class="headerlink" title="数值的整数次方"></a><a href="https://leetcode-cn.com/problems/shu-zhi-de-zheng-shu-ci-fang-lcof/">数值的整数次方</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">myPow</span>(<span class="hljs-params">self, x: float, n: int</span>) -&gt; float:</span><br>    <span class="hljs-comment">### 分治思想</span><br>    <span class="hljs-keyword">if</span> n == <span class="hljs-number">0</span>: <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> <span class="hljs-comment"># base </span><br>    <span class="hljs-keyword">if</span> n&lt;<span class="hljs-number">0</span> : x,n = <span class="hljs-number">1</span>/x,-n  <br>    <span class="hljs-keyword">if</span> n %<span class="hljs-number">2</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">return</span> self.myPow(x,n//<span class="hljs-number">2</span>)**<span class="hljs-number">2</span><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> x * self.myPow(x,n//<span class="hljs-number">2</span>)**<span class="hljs-number">2</span><br></code></pre></td></tr></table></figure>

<h4 id="数字序列中某一位的数字"><a href="#数字序列中某一位的数字" class="headerlink" title="数字序列中某一位的数字"></a><a href="https://leetcode-cn.com/problems/shu-zi-xu-lie-zhong-mou-yi-wei-de-shu-zi-lcof/">数字序列中某一位的数字</a></h4><h4 id="0～n-1中缺失的数字"><a href="#0～n-1中缺失的数字" class="headerlink" title="0～n-1中缺失的数字"></a><a href="https://leetcode-cn.com/problems/que-shi-de-shu-zi-lcof/">0～n-1中缺失的数字</a></h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">missingNumber</span>(<span class="hljs-params">self, nums: List[int]</span>) -&gt; int:</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(nums)):<br>        <span class="hljs-keyword">if</span> nums[i] != i:<br>            <span class="hljs-keyword">return</span> i<br>    <span class="hljs-keyword">return</span> len(nums) <span class="hljs-comment"># 类似 [0]这样的情况，缺失的为1</span><br><br>    <span class="hljs-comment"># 思路2: 排序数组中的搜索问题，首先想到&quot;二分&quot;解决。</span><br>    xxx<br></code></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Leetcode-滑动窗口</title>
    <url>/2020/09/09/Algorithm-sliding-window/</url>
    <content><![CDATA[<h3 id="题目要求"><a href="#题目要求" class="headerlink" title="题目要求"></a>题目要求</h3><p><u>连续</u>子数组</p>
<h3 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h3><ul>
<li>根据要求，找到第一个满足条件的窗口</li>
<li>调整窗口(右移/左移)使其继续满足要求，并进行重复</li>
<li>得到最合适的窗口即为答案</li>
</ul>
<h3 id="Leetcode例题"><a href="#Leetcode例题" class="headerlink" title="Leetcode例题"></a>Leetcode例题</h3><h5 id="无重复字符的最长子串"><a href="#无重复字符的最长子串" class="headerlink" title="无重复字符的最长子串"></a><a href="https://leetcode-cn.com/problems/longest-substring-without-repeating-characters/">无重复字符的最长子串</a></h5><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">lengthOfLongestSubstring</span>(<span class="hljs-params">s</span>):</span><br>    <span class="hljs-keyword">if</span> len(s) == <span class="hljs-number">0</span>: <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>    window_start,res = <span class="hljs-number">0</span>,<span class="hljs-number">0</span><br>    dic = &#123;&#125;<br>    <span class="hljs-keyword">for</span> window_end <span class="hljs-keyword">in</span> range(len(s)):<br>        char = s[window_end]<br>        <span class="hljs-keyword">if</span> char <span class="hljs-keyword">in</span> dic <span class="hljs-keyword">and</span> dic[char]&gt;=window_start: <span class="hljs-comment"># 注意这里的第二个条件dic[char]&gt;=window_start！！ 起到一个覆盖的作用</span><br>            window_start = dic[char] +<span class="hljs-number">1</span><br>        dic[char] = window_end<br>        width = window_end-window_start+<span class="hljs-number">1</span><br>        res = max(width,res)<br>    <span class="hljs-keyword">return</span> res  <br></code></pre></td></tr></table></figure>

<h5 id="大小为-K-且平均值大于等于阈值的子数组数目"><a href="#大小为-K-且平均值大于等于阈值的子数组数目" class="headerlink" title="大小为 K 且平均值大于等于阈值的子数组数目"></a><a href="https://leetcode-cn.com/problems/number-of-sub-arrays-of-size-k-and-average-greater-than-or-equal-to-threshold/">大小为 K 且平均值大于等于阈值的子数组数目</a></h5><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">numOfSubarrays</span>(<span class="hljs-params">arr,k,threshold</span>):</span><br>    <span class="hljs-keyword">if</span> arr == [] <span class="hljs-keyword">or</span> len(arr)&lt;k:<span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>    window_sum,window_start,res = <span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span><br>    temp = threshold*k<br>    <span class="hljs-keyword">for</span> window_end <span class="hljs-keyword">in</span> range(len(arr)):<br>        window_sum+=arr[window_end]<br>        <span class="hljs-keyword">if</span> window_end&gt;=k<span class="hljs-number">-1</span>:<br>            <span class="hljs-keyword">if</span> window_sum &gt;=temp:<br>                res+=<span class="hljs-number">1</span><br>            window_sum-=arr[window_start]<br>            window_start+=<span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure>

<h5 id="替换后的最长重复字符"><a href="#替换后的最长重复字符" class="headerlink" title="替换后的最长重复字符"></a><a href="https://leetcode-cn.com/problems/longest-repeating-character-replacement/">替换后的最长重复字符</a></h5><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><br></code></pre></td></tr></table></figure>

<h5 id="最小覆盖子串"><a href="#最小覆盖子串" class="headerlink" title="最小覆盖子串"></a><a href="https://leetcode-cn.com/problems/minimum-window-substring/">最小覆盖子串</a></h5><h5 id="滑动窗口中位数"><a href="#滑动窗口中位数" class="headerlink" title="滑动窗口中位数"></a><a href="https://leetcode-cn.com/problems/sliding-window-median/">滑动窗口中位数</a></h5><h5 id="滑动窗口最大值"><a href="#滑动窗口最大值" class="headerlink" title="滑动窗口最大值"></a><a href="https://leetcode-cn.com/problems/sliding-window-maximum/">滑动窗口最大值</a></h5>]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Blocking and Relational Entity Resolution</title>
    <url>/2020/10/11/Blocking-and-Relational-Entity-Resolution/</url>
    <content><![CDATA[<h1 id="前情提要"><a href="#前情提要" class="headerlink" title="前情提要"></a>前情提要</h1><p>Entity Resolution 主要分成如下四个</p>
<ol>
<li>Coreference     文本与文本之间找同一个entity</li>
<li>Entity linking  文本与KG中找对应entity     -&gt; Integrating New Candidates </li>
<li>Deduplication   一个KG之间的聚类              -&gt; Merging  Ambiguous Entities</li>
<li>Record linkage  两个不同KG之间entity的对应  -&gt; Combining KGs</li>
</ol>
<h1 id="Blocking"><a href="#Blocking" class="headerlink" title="Blocking"></a>Blocking</h1><h2 id="为什么需要Blocking-gt-reduce-the-number-of-comparisons"><a href="#为什么需要Blocking-gt-reduce-the-number-of-comparisons" class="headerlink" title="为什么需要Blocking -&gt; reduce the number of comparisons"></a>为什么需要Blocking -&gt; reduce the number of comparisons</h2><ul>
<li>Comparing each entity with all other entities is too computationally demanding –&gt; O(N^2)</li>
<li>If partition entities into N ”blocks”– O(N)</li>
<li>Make only within block comparisons, so if largest block is log N in size –&gt; O(NlogN^2)</li>
</ul>
<h2 id="Blocking-分类"><a href="#Blocking-分类" class="headerlink" title="Blocking 分类"></a>Blocking 分类</h2><p>Disjoint Blocking: Each mention appears in one block.(=Set Partition)<br>Non-disjoint Blocking: Mentions can appear in more than one block. </p>
<h2 id="Blocking一般的情形"><a href="#Blocking一般的情形" class="headerlink" title="Blocking一般的情形"></a>Blocking一般的情形</h2><p><img src="/image/blocking_scenario.png"></p>
<h2 id="Blocking衡量指标"><a href="#Blocking衡量指标" class="headerlink" title="Blocking衡量指标"></a>Blocking衡量指标</h2><ul>
<li>Efficiency: Blue/Grey</li>
<li>Recall:     Green/Yellow</li>
<li>Precision:  Green/Blue</li>
<li>Max Canopy Size: 包含mentions个数最多的block的mentions个数</li>
</ul>
<h2 id="Blocking方式"><a href="#Blocking方式" class="headerlink" title="Blocking方式"></a>Blocking方式</h2><h3 id="Feature-based-blocking-keys"><a href="#Feature-based-blocking-keys" class="headerlink" title="Feature-based blocking keys"></a>Feature-based blocking keys</h3><p>思想: 通过选择实体的某一个或者多个属性作为key，将包含该key的实体放在同一个block下，对每个block再进行entity resolution</p>
<p>例子:<br>First three characters of last name<br>City + State + Zip<br>Character or Token n-grams<br>Minimum infrequent n-grams</p>
<h3 id="Clustering-or-sorting"><a href="#Clustering-or-sorting" class="headerlink" title="Clustering or sorting"></a>Clustering or sorting</h3><ol>
<li><p>Sorted Neighborhood Blocking<br>思想: 通过选择实体的某一个属性,根据该属性对实体进行排序，使用一个窗格，窗格内的实体划分到一个block中去</p>
</li>
<li><p>Canopy Clustering<br>Input: Mentions M, x is an entity<br>d(x,y), a distance metric<br>thresholds T1 &gt; T2</p>
</li>
</ol>
<p>思想:</p>
<ol>
<li>Pick a random element x from M</li>
<li>Create new canopy Cx using mentions y s.t. d(x,y) &lt; T1</li>
<li>Delete all mentions y from M s.t. d(x,y) &lt; T2 </li>
<li>Return to Step 1 if M is not empty</li>
</ol>
<p><img src="/image/canopy_cluster.png"></p>
<h3 id="Hashing"><a href="#Hashing" class="headerlink" title="Hashing"></a>Hashing</h3><p>思想:</p>
<ol>
<li>Each block Ci is associated with a hash key hi.</li>
<li>Mention x is hashed to Ci if hash(x) = hi.</li>
<li>Within a block, all pairs are compared.</li>
<li>Each hash function results in disjoint blocks.</li>
</ol>
<h2 id="Blocking选择考虑因素"><a href="#Blocking选择考虑因素" class="headerlink" title="Blocking选择考虑因素"></a>Blocking选择考虑因素</h2><ul>
<li>key的选择:learn the keys, or use expert knowledge/heuristics?</li>
<li>Schema awareness: what do we know about the attributes?</li>
<li>Key type: exact equality, similarity-based, or hybrid 相同的key放到一个block还是相似放一个block</li>
<li>Redundancy: entity in one or multiple blocks? Does matching in multiple blocks increase the match probability</li>
<li>Frequency limits</li>
<li>Adaptive keys based on frequency</li>
<li>Learning keys based on data </li>
</ul>
<h2 id="Learning-to-block"><a href="#Learning-to-block" class="headerlink" title="Learning to block"></a>Learning to block</h2><p>Using one or more blocking predicates may be insufficient =&gt; Construct blocking predicates by combining simple predicates</p>
<h1 id="Collective-Relational-Entity-Resolution"><a href="#Collective-Relational-Entity-Resolution" class="headerlink" title="Collective Relational Entity Resolution"></a>Collective Relational Entity Resolution</h1><h2 id="策略-Using-PSL-for-collective-KG-ER"><a href="#策略-Using-PSL-for-collective-KG-ER" class="headerlink" title="策略: Using PSL for collective KG ER"></a>策略: Using PSL for collective KG ER</h2><ol>
<li>Encode ER dependencies in a set of rules</li>
<li>Use soft-logic values to capture similarities</li>
<li>Use logic to capture the constraints</li>
</ol>
]]></content>
      <tags>
        <tag>Knowledge Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>CSCI570-outline</title>
    <url>/2020/09/08/CSCI570-outline/</url>
    <content><![CDATA[<p>hello world!</p>
]]></content>
      <tags>
        <tag>Course</tag>
      </tags>
  </entry>
  <entry>
    <title>命令行备忘录</title>
    <url>/2020/12/01/Command-Lib/</url>
    <content><![CDATA[<h2 id="1-hexo"><a href="#1-hexo" class="headerlink" title="1. hexo"></a>1. hexo</h2><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python">$ hexo new <span class="hljs-string">&#x27;post&#x27;</span> <span class="hljs-comment"># Create a new post</span><br>$ hexo g  <span class="hljs-comment"># Generate static files  </span><br>$ hexo s  <span class="hljs-comment"># Run server</span><br>$ hexo d  <span class="hljs-comment"># Deploy to remote sites</span><br>$ hexo clean <span class="hljs-comment"># Remove cache file (db.json) and static files (public)</span><br></code></pre></td></tr></table></figure>


<h2 id="2-Conda"><a href="#2-Conda" class="headerlink" title="2. Conda"></a>2. Conda</h2><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python">$ conda create -n env_name python=<span class="hljs-number">3.6</span>   <span class="hljs-comment"># 创建虚拟环境</span><br>$ conda activate env_name   <span class="hljs-comment"># 激活虚拟环境</span><br>$ conda deactivate env_name <span class="hljs-comment"># 退出虚拟环境</span><br>$ conda env list <span class="hljs-comment"># 查看conda环境</span><br><br><span class="hljs-comment"># 关于切换jupyter中服务内核(在jupyter notebook中使用不同的环境)</span><br><span class="hljs-number">1.</span> $ conda activate env_name <br><span class="hljs-number">2.</span> $ conda install ipykernel<br><span class="hljs-number">3.</span> $ python -m ipykernel install --name env_name<br><span class="hljs-number">4.</span> $ jupyter notebook  新建python,出现对应环境提示<br><br><span class="hljs-comment"># 修改环境名称（无法修改，只能复制+删除）</span><br><span class="hljs-number">1.</span> $ conda create -n new_env --clone old_env<br><span class="hljs-number">2.</span> $ conda remove -n old_env --all<br><br></code></pre></td></tr></table></figure>

<h2 id="3-Linux"><a href="#3-Linux" class="headerlink" title="3. Linux"></a>3. Linux</h2><figure class="highlight powershell"><table><tr><td class="code"><pre><code class="hljs powershell"><span class="hljs-variable">$</span> du <span class="hljs-literal">-h</span> -<span class="hljs-literal">-max</span><span class="hljs-literal">-depth</span>=<span class="hljs-number">1</span>  <span class="hljs-built_in">dir</span><span class="hljs-literal">-name</span> <span class="hljs-comment"># 查看文件夹大小</span><br><span class="hljs-variable">$</span> du <span class="hljs-literal">-sh</span> * <span class="hljs-comment">#查看当前文件以及文件夹的大小</span><br><span class="hljs-variable">$</span> htop  <span class="hljs-comment"># 查看系统cpu、mem使用情况 </span><br><span class="hljs-variable">$</span> htop <span class="hljs-literal">-u</span> username <span class="hljs-comment">#查看某个用户的进程使用</span><br><span class="hljs-variable">$</span> cpulimit <span class="hljs-literal">-l</span> <span class="hljs-number">50</span> <span class="hljs-literal">-p</span> pid <span class="hljs-comment">#限制某个进程的cpu使用率为50%</span><br><span class="hljs-variable">$</span> rclone <span class="hljs-comment"># 本地文件与google drive连接的桥梁</span><br><br><span class="hljs-variable">$</span> <span class="hljs-built_in">ls</span> <span class="hljs-literal">-l</span> | grep <span class="hljs-string">&quot;^-&quot;</span> | wc <span class="hljs-literal">-l</span>   <span class="hljs-comment"># 统计当前文件夹下文件的个数（不包括目录）</span><br><span class="hljs-variable">$</span> <span class="hljs-built_in">ls</span> <span class="hljs-literal">-lR</span>| grep <span class="hljs-string">&quot;^-&quot;</span> | wc <span class="hljs-literal">-l</span>   <span class="hljs-comment"># 统计当前目录下文件的个数（包括子目录）</span><br><span class="hljs-variable">$</span> <span class="hljs-built_in">ls</span> <span class="hljs-literal">-lR</span>| grep <span class="hljs-string">&quot;^-&quot;</span> | wc <span class="hljs-literal">-l</span>   <span class="hljs-comment"># 统计当前目录下目录的个数（包括子目录）</span><br><br><span class="hljs-variable">$</span> gzip –c filename &gt; filename.gz  	<span class="hljs-comment"># 压缩文件成gz文件（保留原文件）</span><br><span class="hljs-variable">$</span> gunzip –c filename.gz &gt; filename  <span class="hljs-comment"># 解压gz文件（保留gz文件）</span><br><br><span class="hljs-variable">$</span> head <span class="hljs-literal">-n</span> <span class="hljs-number">10</span> filename  <span class="hljs-comment"># 查看文件前十行内容</span><br><span class="hljs-variable">$</span> tail <span class="hljs-literal">-n</span> <span class="hljs-number">10</span> filename  <span class="hljs-comment"># 查看文件后十行内容</span><br></code></pre></td></tr></table></figure>


<h2 id="4-CPU信息"><a href="#4-CPU信息" class="headerlink" title="4. CPU信息"></a>4. CPU信息</h2><figure class="highlight powershell"><table><tr><td class="code"><pre><code class="hljs powershell">Mac:<br><span class="hljs-variable">$</span> sysctl machdep.cpu <span class="hljs-comment">#所有信息</span><br><span class="hljs-variable">$</span> sysctl <span class="hljs-literal">-n</span> machdep.cpu.brand_string    <span class="hljs-comment"># CPU型号</span><br><span class="hljs-variable">$</span> sysctl <span class="hljs-literal">-n</span> machdep.cpu.core_count      <span class="hljs-comment"># CPU核心数</span><br><span class="hljs-variable">$</span> sysctl <span class="hljs-literal">-n</span> machdep.cpu.thread_count    <span class="hljs-comment"># 线程数</span><br><br>Linux<br><span class="hljs-variable">$</span> grep <span class="hljs-string">&#x27;physical id&#x27;</span> /proc/cpuinfo | <span class="hljs-built_in">sort</span> <span class="hljs-literal">-u</span>  <span class="hljs-comment"># 查看物理cpu个数</span><br><span class="hljs-variable">$</span> grep <span class="hljs-string">&#x27;core id&#x27;</span> /proc/cpuinfo | <span class="hljs-built_in">sort</span> <span class="hljs-literal">-u</span> | wc <span class="hljs-literal">-l</span>  <span class="hljs-comment"># 核心数量</span><br><span class="hljs-variable">$</span> grep <span class="hljs-string">&#x27;processor&#x27;</span> /proc/cpuinfo | <span class="hljs-built_in">sort</span> <span class="hljs-literal">-u</span> | wc <span class="hljs-literal">-l</span>  <span class="hljs-comment"># 线程数</span><br></code></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>Commands</tag>
      </tags>
  </entry>
  <entry>
    <title>DSCI-553-outline</title>
    <url>/2020/10/23/DSCI-553-outline/</url>
    <content><![CDATA[<h2 id="DSCI551-Foundations-and-Applications-of-Data-Mining"><a href="#DSCI551-Foundations-and-Applications-of-Data-Mining" class="headerlink" title="DSCI551 - Foundations and Applications of Data Mining"></a>DSCI551 - Foundations and Applications of Data Mining</h2><h3 id="Week1"><a href="#Week1" class="headerlink" title="Week1:  "></a>Week1:  <a href="/2020/11/10/553week1/" title="数据挖掘, MapReduce 介绍">数据挖掘, MapReduce 介绍</a></h3><h3 id="Part2"><a href="#Part2" class="headerlink" title="Part2:"></a>Part2:</h3><!-- <a href="/2020/09/07/JSON-review/" title="JSON-review">JSON-review</a>  -->


<h3 id="Part3"><a href="#Part3" class="headerlink" title="Part3: "></a>Part3: <a href="/2020/10/23/Social-network/" title="社交网络">社交网络</a></h3><!-- <a href="/2020/09/10/File-Systems/" title="File systems">File systems</a>  -->




]]></content>
      <tags>
        <tag>Course</tag>
      </tags>
  </entry>
  <entry>
    <title>DSCI551 outline</title>
    <url>/2020/09/06/DSCI551-outline/</url>
    <content><![CDATA[<h2 id="DSCI551-Foundations-of-Data-Management"><a href="#DSCI551-Foundations-of-Data-Management" class="headerlink" title="DSCI551 - Foundations of Data Management"></a>DSCI551 - Foundations of Data Management</h2><h3 id=""><a href="#" class="headerlink" title=""></a><a href="/2020/09/06/Firebase-rest-web-api/" title="Firebase rest &amp; web api">Firebase rest &amp; web api</a></h3><h3 id="-1"><a href="#-1" class="headerlink" title=""></a><a href="/2020/09/07/JSON-review/" title="JSON-review">JSON-review</a></h3><h3 id="-2"><a href="#-2" class="headerlink" title=""></a><a href="/2020/09/08/Storage-systems/" title="Storage systems">Storage systems</a></h3><h3 id="-3"><a href="#-3" class="headerlink" title=""></a><a href="/2020/09/10/File-Systems/" title="File systems">File systems</a></h3><h3 id="-4"><a href="#-4" class="headerlink" title=""></a><a href="/2020/09/11/NFS/" title="Network File System">Network File System</a></h3><h3 id="-5"><a href="#-5" class="headerlink" title=""></a><a href="/2020/09/18/File-Format/" title="XML">XML</a></h3>]]></content>
      <tags>
        <tag>Course</tag>
      </tags>
  </entry>
  <entry>
    <title>DSCI558 Building Knowledge Graphs</title>
    <url>/2020/10/11/DSCI558-outline/</url>
    <content><![CDATA[<h2 id="DSCI558-Building-Knowledge-Graphs"><a href="#DSCI558-Building-Knowledge-Graphs" class="headerlink" title="DSCI558 - Building Knowledge Graphs"></a>DSCI558 - Building Knowledge Graphs</h2><h3 id=""><a href="#" class="headerlink" title=""></a><a href="/2020/12/01/558intro/" title="知识图谱简介">知识图谱简介</a></h3><h3 id="-1"><a href="#-1" class="headerlink" title=""></a><a href="/2020/12/01/558data-acq/" title="知识图谱数据获取与知识产权">知识图谱数据获取与知识产权</a></h3><h3 id="-2"><a href="#-2" class="headerlink" title=""></a><a href="/2020/12/01/558info-extra/" title="知识图谱数据提取">知识图谱数据提取</a></h3><h3 id="-3"><a href="#-3" class="headerlink" title=""></a><a href="/2020/12/01/558kg-rep/" title="知识图谱表示">知识图谱表示</a></h3><h3 id="-4"><a href="#-4" class="headerlink" title=""></a><a href="/2020/12/01/558ent-res/" title="知识图谱实体解析">知识图谱实体解析</a></h3><h3 id="-5"><a href="#-5" class="headerlink" title=""></a><a href="/2020/12/01/558query-kg/" title="知识图谱查询">知识图谱查询</a></h3><h3 id="-6"><a href="#-6" class="headerlink" title=""></a><a href="/2020/12/01/558ent-link/" title="知识图谱实体链接">知识图谱实体链接</a></h3><h3 id="-7"><a href="#-7" class="headerlink" title=""></a><a href="/2020/12/01/558str-match/" title="知识图谱字符串匹配">知识图谱字符串匹配</a></h3><h3 id="-8"><a href="#-8" class="headerlink" title=""></a><a href="/2020/10/11/Probabilistic-Models-for-KG-Construction/" title="知识图谱识别">知识图谱识别</a></h3><h3 id="-9"><a href="#-9" class="headerlink" title=""></a><a href="/2020/12/02/558blk-RER/" title="知识图谱Blocking&amp;关系型实体解析">知识图谱Blocking&amp;关系型实体解析</a></h3><h3 id="-10"><a href="#-10" class="headerlink" title=""></a><a href="/2020/12/03/558ont-rea/" title="知识图谱本体与推理">知识图谱本体与推理</a></h3><h3 id="-11"><a href="#-11" class="headerlink" title=""></a><a href="/2020/12/03/558rdfa/" title="知识图谱RDFa">知识图谱RDFa</a></h3><h3 id="-12"><a href="#-12" class="headerlink" title=""></a><a href="/2020/12/03/558-table-und/" title="知识图谱表格理解">知识图谱表格理解</a></h3><h3 id="-13"><a href="#-13" class="headerlink" title=""></a><a href="/2020/12/03/558kg-emb/" title="知识图谱图嵌入">知识图谱图嵌入</a></h3><h3 id="-14"><a href="#-14" class="headerlink" title=""></a><a href="/2020/12/03/558ld-sw/" title="知识图谱键连数据与语义网络">知识图谱键连数据与语义网络</a></h3><h3 id="-15"><a href="#-15" class="headerlink" title=""></a><a href="/2020/12/03/558overview/" title="知识图谱复习大纲">知识图谱复习大纲</a></h3><!-- ### <a href="/2020/10/11/Blocking-and-Relational-Entity-Resolution/" title="Blocking and Relational Entity Resolution">Blocking and Relational Entity Resolution</a>   -->
]]></content>
      <tags>
        <tag>Course</tag>
      </tags>
  </entry>
  <entry>
    <title>分类问题中的评估指标</title>
    <url>/2020/11/04/Eva-Classification-Problem/</url>
    <content><![CDATA[<h2 id="Term"><a href="#Term" class="headerlink" title="Term"></a>Term</h2><ul>
<li>True Positives (TP): should be TRUE, you predicted TRUE</li>
<li>False Positives (FP): should be FALSE, you predicted TRUE</li>
<li>True Negative (TN): should be FALSE, you predicted FALSE</li>
<li>False Negatives (FN): should be TRUE, you predicted FALSE</li>
</ul>
<p>个人看来，TP和TN都很好理解，二者反映了模型对于预测的准确性。FP和FN有点容易记不太顺，所以使用一个例子就很好去区分：<br>张三去看病，是否得了癌症, FP=&gt;误诊, FN=&gt;没查出来。</p>
<h2 id="Accuracy-准确率"><a href="#Accuracy-准确率" class="headerlink" title="Accuracy - 准确率"></a>Accuracy - 准确率</h2><blockquote>
<p>What percentage of my predictions are correct? 模型预测的正确性</p>
</blockquote>
<p><img src="/image/acc.png"></p>
<ul>
<li>Good for single label, binary classifcation.</li>
<li>Not good for imbalanced datasets. 受数据不均衡影响很大<br>  If, in the dataset, 99% of samples are TRUE and you blindly predict TRUE for everything, you’ll have 0.99 accuracy, but you haven’t actually learned anything.</li>
</ul>
<h2 id="Precision-查准率"><a href="#Precision-查准率" class="headerlink" title="Precision - 查准率"></a>Precision - 查准率</h2><blockquote>
<p>Of the points that I predicted TRUE, how many are actually TRUE? 模型预测的正例中。真实正例的比例</p>
</blockquote>
<p><img src="/image/pre.png"></p>
<ul>
<li>Good for multi-label / multi-class classification and information retrieval</li>
<li>Good for unbalanced datasets</li>
</ul>
<h2 id="Recall-查全率"><a href="#Recall-查全率" class="headerlink" title="Recall - 查全率"></a>Recall - 查全率</h2><blockquote>
<p>Of all the points that are actually TRUE, how many did I correctly predict 真实正例中，模型预测为正的比例</p>
</blockquote>
<p><img src="/image/recall.png"></p>
<ul>
<li>Good for multi-label / multi-class classification and information retrieval</li>
<li>Good for unbalanced datasets</li>
</ul>
<h2 id="F1"><a href="#F1" class="headerlink" title="F1"></a>F1</h2><blockquote>
<p>Can you give me a single metric that balances precision and recall?  F1-score (alternatively, F1-Measure), is a mixed metric that takes into account both Precision and Recall. 将查全与查准都考虑进去的F1</p>
</blockquote>
<p><img src="/image/f1.png"></p>
<ul>
<li>Gives equal weight to precision and recall</li>
<li>Good for unbalanced datasets</li>
</ul>
<h2 id="AUC-Area-under-ROC-Curve"><a href="#AUC-Area-under-ROC-Curve" class="headerlink" title="AUC (Area under ROC Curve)"></a>AUC (Area under ROC Curve)</h2><blockquote>
<p>Is my model better than just random guessing?<br>…待定。。。</p>
</blockquote>
<p>源地址： <a href="https://queirozf.com/entries/evaluation-metrics-for-classification-quick-examples-references#precision">https://queirozf.com/entries/evaluation-metrics-for-classification-quick-examples-references#precision</a></p>
]]></content>
      <tags>
        <tag>Evaluation</tag>
      </tags>
  </entry>
  <entry>
    <title>排名问题中的评估指标</title>
    <url>/2020/11/04/Eva-Ranking-Problem/</url>
    <content><![CDATA[<h2 id="Ranking-Problems"><a href="#Ranking-Problems" class="headerlink" title="Ranking Problems"></a>Ranking Problems</h2><p>In many domains, data scientists are asked to not just predict what class/classes an example belongs to, but to rank classes according to how likely they are for a particular example.<br>在许多领域，数据科学家不仅被要求预测一个示例属于哪个类别，还应根据特定示例对它们进行分类的可能性来对类别进行排名。</p>
<table>
<thead>
<tr>
<th>Classification</th>
<th>Ranking</th>
</tr>
</thead>
<tbody><tr>
<td>Order of predictions doesn’t matter</td>
<td>Order of predictions does matter</td>
</tr>
</tbody></table>
<p>This is often the case because, in the real world, resources are limited.<br>这是经常发生的情况，因为在现实世界中，资源是有限的。</p>
<p>This means that whoever will use the predictions your model makes has limited time, limited space. So they will likely prioritize.Some domains where this effect is particularly noticeable:<br>这意味着对于预测出的结果，任何人只能有限的空间以及时间，所以他们更倾向于优先级高的。在一些特定领域尤其明显:</p>
<p>• Search engines: Predict which documents match a query on a search engine.<br>• Tag suggestion for Tweets: Predict which tags should be assigned to a tweet.<br>• Image label prediction: Predict what labels should be suggested for an uploaded picture.<br>• 搜索引擎：预测哪些文档匹配搜索引擎上的查询。<br>• 推文标签建议：预测应将哪些标签分配给推文。<br>• 图像标签预测：预测应该为上传的图片建议哪些标签。</p>
<p>If your machine learning model produces a real-value for each of the possible classes, you can turn a classification problem into a ranking problem.<br>如果机器学习模型为每一个类生成一个可能的实数，那么就可以将一个分类问题转化为排名问题。（个人感觉有一种硬分类和软分类的意味。）</p>
<p>In other words, if you predict scores for a set of examples and you have a ground truth, you can order your predictions from highest to lowest and compare them with the ground truth:<br>换句话说，如果预测一组示例的得分并且有基本事实，则可以将预测从最高到最低排序，并将它们与基本事实进行比较</p>
<p>• Search engines: Do relevant documents appear up on the list or down at the bottom?<br>• Tag suggestion for Tweets: Are the correct tags predicted with higher score or not?<br>• Image label prediction: Does your system correctly give more weight to correct labels?<br>• 搜索引擎：相关文档是否出现在列表的顶部或底部？<br>• 推特的标签建议：是否以较高的分数预测了正确的标签?<br>• 图像标签预测：系统是否正确地赋予了正确标签更多的权重？</p>
<p>In the following sections, we will go over many ways to evaluate ranked predictions with respect to actual values, or ground truth.<br>在以下各节中，我们将介绍许多方法来评估关于实际值或基本事实的排名预测。</p>
<h2 id="Sample-dataset-Ground-Truth"><a href="#Sample-dataset-Ground-Truth" class="headerlink" title="Sample dataset (Ground Truth)"></a>Sample dataset (Ground Truth)</h2><p>We will use the following dummy dataset to illustrate examples in this post:<br>使用以下虚拟数据集来说明本文中的示例:<br><img src="/image/sample-data.png"></p>
<h2 id="Precision-k"><a href="#Precision-k" class="headerlink" title="Precision @k"></a>Precision @k</h2><p>Precision @k is simply precision evaluated only up to the k-th prediction, i.e.:<br>Prec@K表示设定一个阈值K，在检索结果到第K个预测例子为止，排序结果的查准率<br><img src="/image/pred_k.png"> <img src="/image/pred_sample.png"></p>
<p>例子: Precision @1 = 1/1 =1 ,  Precision @4 = 3/4 =0.75,  Precision @8  = Precision = 4/8 = 0.5</p>
<h2 id="Recall-k"><a href="#Recall-k" class="headerlink" title="Recall @k"></a>Recall @k</h2><p>Recall @k is simply Recall evaluated only up to the -th prediction, i.e.:<br>Recall@K表示设定一个阈值K，在检索结果到第K个预测例子为止，排序结果的查全率</p>
<p><img src="/image/recall_k.png"> <img src="/image/rec_sample.png"></p>
<p>例子: Recall @1 = 1/4 =0.25 ,  Recall @4 = 3/4 =0.75,  Recall @8  = Recall = 4/4 = 1</p>
<h2 id="F1-k"><a href="#F1-k" class="headerlink" title="F1 @k"></a>F1 @k</h2><p>F1 @k is a rank-based metric that can be summarized as follows: “What F1-score do I get if I only consider the top k predictions my model outputs?<br>F1@K表示设定一个阈值K，在检索结果到第K个预测例子为止，排序结果的F1-score</p>
<p><img src="/image/f1_k.png"><img src="/image/sample_f1_k.png"></p>
<p>例子: F1 @1 = 2x (1/1 * 1/4 )/(1/1+1/4) = 0.4 </p>
<h2 id="AP-Average-Precision"><a href="#AP-Average-Precision" class="headerlink" title="AP (Average Precision)"></a>AP (Average Precision)</h2><p>AP is a metric that tells you how much of the relevant documents are concentrated in the highest ranked predictions.<br>AP是一种指标，它告诉您有多少相关文档集中在排名最高的预测中。</p>
<p>So for each threshold level (k) you take the difference between the Recall at the current level and the Recall at the previous threshold and multiply by the Precision at that level. Then sum the contributions of each.<br>因此，对于每个阈值级别（k），需要取当前级别的Recall和前一个阈值的Recall之间的差，然后乘以该级别的Precision，最后求和。</p>
<p>为什么会有AP？<br>precison只是考虑了返回结果中相关文档的个数，没有考虑文档之间的序。对一个搜索引擎或推荐系统而言返回的结果必然是有序的，而且越相关的文档排的越靠前越好，于是有了AP的概念。</p>
<p><img src="/image/AP.png"></p>
<h2 id="MAP-Mean-Average-Precision"><a href="#MAP-Mean-Average-Precision" class="headerlink" title="MAP (Mean Average Precision)"></a>MAP (Mean Average Precision)</h2><p>AP (Average Precision) is a metric that tells you how a single sorted prediction compares with the ground truth. E.g. AP would tell you how correct a single ranking of documents is, with respect to a single query.<br>AP可以得到单个排序的预测与基本事实进行比较。例如，对于单次查询得到的排序结果的文档到底有多正确。所以可以对所有的AP取平均得到MAP</p>
<blockquote>
<p>AP: Informs you how correct a model’s ranked predictions are for a single example<br> MAP: Informs you how correct a model’s ranked predictions are, on average, over a whole validation dataset.</p>
</blockquote>
<p>源地址: <a href="https://queirozf.com/entries/evaluation-metrics-for-ranking-problems-introduction-and-examples#ranking-problems">https://queirozf.com/entries/evaluation-metrics-for-ranking-problems-introduction-and-examples#ranking-problems</a></p>
]]></content>
      <tags>
        <tag>Evaluation</tag>
      </tags>
  </entry>
  <entry>
    <title>XML</title>
    <url>/2020/09/18/File-Format/</url>
    <content><![CDATA[<h3 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h3><ul>
<li>XML：eXtensible Markup Language</li>
<li>XML is a syntax (serialization format) for data sharing and exchange on the Web<ul>
<li>Can translate <em>any</em> data to XML</li>
<li>Can ship XML over the Web (HTTP)</li>
<li>Can input XML into any application</li>
</ul>
</li>
</ul>
<h3 id="XML-Structure"><a href="#XML-Structure" class="headerlink" title="XML Structure"></a>XML Structure</h3><p><img src="/image/XML.png"></p>
<ul>
<li>XML is self-describing</li>
<li>Schema elements become part of the data(关系型数据记录中schema不算是内容，但是在xml中节点自身也携带内容信息) </li>
<li>XML is semi-structured <ul>
<li>missing attributes  (could be represented as null in table)</li>
<li>repeated attributes (impossible in table)</li>
<li>Attributes with different types in different objects</li>
<li>Nested structures</li>
<li>Heterogeneous contents</li>
</ul>
</li>
<li>an XML document has a single root element</li>
</ul>
<figure class="highlight xml"><table><tr><td class="code"><pre><code class="hljs xml">example:<br><span class="hljs-tag">&lt;<span class="hljs-name">bib</span>&gt;</span><br>...<br>  <span class="hljs-tag">&lt;<span class="hljs-name">book</span> <span class="hljs-attr">price</span>=<span class="hljs-string">&quot;35&quot;</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">publisher</span>&gt;</span>Addison-Wesley<span class="hljs-tag">&lt;/<span class="hljs-name">publisher</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">author</span>&gt;</span>Serge Abiteboul<span class="hljs-tag">&lt;/<span class="hljs-name">author</span>&gt;</span> <br>    <span class="hljs-tag">&lt;<span class="hljs-name">author</span>&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">first-name</span>&gt;</span>Rick<span class="hljs-tag">&lt;/<span class="hljs-name">first-name</span>&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">last-name</span>&gt;</span>Hull<span class="hljs-tag">&lt;/<span class="hljs-name">last-name</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">author</span>&gt;</span> <br>    <span class="hljs-tag">&lt;<span class="hljs-name">author</span> <span class="hljs-attr">age</span>=<span class="hljs-string">&quot;20&quot;</span>&gt;</span>Victor Vianu<span class="hljs-tag">&lt;/<span class="hljs-name">author</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">title</span>&gt;</span>Foundations of Databases<span class="hljs-tag">&lt;/<span class="hljs-name">title</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">year</span>&gt;</span>1995<span class="hljs-tag">&lt;/<span class="hljs-name">year</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">price</span>&gt;</span>38.8<span class="hljs-tag">&lt;/<span class="hljs-name">price</span>&gt;</span> <br>  <span class="hljs-tag">&lt;/<span class="hljs-name">book</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">book</span> <span class="hljs-attr">price</span>=<span class="hljs-string">&quot;55&quot;</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">publisher</span>&gt;</span>Freeman<span class="hljs-tag">&lt;/<span class="hljs-name">publisher</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">author</span>&gt;</span>Jeffrey D. Ullman<span class="hljs-tag">&lt;/<span class="hljs-name">author</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">title</span>&gt;</span>Principles of Database and Knowledge Base Systems<span class="hljs-tag">&lt;/<span class="hljs-name">title</span>&gt;</span> <br>    <span class="hljs-tag">&lt;<span class="hljs-name">year</span>&gt;</span>1998<span class="hljs-tag">&lt;/<span class="hljs-name">year</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">book</span>&gt;</span> <br>... <br><span class="hljs-tag">&lt;/<span class="hljs-name">bib</span>&gt;</span><br></code></pre></td></tr></table></figure>

<h3 id="Querying-XML-Data"><a href="#Querying-XML-Data" class="headerlink" title="Querying XML Data"></a>Querying XML Data</h3><h4 id="XPath-simple-navigation-through-the-tree"><a href="#XPath-simple-navigation-through-the-tree" class="headerlink" title="XPath: simple navigation through the tree"></a>XPath: simple navigation through the tree</h4><p><img src="/image/xpath.png"></p>
<h3 id="lxml"><a href="#lxml" class="headerlink" title="lxml"></a>lxml</h3><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python">$ pip install lxml<br><br><span class="hljs-keyword">from</span> lxml <span class="hljs-keyword">import</span> etree<br>f = open(<span class="hljs-string">&#x27;bibs.xml&#x27;</span>)<br>tree = etree.parse(f)<br>print(etree.tostring(tree, pretty_print=<span class="hljs-literal">True</span>))<br><br><span class="hljs-keyword">for</span> element <span class="hljs-keyword">in</span> tree.xpath(<span class="hljs-string">&quot;//author&quot;</span>): <br>    print(etree.tostring(element))<br>    print(element.tag, element.text)<br><br></code></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>Python</tag>
        <tag>File Format</tag>
      </tags>
  </entry>
  <entry>
    <title>File systems</title>
    <url>/2020/09/10/File-Systems/</url>
    <content><![CDATA[<h3 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h3><blockquote>
<p>A filesystem is the methods and data structures that an operating system uses to keep track of files on a disk or partition; that is, the way the files are organized on the disk.</p>
</blockquote>
<ul>
<li>File content stored in blocks on storage device</li>
<li>Files are organized into directories (folders)<br><img src="/image/file-system.png"></li>
</ul>
<h3 id="Detail-about-file"><a href="#Detail-about-file" class="headerlink" title="Detail about file"></a>Detail about file</h3><h4 id="File-descriptor"><a href="#File-descriptor" class="headerlink" title="File descriptor"></a>File descriptor</h4><p>A file descriptor is a number that uniquely identifies an open file in a computer’s operating system. It describes a data resource, and how that resource may be accessed.<br>简单的说，根据Linux一切皆文件的概念来看，当进程打开或者创建文件的时候，内核会向进程返回一个数字，这个数字就是文件描述符，所有执行I/O操作的系统调用都通过文件描述符来进行。</p>
<h4 id="Hard-link-and-symbolic-link-soft-link"><a href="#Hard-link-and-symbolic-link-soft-link" class="headerlink" title="Hard link and symbolic link/soft link"></a>Hard link and symbolic link/soft link</h4><p>A hard link is essentially a synced carbon copy of a file that refers directly to the inode of a file. Symbolic links on the other hand refer directly to the file which refers to the inode, a shortcut.</p>
<p><img src="/image/hard-soft-link.png"></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">$ ln a.txt b.txt    <span class="hljs-comment"># 创建硬链接 - 副本</span><br>$ ln -s a.txt c.txt <span class="hljs-comment"># 创建软链接 - 替身（快捷方式）</span><br>$ unlink c.txt      <span class="hljs-comment"># 取消链接</span><br></code></pre></td></tr></table></figure>

<h4 id="File-permission-mode"><a href="#File-permission-mode" class="headerlink" title="File permission mode"></a>File permission mode</h4><p>3种模式  r:read w:write x:execute<br>3种身份: user group others<br>3*3 = 27 种访问权限<br>rw-r–r– =&gt; 110 (owner permission) 100 (group) 100 (others)<br>可以使用<code>chomod</code> 来进行权限的修改</p>
<h4 id="Inode"><a href="#Inode" class="headerlink" title="Inode"></a>Inode</h4><p>The inode (index node) is a data structure in a Unix-style file system that describes a file-system object such as a file or a directory. <a href="http://www.ruanyifeng.com/blog/2011/12/inode.html">更多inode细节</a> </p>
<ul>
<li>Stores metadata/attributes about the file ( use <code>stat file</code> to check the metadata )</li>
<li>Also stores locations of blocks holding the content of the file</li>
</ul>
<h5 id="metadata"><a href="#metadata" class="headerlink" title="metadata"></a>metadata</h5><figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk">More details about the file metadata<br><br>struct stat &#123;<br>dev_t st_dev; <span class="hljs-regexp">/* ID of device containing file */</span><br>ino_t st_ino; <span class="hljs-regexp">/* inode number */</span><br>mode_t st_mode; <span class="hljs-regexp">/* protection */</span><br>nlink_t st_nlink; <span class="hljs-regexp">/* number of (hard) links */</span><br>uid_t st_uid; <span class="hljs-regexp">/* user ID of owner */</span><br>gid_t st_gid; <span class="hljs-regexp">/* group ID of owner */</span><br>dev_t st_rdev; <span class="hljs-regexp">/* device ID (if special device file, e.g., /</span>etc<span class="hljs-regexp">/tty) */</span> <br>off_t st_size; <span class="hljs-regexp">/* total size, in bytes */</span><br>blksize_t st_blksize; <span class="hljs-regexp">/* blocksize for filesystem I/</span>O */<br>blkcnt_t st_blocks; <span class="hljs-regexp">/* number of blocks allocated */</span><br>time_t st_atime; <span class="hljs-regexp">/* last time file content was accessed */</span><br>time_t st_mtime; <span class="hljs-regexp">/* last time file content was modified */</span> <br>time_t st_ctime; <span class="hljs-regexp">/* last time inode was changed */</span><br>&#125;;<br><br>time_t st_atime; <span class="hljs-regexp">/* last time file content was accessed */</span> 关于这一项的思考：<br>这一项表明了不论对文件是什么操作，都会进行inode的修改，有的时候为了提升I/O性能，会在挂载文件系统的时候指定“noatime,nodiratime”参数，意味着当访问一个文件和目录的时候，access time都不会更新<br>可以通过 cat <span class="hljs-regexp">/etc/</span>fstab 查看具体信息<br></code></pre></td></tr></table></figure>
<p><img src="/image/inode.png"></p>
<h5 id="locations-of-data"><a href="#locations-of-data" class="headerlink" title="locations of data"></a>locations of data</h5><p>An inode has:</p>
<ul>
<li>A number of direct pointers,each points to a data block<br>example: a inode has 8 pointers, each pointer points to a 4K block, then this inode is enogh for 8*4K = 32KB size of file</li>
<li>Also has a slot for indirect pointer : a pointer points to a data block storing direct pointers<br>example: 1 block’s size: 4KB, pointer size: 4 bytes, then a block can hold 1024 pointers =&gt; Now file can have (8 + 1024) blocks , enough for 4MB size of file</li>
</ul>
<p>How to store larger files? - Multi-level index</p>
<ul>
<li>Pointers may be organized into multiple levels (double,triple,…n times) </li>
</ul>
<p><img src="/image/double-indirect-pointers.png"><br>1 direct pointer =&gt; 1 block (4KB)<br>1 indirect pointer =&gt; 2^10 direct pointer  =  2^10*4KB  = 4MB<br>1 double indirect pointer =&gt; 2^10 indirect pointers =&gt; 2^20 direct pointers  =&gt; 2^20*4KB = 4GB</p>
<h4 id="Organization-of-blocks"><a href="#Organization-of-blocks" class="headerlink" title="Organization of blocks"></a>Organization of blocks</h4><p>Assumption:(便于理解，做些假设)</p>
<ol>
<li>Disk consists of a list of blocks and they are array-based.(other forms:Tree-based, e.g., SGI XFS -Blocks are organized into variable-length extents)</li>
<li>a disk with 64 blocks<ul>
<li>4KB/block   =&gt;文件系统的最小单元</li>
<li>512B/sector =&gt;存储系统的最小单元 </li>
<li>so there are 2^12/2^9 = 2^3 = 8 sectors/block and capacity of disk = 64 * 4KB = 256KB</li>
</ul>
</li>
</ol>
<p>Structure about these blocks: </p>
<ol>
<li>Data region (56 blocks (#8-63))</li>
<li>Inode table (5 blocks #3 – #7) 元数据+文件内容地址<ul>
<li>assume 256 bytes/inode 5 blocks, 4KB/block</li>
<li>=&gt; 80 inodes total  (4KB/256B * 5)</li>
<li>=&gt; File system can store at most 80 files</li>
</ul>
</li>
<li>Bitmaps (#1, #2)  a vector of bits, 0 for free (inode/block), 1 for in-use<ul>
<li>Inode bitmap (imap):keep track of which inodes in the inode table are available  (这里4KB一个block 1byte=8bit，所以一个block的话最多以存储80K个inode的状态)</li>
<li>Data bitmap (dmap):keep track of which blocks in data region are available</li>
</ul>
</li>
<li>Superblock(#0): Track where i/d blocks and inode table are; Indicate type of FS &amp; inumber of its root dir; Will be read first when file system is mounted</li>
</ol>
<p><img src="/image/block_info_fs.png"></p>
<h4 id="Inumber"><a href="#Inumber" class="headerlink" title="Inumber"></a>Inumber</h4><p>Each inode is identified by a number: Low-level number of file name<br>Can figure out location of inode from inumber 通过inumber可以寻址到其代表的inode所在的sector<br>Location:  ⌊(inodeStartAddress + inumber ∗ inode size)/sector size⌋</p>
<p>example: inumber = 32<br>Address:12K+32*256=20K<br>Sector #: 20K/512 = 40<br><img src="/image/inumber.png"></p>
<p>总结inumber,inode,data一句话来说就是inumber =&gt; inode =&gt; data</p>
<h3 id="Detail-about-directory"><a href="#Detail-about-directory" class="headerlink" title="Detail about directory"></a>Detail about directory</h3><h4 id="Basic-1"><a href="#Basic-1" class="headerlink" title="Basic"></a>Basic</h4><ul>
<li>Directory itself stored as a file</li>
<li>For each file in the directory, it stores:<ul>
<li>name, inumber, record length, string length</li>
</ul>
</li>
<li>If file is deleted (using rm command) or a name is unlinked (using unlink command),then inumber in its directory entry set to 0 (reserved for empty entry)<ul>
<li>File is finally deleted when its last (hard) link is removed<br><img src="/image/directory.png"></li>
</ul>
</li>
</ul>
<h4 id="Storing-a-directory"><a href="#Storing-a-directory" class="headerlink" title="Storing a directory"></a>Storing a directory</h4><ul>
<li>Also as a file with its own inode + data block</li>
<li>inode:<ul>
<li>file type: directory (instead of regular file)</li>
<li>pointer to block(s) in data region storing directory entries</li>
</ul>
</li>
</ul>
<h3 id="Operations-on-file"><a href="#Operations-on-file" class="headerlink" title="Operations on file"></a>Operations on file</h3><ul>
<li>Create: open(), write()</li>
<li>Read:  open(),read(), lseek()</li>
<li>Update: write(), lseek()</li>
<li>Delete: unlink()</li>
</ul>
<p>Note: 这些函数都是操作系统自带提供的系统调用函数</p>
<h4 id="Read"><a href="#Read" class="headerlink" title="Read"></a>Read</h4><ul>
<li>User interface via GUI or touch command in Linux</li>
<li>Implementation, e.g., via a C program with a system call: open()</li>
<li>open() returns a file descriptor. Reserved fds: stdin 0, stdout, 1, stderr 2</li>
<li>After getting the file descriptor of file, start read() <figure class="highlight vhdl"><table><tr><td class="code"><pre><code class="hljs vhdl">fd = <span class="hljs-keyword">open</span>(<span class="hljs-string">&quot;/foo/bar&quot;</span>, O_RDONLY)<br>- Need <span class="hljs-keyword">to</span> locate inode <span class="hljs-keyword">of</span> the <span class="hljs-keyword">file</span> <span class="hljs-string">&quot;/foo/bar&quot;</span><br>- <span class="hljs-keyword">Assume</span> inumber <span class="hljs-keyword">of</span> root, say <span class="hljs-number">2</span>, <span class="hljs-keyword">is</span> known (e.g., <span class="hljs-keyword">when</span> the <span class="hljs-keyword">file</span> system <span class="hljs-keyword">is</span> mounted)<br><br>step1: read inode <span class="hljs-keyword">and</span> content <span class="hljs-keyword">of</span> /  (<span class="hljs-number">2</span> reads)<br>- Look <span class="hljs-keyword">for</span> <span class="hljs-string">&quot;foo&quot;</span> <span class="hljs-keyword">in</span> / -&gt; foo<span class="hljs-symbol">&#x27;s</span> inumber<br><br>step2: read inode <span class="hljs-keyword">and</span> content <span class="hljs-keyword">of</span> /foo (<span class="hljs-number">2</span> reads)<br>- Look <span class="hljs-keyword">for</span> <span class="hljs-string">&quot;bar&quot;</span> <span class="hljs-keyword">in</span> /foo -&gt; bar<span class="hljs-symbol">&#x27;s</span> inumber<br><br>step3: read inode <span class="hljs-keyword">of</span> /foo/bar (<span class="hljs-number">1</span> read)<br>– Permission check + allocate <span class="hljs-keyword">file</span> descriptor<br><br><span class="hljs-keyword">open</span>-<span class="hljs-keyword">file</span> table per <span class="hljs-keyword">process</span>(维护这些进程，系统有一个 <span class="hljs-keyword">open</span>-<span class="hljs-keyword">file</span> table)<br><br><br>read(fd, <span class="hljs-keyword">buffer</span>, size)<br>– <span class="hljs-literal">Note</span> fd <span class="hljs-keyword">is</span> maintained <span class="hljs-keyword">in</span> per-<span class="hljs-keyword">process</span> <span class="hljs-keyword">open</span>-<span class="hljs-keyword">file</span> table<br>– Table translates fd -&gt; inumber <span class="hljs-keyword">of</span> <span class="hljs-keyword">file</span><br><br>step1: consult bar<span class="hljs-symbol">&#x27;s</span> inode <span class="hljs-keyword">to</span> locate a <span class="hljs-keyword">block</span><br>step2: read the <span class="hljs-keyword">block</span><br>step3: update inode <span class="hljs-keyword">with</span> newest <span class="hljs-keyword">file</span> <span class="hljs-keyword">access</span> <span class="hljs-built_in">time</span><br>step4: update <span class="hljs-keyword">open</span>-<span class="hljs-keyword">file</span> table <span class="hljs-keyword">with</span> <span class="hljs-keyword">new</span> offset<br>step5: repeat above steps <span class="hljs-keyword">until</span> done(<span class="hljs-keyword">with</span> reading data <span class="hljs-keyword">of</span> given size)<br><br></code></pre></td></tr></table></figure>
<img src="/image/read.png"></li>
</ul>
<p>I/O cost for open(): 5 reads<br>I/O cost for reading a block: 2 reads + 1 write</p>
<h4 id="Write"><a href="#Write" class="headerlink" title="Write"></a>Write</h4><figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql">int fd = open(&quot;/foo/bar&quot;, O_WRONLY)<br>– Or int fd = <span class="hljs-keyword">create</span>((<span class="hljs-string">&quot;/foo/bar&quot;</span>)<br>– Assume bar <span class="hljs-keyword">is</span> a <span class="hljs-keyword">new</span> <span class="hljs-keyword">file</span> <span class="hljs-keyword">under</span> foo<br><br>step1: <span class="hljs-keyword">read</span><span class="hljs-string">&#x27;/&#x27;</span>inode &amp; <span class="hljs-keyword">content</span> -&gt; obtain foo<span class="hljs-string">&#x27;s inumber</span><br><span class="hljs-string">step2: read&#x27;</span>/foo<span class="hljs-string">&#x27;inode &amp; content -&gt; check if bar exists</span><br><span class="hljs-string">step3: read imap,to find a free inode for bar</span><br><span class="hljs-string">step4: update imap,setting 1 for allocated inode</span><br><span class="hljs-string">step5: write bar&#x27;</span>s inode<br>step6: <span class="hljs-keyword">update</span> foo<span class="hljs-string">&#x27;s content block  - adding an entry for bar</span><br><span class="hljs-string">step7: update foo&#x27;</span>s inode  -<span class="hljs-keyword">update</span> its <span class="hljs-keyword">modification</span> <span class="hljs-built_in">time</span><br><br>write(fd, buffer, <span class="hljs-keyword">size</span>)<br>step1: <span class="hljs-keyword">read</span> inode <span class="hljs-keyword">of</span> bar(<span class="hljs-keyword">by</span> looking up its inumber <span class="hljs-keyword">in</span> the <span class="hljs-keyword">open</span>-<span class="hljs-keyword">file</span> <span class="hljs-keyword">table</span>)<br>step2: <span class="hljs-keyword">allocate</span> <span class="hljs-keyword">new</span> <span class="hljs-keyword">data</span> <span class="hljs-keyword">block</span> – <span class="hljs-keyword">read</span> <span class="hljs-keyword">and</span> write bmap<br>step3: write <span class="hljs-keyword">to</span> <span class="hljs-keyword">data</span> <span class="hljs-keyword">block</span> <span class="hljs-keyword">of</span> bar<br>step4: <span class="hljs-keyword">update</span> bar node - <span class="hljs-keyword">new</span> <span class="hljs-keyword">modification</span> <span class="hljs-built_in">time</span>, <span class="hljs-keyword">add</span> pointer <span class="hljs-keyword">to</span> <span class="hljs-keyword">block</span><br></code></pre></td></tr></table></figure>
<p><img src="/image/write.png"></p>
]]></content>
      <tags>
        <tag>Operating System</tag>
      </tags>
  </entry>
  <entry>
    <title>Firebase rest &amp; web api</title>
    <url>/2020/09/06/Firebase-rest-web-api/</url>
    <content><![CDATA[<h2 id="Firebase-概述（BaaS）"><a href="#Firebase-概述（BaaS）" class="headerlink" title="Firebase 概述（BaaS）"></a>Firebase 概述（BaaS）</h2><p>Firebase谷歌的一款应用后台服务。借助Firebase，应用开发者们可以快速搭建应用后台。</p>
<h3 id="产品"><a href="#产品" class="headerlink" title="产品"></a>产品</h3><ol>
<li>Firebase (realtime) database: Manage <strong>JSON</strong> documents + Real-time <strong>syncing</strong> data between users and devices</li>
<li>Firebase (cloud) storage: Store images, photos, videos</li>
<li>Firebase (user) authentication: Support sign in using Google, Facebook</li>
</ol>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><ol>
<li>Create a Firebase account: first use Google account,then go to Firebase console: <a href="https://console.firebase.google.com/">https://console.firebase.google.com/</a></li>
<li>Click on “Add project”  to create a Firebase project</li>
<li>Add Firebase to your web app</li>
<li>any other interesting operation …</li>
</ol>
<h3 id="关于-Realtime-database"><a href="#关于-Realtime-database" class="headerlink" title="关于 Realtime database"></a>关于 Realtime database</h3><ul>
<li>Data in JSON(Javascript Object Notation) format  <a href="/2020/09/07/JSON-review/" title="JSON-review">JSON-review</a> </li>
<li>When creating a real-time database, we should open up the access to allow us to read and write the data.</li>
<li>There are some difference between storing style from JSON file and Firebase real-time database<ol>
<li>JSON文件中的array元素在Realtime database下以object的形式存在，object对应的key为数组元素的索引</li>
<li>JSON文件可以保存value为null的键值对，Realtime database则会忽略这一对键值对<br><img src="/image/firebase1.png"></li>
</ol>
</li>
</ul>
<h2 id="Firebase-REST-API"><a href="#Firebase-REST-API" class="headerlink" title="Firebase REST API"></a>Firebase REST API</h2><h3 id="关于RESTful-API"><a href="#关于RESTful-API" class="headerlink" title="关于RESTful API"></a>关于RESTful API</h3><p> REST：Representation State Transfer，表现层状态转移<br> 一句话解释的话就是:通过URL定位资源，用HTTP动词（GET, POST, PUT, DELETE)描述操作从而用来实现前后端数据传输的协议。</p>
<h3 id="命令行进行http数据传输：curl"><a href="#命令行进行http数据传输：curl" class="headerlink" title="命令行进行http数据传输：curl"></a>命令行进行http数据传输：curl</h3><p>  For command operation, it’s convenient to use <strong>curl</strong> (Command line tool for data transfer)<br>  curl详细使用：<a href="https://itbilu.com/linux/man/4yZ9qH_7X.html">https://itbilu.com/linux/man/4yZ9qH_7X.html</a><br>  注意，curl大小写敏感，请求的命令参数均为大写字符</p>
<h3 id="curl例子-CRUD"><a href="#curl例子-CRUD" class="headerlink" title="curl例子 - CRUD"></a>curl例子 - CRUD</h3><p>  PUT &amp; POST (C), GET (R), PATCH (U) ,DELETE (D)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python">JSON tree:<br>&#123;<br>  <span class="hljs-string">&quot;firstName&quot;</span>: <span class="hljs-string">&quot;John&quot;</span>,<br>  <span class="hljs-string">&quot;lastName&quot;</span>: <span class="hljs-string">&quot;Smith&quot;</span>,<br>  <span class="hljs-string">&quot;isMarried&quot;</span>: false,<br>  <span class="hljs-string">&quot;age&quot;</span>: <span class="hljs-number">25</span>,<br>  <span class="hljs-string">&quot;height_cm&quot;</span>: <span class="hljs-number">167.6</span>,<br>  <span class="hljs-string">&quot;address&quot;</span>: &#123;<br>    <span class="hljs-string">&quot;streetAddress&quot;</span>: <span class="hljs-string">&quot;22nd Street&quot;</span>,<br>    <span class="hljs-string">&quot;city&quot;</span>: <span class="hljs-string">&quot;New York&quot;</span>,<br>    <span class="hljs-string">&quot;state&quot;</span>: <span class="hljs-string">&quot;NY&quot;</span>,<br>    <span class="hljs-string">&quot;postalCode&quot;</span>: <span class="hljs-string">&quot;10021-3100&quot;</span><br>  &#125;,<br>  <span class="hljs-string">&quot;phoneNumbers&quot;</span>: [<br>    &#123;<br>      <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;home&quot;</span>,<br>      <span class="hljs-string">&quot;number&quot;</span>: <span class="hljs-string">&quot;212 555-1234&quot;</span><br>    &#125;,<br>    &#123;<br>      <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;office&quot;</span>,<br>      <span class="hljs-string">&quot;number&quot;</span>: <span class="hljs-string">&quot;646 555-4567&quot;</span>,<br>      <span class="hljs-string">&quot;xyz&quot;</span>: null<br>    &#125;<br>  ],<br>  <span class="hljs-string">&quot;children&quot;</span>: [],<br>  <span class="hljs-string">&quot;spouse&quot;</span>: null,<br>  <span class="hljs-string">&quot;scores&quot;</span>: [<span class="hljs-number">8.5</span>, <span class="hljs-number">9</span>, <span class="hljs-number">7</span>, <span class="hljs-number">10</span>]<br>&#125;<br></code></pre></td></tr></table></figure>

<h4 id="GET-get-the-specific-resource"><a href="#GET-get-the-specific-resource" class="headerlink" title="GET: get the specific resource"></a>GET: get the specific resource</h4><figure class="highlight powershell"><table><tr><td class="code"><pre><code class="hljs powershell"><span class="hljs-variable">$</span> <span class="hljs-built_in">curl</span> <span class="hljs-literal">-X</span> GET <span class="hljs-string">&#x27;https://rest-apidemo.firebaseio.com/address/city.json&#x27;</span>   <span class="hljs-keyword">return</span>: <span class="hljs-string">&quot;New York&quot;</span>%  <br><span class="hljs-literal">-X</span>   可以省略 直接 <span class="hljs-built_in">curl</span> GET url; GET  可以省略 直接 <span class="hljs-built_in">curl</span> url<br><br><span class="hljs-variable">$</span> <span class="hljs-built_in">curl</span> <span class="hljs-string">&#x27;https://rest-apidemo.firebaseio.com/phoneNumbers/0.json&#x27;</span> <span class="hljs-comment">#refer to arr element by index</span><br><span class="hljs-keyword">return</span>:&#123;<span class="hljs-string">&quot;number&quot;</span>:<span class="hljs-string">&quot;212 555-1234&quot;</span>,<span class="hljs-string">&quot;type&quot;</span>:<span class="hljs-string">&quot;home&quot;</span>&#125;%  <br></code></pre></td></tr></table></figure>

<h4 id="PUT-write-a-given-value-e-g-“Mary”-to-a-specify-node-e-g-“spouse”"><a href="#PUT-write-a-given-value-e-g-“Mary”-to-a-specify-node-e-g-“spouse”" class="headerlink" title="PUT: write a given value (e.g., “Mary”) to a specify node (e.g., “spouse”)"></a>PUT: write a given value (e.g., “Mary”) to a specify node (e.g., “spouse”)</h4><figure class="highlight crmsh"><table><tr><td class="code"><pre><code class="hljs crmsh">Add if <span class="hljs-keyword">node</span> <span class="hljs-title">not</span> exists (could add embedded nodes) - 添加数据<br>Overwrite if <span class="hljs-keyword">node</span> <span class="hljs-title">already</span> has value               - 重写数据<br><br>$ curl -X PUT &#x27;https://rest-apidemo.firebaseio.com/spouse.json&#x27; -d &#x27;<span class="hljs-string">&quot;Mary&quot;</span>&#x27;  <span class="hljs-comment"># 注意引号</span><br><br><span class="hljs-comment">#This will add a new node &quot;country&quot; (assuming it does not exist yet) </span><br><span class="hljs-comment">#and a child of this node with key &quot;province&quot; and content: &#123;&quot;name&quot;: &quot;Anhui&quot;&#125;</span><br>$ curl -X PUT &#x27;https://rest-apidemo.firebaseio.com/country/province.json&#x27; -d &#x27;&#123;<span class="hljs-string">&quot;name&quot;</span>: <span class="hljs-string">&quot;Anhui&quot;</span>&#125;&#x27; <br>$ curl -X PUT &#x27;https://rest-apidemo.firebaseio.com/country.json&#x27; -d &#x27;&#123;<span class="hljs-string">&quot;province&quot;</span>: &#123;<span class="hljs-string">&quot;name&quot;</span>: <span class="hljs-string">&quot;Anhui&quot;</span>&#125;&#125;&#x27;<br></code></pre></td></tr></table></figure>

<h4 id="POST-add-new-value-to-a-given-node"><a href="#POST-add-new-value-to-a-given-node" class="headerlink" title="POST: add new value to a given node"></a>POST: add new value to a given node</h4><figure class="highlight maxima"><table><tr><td class="code"><pre><code class="hljs maxima">Automatically generates a <span class="hljs-built_in">new</span> <span class="hljs-built_in">key</span> &amp; <span class="hljs-keyword">then</span> stores the value <span class="hljs-keyword">for</span> the <span class="hljs-built_in">new</span> <span class="hljs-built_in">key</span><br>由于对于添加的数据，其自动生成一个<span class="hljs-built_in">key</span>，所以保证不会重写数据(In contrast, PUT will simply overwrite the <span class="hljs-built_in">key</span>)<br> <br>$ curl -X POST &#x27;https://<span class="hljs-built_in">rest</span>-apidemo.firebaseio.com/country.json&#x27; -d &#x27;&#123;<span class="hljs-string">&quot;province&quot;</span>: &#123;<span class="hljs-string">&quot;name&quot;</span>: <span class="hljs-string">&quot;Anhui&quot;</span>&#125;&#125;&#x27;<br>$ curl -X POST &#x27;https://<span class="hljs-built_in">rest</span>-apidemo.firebaseio.com/country.json&#x27; -d  &#x27;<span class="hljs-string">&quot;Anhui&quot;</span>&#x27;<br></code></pre></td></tr></table></figure>

<h4 id="PATCH-upsert-a-value-to-a-given-node"><a href="#PATCH-upsert-a-value-to-a-given-node" class="headerlink" title="PATCH:  upsert a value to a given node"></a>PATCH:  upsert a value to a given node</h4><figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql">Performs the <span class="hljs-keyword">update</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">value</span> already <span class="hljs-keyword">exists</span>  -更新节点<br>Otherwise, it inserts the <span class="hljs-keyword">new</span> <span class="hljs-keyword">value</span>          -插入新节点<br><span class="hljs-keyword">PATCH</span>本质上是一个深度搜索符合节点的条件进行更新或者添加)<br><br>curl -X <span class="hljs-keyword">PATCH</span> <span class="hljs-string">&#x27;https://rest-apidemo.firebaseio.com/country.json&#x27;</span> -d <span class="hljs-string">&#x27;&#123;&quot;province&quot;: &#123;&quot;name&quot;: &quot;Hubei&quot;&#125;&#125;&#x27;</span><br><br><br>好处:相比于PUT（对于已经存在的节点进行全局的修改），本质上来说<span class="hljs-keyword">PATCH</span>实现的是对该节点的局部更新<br>注意:当需要更新的节点没有子节点（仅仅是一个<span class="hljs-keyword">key</span>-<span class="hljs-keyword">value</span>格式），无法使用<span class="hljs-keyword">PATCH</span>，因为 -d 参数后需要传入键值对的格式<br>curl -X <span class="hljs-keyword">PATCH</span> <span class="hljs-string">&#x27;https://rest-apidemo.firebaseio.com/spouse.json&#x27;</span> -d <span class="hljs-string">&#x27;&quot;Sam&quot;&#x27;</span> <span class="hljs-comment"># fail</span><br>&#123;<br>  <span class="hljs-string">&quot;error&quot;</span> : <span class="hljs-string">&quot;Invalid data; couldn&#x27;t parse JSON object. Are you sending a JSON object with valid key names?&quot;</span><br>&#125;<br>curl -X PUT <span class="hljs-string">&#x27;https://rest-apidemo.firebaseio.com/spouse.json&#x27;</span> -d <span class="hljs-string">&#x27;&quot;Sam&quot;&#x27;</span> <span class="hljs-comment"># success</span><br></code></pre></td></tr></table></figure>
<h4 id="DELETE-delete-a-node"><a href="#DELETE-delete-a-node" class="headerlink" title="DELETE:  delete a node"></a>DELETE:  delete a node</h4><figure class="highlight sas"><table><tr><td class="code"><pre><code class="hljs sas">curl -<span class="hljs-meta">X</span> <span class="hljs-meta">DELETE</span> <span class="hljs-string">&#x27;https://rest-apidemo.firebaseio.com/spouse.json&#x27;</span><br></code></pre></td></tr></table></figure>

<h4 id="从数据增加、减少上对PUT、PATCH和POST进行比较"><a href="#从数据增加、减少上对PUT、PATCH和POST进行比较" class="headerlink" title="从数据增加、减少上对PUT、PATCH和POST进行比较"></a>从数据增加、减少上对PUT、PATCH和POST进行比较</h4><p>PUT可能造成数据的增加（添加新节点下的value）和减少（overwrite整个节点导致其内嵌的节点被删除）；<br>PATCH可能造成数据的增加（添加新节点下的value)和减少（update节点的用更少的数据去取代原先的数据）<br>POST一定会造成数据的增加（因为会自动加上key）；</p>
<h4 id="data-querying-by-RESful-API"><a href="#data-querying-by-RESful-API" class="headerlink" title="data querying by RESful API"></a>data querying by RESful API</h4><ul>
<li>orderBy=”$key”  </li>
<li>orderBy=”<path-to-child-key>“</li>
<li>orderBy=”$value”  需要在Realtime database的Rules先声明</li>
<li>startAt/endAt</li>
<li>equalTo</li>
<li>limitToFirst/limitToLast </li>
</ul>
<p>Specified in database rules:<br><a href="https://firebase.google.com/docs/database/security/indexing-data">https://firebase.google.com/docs/database/security/indexing-data</a></p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust">curl -X GET <span class="hljs-symbol">&#x27;https</span>:<span class="hljs-comment">//rest-apidemo.firebaseio.com/scores.json?orderBy=&quot;$key&quot;&amp;equalTo=&quot;1&quot;&#x27;</span><br>curl -X GET <span class="hljs-symbol">&#x27;https</span>:<span class="hljs-comment">//rest-apidemo.firebaseio.com/scores.json?orderBy=&quot;$key&quot;&amp;startAt=&quot;1&quot;&#x27;</span><br>curl -X GET <span class="hljs-symbol">&#x27;https</span>:<span class="hljs-comment">//rest-apidemo.firebaseio.com/scores.json?orderBy=&quot;$value&quot;&#x27;</span><br><br>Orders ascendingly:<br>null -&gt; <span class="hljs-literal">false</span> -&gt;<span class="hljs-literal">true</span> -&gt; number -&gt; string -&gt; object<br></code></pre></td></tr></table></figure>


<h2 id="REST-API-in-Python"><a href="#REST-API-in-Python" class="headerlink" title="REST API in Python"></a>REST API in Python</h2><figure class="highlight haskell"><table><tr><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> requests<br><br><span class="hljs-title">url</span> = &#x27;xxxx&#x27;<br><span class="hljs-class"><span class="hljs-keyword">data</span> = &#x27;xxx&#x27;</span><br><br><span class="hljs-title">requests</span>.get(url) <br><span class="hljs-title">requests</span>.put(url, <span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br><span class="hljs-title">requests</span>.patch(url, <span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br><span class="hljs-title">requests</span>.delete(url)<br><span class="hljs-title">requests</span>.post(url, <span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br></code></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>Commands</tag>
        <tag>Tools</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch-BigGraph 从实体嵌入到边得分</title>
    <url>/2020/10/23/From-embedding-to-scores/</url>
    <content><![CDATA[<h1 id="From-entity-embeddings-to-edge-scores"><a href="#From-entity-embeddings-to-edge-scores" class="headerlink" title="From entity embeddings to edge scores"></a>From entity embeddings to edge scores</h1><p>The goal of training is to embed each entity in ℝ^D so that the embeddings of two entities are a good proxy to predict whether there is a relation of a certain type between them.<br>训练的目的是将每个实体嵌入到D维的向量空间中，以便两个实体的嵌入可以很好地预测它们之间是否存在某种类型的关系。</p>
<p>To be more precise, the goal is to learn an embedding for each entity and a function for each relation type that takes two entity embeddings and assigns them a score, with the goal of having positive relations achieve higher scores than negative ones.<br>更确切地说，PBG的目标是学习每个实体的嵌入和每种关系类型的函数，该函数接受两个实体嵌入并为其分配一个分数，目的是使正面关系比负面关系获得更高的分数。</p>
<blockquote>
<p>key point: 实体-&gt;D维向量 关系-&gt;函数</p>
</blockquote>
<p>All the edges provided in the training set are considered positive instances. In order to perform training, a set of negative edges is needed as well. These are not provided by the user but instead generated by the system during training (Negative sampling), usually by fixing the left-hand side entity and the relation type and sampling a new right-hand side entity, or vice versa. This sampling scheme makes sense for large sparse graphs, where there is a low probability that edges generated this way are true positives edges in the graph.<br>训练集中提供的所有边被视为正例。为了执行训练，还需要一些负边（负样本）。 这些不是由用户提供的，而是由系统在训练过程中生成的（负采样），通常是通过固定左侧实体和关系类型并采样新的右侧实体，反之亦然。 对于大的稀疏图，这种采样方案是有意义的，因为在这样的情况下，以这种方式生成的边为图中的正例的可能性很小。</p>
<p>A priori, entity embeddings could take any value in ℝ^D. Although, in some cases (for example when restricting them to be within a certain ball, or when comparing them using cosine distance), their “angle” will have greater importance than their norm.<br>先验实体嵌入可以在D维空间中取任何值。但是在某些情况下（例如，当将它们限制在某个球内或使用余弦距离进行比较时），它们的“角度”比范数更具有意义。</p>
<p>Per-relation scoring functions, however, must be expressible in a specific form (the most common functions in the literature can be converted to such a representation). In the current implementation, they are only allowed to transform the embedding of one of the two sides, which is then compared to the un-transformed embedding of the other side using a generic symmetric comparator function, which is the same for all relations. Formally, for left- and right-hand side entities 𝑥 and 𝑦 respectively, and for a relation type 𝑟, the score is:<center>𝑓𝑟(𝜃𝑥,𝜃𝑦)=𝑐(𝜃𝑥,𝑔𝑟(𝜃𝑦))</center><br>where 𝜃𝑥 and 𝜃𝑦 are the embeddings of 𝑥 and 𝑦 respectively, 𝑓𝑟 is the scoring function for 𝑟, 𝑔𝑟 is the operator for 𝑟 and 𝑐 is the comparator.</p>
<!-- $$f_{r}\left(\theta_{x}, \theta_{y}\right)=c\left(\theta_{x}, g_{r}\left(\theta_{y}\right)\right) -->
<p>然而，对于每个关系的得分计算函数，必须以特定的形式表现出来（文献中最常见的函数可以转换为这种表示形式）。在目前的实现中，只允许对一侧的实体进行嵌入转换，通过使用通用的对称比较器将其与另一侧未转换的实体嵌入进行比较，这样的操作对于所有的关系类型都是一样的。形式上，分别对于左侧和右侧实体𝑥和,，以及对于关系类型𝑟，得分为：𝑓𝑟(𝜃𝑥,𝜃𝑦)=𝑐(𝜃𝑥,𝑔𝑟(𝜃𝑦)), 其中，𝜃𝑥和𝜃𝑦分别是𝑥和𝑦的嵌入，𝑓𝑟是关系𝑟的评分函数，𝑔𝑟是关系r的算子，𝑐是比较器。</p>
<p>Under “normal” circumstances (the so-called “standard” relations mode) the operator is solely applied to the right-hand side entities. This is not the case when using dynamic relations. Applying the operator to both sides would oftentimes be redundant. Also, preferring one side over the other allows to break the symmetry and capture the direction of the edge.<br>在通常情况下(或者说是标准的关系模式中)，算子仅仅是适用于右侧实体。但是在PBG的动态关系中则不是这样。通常，将算子运用在两侧的实体嵌入上是多余的，并且倾向于选择某一侧的举动可以很好的打破对称性并且捕捉到边的方向。</p>
<h2 id="Embeddings"><a href="#Embeddings" class="headerlink" title="Embeddings"></a>Embeddings</h2><p>Embeddings live in a 𝐷-dimensional real space, where 𝐷 is determined by the dimension configuration parameter.<br>嵌入是一个𝐷维实空间的向量，其中𝐷的大小由配置参数确定。</p>
<p>Normally, each entity has its own embedding, which is entirely independent from any other entity’s embedding. When using featurized entities however this works differently, and an entity’s embedding will be the average of the embeddings of its features.<br>通常的，每一个实体都有属于自己的完全和其他实体嵌入独立的嵌入。然而，当使用特征化的实体时，情况有所不同。此时实体的嵌入为其特征嵌入的平均值。</p>
<p>If the max_norm configuration parameter is set, embeddings will be projected onto the unit ball with radius max_norm after each parameter update.<br>如果设置了max_norm配置参数，则在每次更新参数后，嵌入将会被投影到半径为max_norm的单位球上。</p>
<blockquote>
<p>🤔️ 暂时没看懂。</p>
</blockquote>
<p>To add a new type of embedding, one needs to subclass the torchbiggraph.model.AbstractEmbedding class.<br>要添加一种新型的嵌入，需要将torchbiggraph.model.AbstractEmbedding类作为子类。</p>
<blockquote>
<p>🤔️ 暂时没看懂。</p>
</blockquote>
<h2 id="Global-embeddings"><a href="#Global-embeddings" class="headerlink" title="Global embeddings"></a>Global embeddings</h2><p>When the global_emb configuration option is active, each entity’s embedding will be translated by a vector that is specific to each entity type (and that is learned at the same time as the embeddings).<br>当global_emb配置参数设置为True的时候，每个实体的嵌入将会被表示成一个向量，该向量对于每种实体类型都是特定的。（并且该向量是与嵌入同时学习得到的）</p>
<blockquote>
<p>🤔️ 暂时没看懂。</p>
</blockquote>
<h2 id="Operators"><a href="#Operators" class="headerlink" title="Operators"></a>Operators</h2><p>The operators that are currently provided are:<br>• none, no-op, which leaves the embeddings unchanged;<br>• translation, which adds to the embedding a vector of the same dimension;<br>• diagonal, which multiplies each dimension by a different coefficient (equivalent to multiplying by a diagonal matrix);<br>• linear, which applies a linear map, i.e., multiplies by a full square matrix<br>• affine, which applies a affine transformation, i.e., linear followed by translation.<br>• complex_diagonal, which interprets the 𝐷-dimensional real vector as a 𝐷/2-dimensional complex vector (𝐷 must be even; the first half of the vector are the real parts, the second half the imaginary parts) and then multiplies each entry by a different complex parameter, just like diagonal.<br>当前提供的算子有:<br>• 无，无操作，使嵌入保持不变；<br>• 平移算子，将相同尺寸的向量添加到嵌入中；<br>• 对角算子，将每个维度乘以不同的系数（相当于乘以对角矩阵）；<br>• 线性算子，运用一个线性映射，例如，讲嵌入和一个全方阵点乘；<br>• 仿射算子，应用仿射变换，即线性变换后再进行平移；<br>• 复对角算子，将D维实向量转化为D/2维复矢量（𝐷必须是偶数；矢量的前半部分是实数部分，后半部分是虚数部分），然后将每个项乘以不同的复数参数，就像对角算子一样。</p>
<p>All the operators’ parameters are learned during training.<br>训练过程中将学习所有算子的参数。</p>
<p>To define an additional operator, one must subclass the torchbiggraph.model.AbstractOperator class (or the torchbiggraph.model.AbstractDynamicOperator one when using dynamic relations; their docstrings explain what must be implemented) and decorate it with the torchbiggraph.model.OPERATORS.register_as() decorator (respectively the torchbiggraph.model.DYNAMIC_OPERATORS.register_as() one), specifying a new name that can then be used in the config to select that comparator. All of the above can be done inside the config file itself.<br>如果要自定义新的算子，需要实现torchbiggraph.model.AbstractOperator的子类（动态关系情况下实现torchbiggraph.model.AbstractDynamicOperator子类，docstrings解释了必须实现什么）并且在torchbiggraph.model.OPERATORS.register_as()装饰器中注册（或者torchbiggraph.model.DYNAMIC_OPERATORS.register_as() ）指定一个新名称，然后在配置中使用该名称来选择比较器。上述所有操作都可以在配置文件内部完成。</p>
<blockquote>
<p>🤔️ 暂时没看懂。</p>
</blockquote>
<h2 id="Comparators"><a href="#Comparators" class="headerlink" title="Comparators"></a>Comparators</h2><p>The available comparators are:<br>• dot, the dot-product, which computes the scalar or inner product of the two embedding vectors;<br>• cos, the cos distance, which is the cosine of the angle between the two vectors or, equivalently, the dot product divided by the product of the vectors’ norms.<br>• l2, the negative L2 distance, a.k.a. the Euclidean distance (negative because smaller distances should get higher scores).<br>• squared_l2, the negative squared L2 distance.<br>当前提供的比较器有：<br>• 点乘，计算两个实体嵌入向量的标量或内积；<br>• 余弦距离，两个实体嵌入向量的余弦夹角，或者等效的说是 dot(a,b)/(sqrt(a^2)*sqrt(a^2))<br>• 负L2距离，又称欧几里得距离（使用负的二范数是因为真正比较的是两者的相似度，较小的距离应获得更高的分数，这里的分数其实就类似于相似度)<br>• L2的负平方距离。</p>
<p>Custom comparators need to extend the torchbiggraph.model.AbstractComparator class (its docstring explains how) and decorate it with the torchbiggraph.model.COMPARATORS.register_as() decorator, specifying a new name that can then be used in the config to select that comparator. All of the above can be done inside the config file itself.<br>自定义比较器需要扩展torchbiggraph.model.AbstractComparator类（其文档字符串说明方式），并使用torchbiggraph.model.COMPARATORS.register_as（）装饰器对其进行修饰，并指定一个新名称，该名称随后可在配置中用于选择该比较器。以上所有操作均可在配置文件本身中完成。</p>
<h2 id="Bias"><a href="#Bias" class="headerlink" title="Bias"></a>Bias</h2><p>If the bias configuration key is in use, then the first coordinate of the embeddings will act as a bias in the comparator computation. This means that the comparator will be computed on the last 𝐷−1 entries of the vectors only, and then both the first entries of the two vectors will be added to the result.<br>如果bias参数在配置文件中设置为True，那么嵌入的第一个坐标将充当比较器计算中的偏置。这意味着比较器将仅在向量得第二维到第D维上进行计算，然后将两个向量的第一维都直接被添加到结果中。</p>
<h2 id="Coherent-sets-of-configuration-parameters"><a href="#Coherent-sets-of-configuration-parameters" class="headerlink" title="Coherent sets of configuration parameters"></a>Coherent sets of configuration parameters</h2><p>While the parameters described in this chapter are exposed as uncoupled knobs in the configuration file (to more closely match the implementation, and to allow for more flexible tuning), some combinations of them are more sensible than others.<br>尽管本章中描述的参数在配置文件中显示为未耦合的旋钮（为了更近似匹配实现效果，并允许更灵活的调优），但它们中的某些组合比其他组合更合理。</p>
<p>Apart from the default one, the following configuration has been found to work well: init_scale = 0.1, comparator = dot, bias = true, loss_fn = logistic, lr = 0.1.<br>除默认配置外，还发现以下配置可以正常运行：init_scale = 0.1，comparator = dot, bias = true, loss_fn = logistic, lr = 0.1。</p>
<h2 id="Interpreting-the-scores"><a href="#Interpreting-the-scores" class="headerlink" title="Interpreting the scores"></a>Interpreting the scores</h2><p>The scores will be tuned to have different meaning and become more suitable for certain applications based on the loss function used during training. Common options include ranking what other entities may be related to a given entity, determining the probability that a certain relation exists between two given entities, etc.<br>根据训练过程中使用的损失函数，得分将被调整为具有不同的含义，并且变得更适合某些特定的应用。常见选项包括对其他实体可能与给定实体相关的等级进行排名、确定两个给定实体之间存在某种关系的可能性等指标。</p>
<p>源地址：<br><a href="https://torchbiggraph.readthedocs.io/en/latest/scoring.html">https://torchbiggraph.readthedocs.io/en/latest/scoring.html</a></p>
]]></content>
      <tags>
        <tag>PyTorch-BigGraph</tag>
      </tags>
  </entry>
  <entry>
    <title>JSON-review</title>
    <url>/2020/09/07/JSON-review/</url>
    <content><![CDATA[<h2 id="JSON概述"><a href="#JSON概述" class="headerlink" title="JSON概述"></a>JSON概述</h2><p><strong>JSON</strong>（<strong>J</strong>ava<strong>S</strong>cript <strong>O</strong>bject <strong>N</strong>otation)，由道格拉斯·克罗克福特构想和设计、轻量级的数据交换语言,该语言以易于让人阅读的文字为基础，用来<strong>传输由属性值或者序列性的值组成的数据对象</strong>。尽管JSON是JavaScript的一个子集，但JSON是独立于语言的文本格式。</p>
<ul>
<li>Light-weight data exchange format<br>Much simpler than XML; Language-independent; Inspired by the syntax of JavaScript object literals</li>
<li>Some differences from JavaScript objects<br><strong>String in JSON must be double-quoted</strong>; Okay to single-quote in JavaScript (&amp; Python)</li>
<li>JSON is case-sensitive</li>
</ul>
<p>一句话表达就是，JSON是一种格式，基于文本，优于轻量，用于交换数据，由于其用文本格式的保存方式，所以一般也叫JSON字符串。</p>
<h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><ul>
<li>value = string | number | object | array | true | false | null</li>
<li>object = {} | { members }<ul>
<li>members = pair | pair, members </li>
<li>pair = string : value</li>
</ul>
</li>
<li>array = [] | [ elements ]<ul>
<li>elements = value | value, elements</li>
</ul>
</li>
</ul>
<p>一般情况下，JSON大多为一个array，array里的每一个元素为一个object，object可以理解为字典<br>JSON 在线检查器:   <a href="https://www.json.cn/">https://www.json.cn/</a></p>
<h2 id="Python中使用JSON"><a href="#Python中使用JSON" class="headerlink" title="Python中使用JSON"></a>Python中使用JSON</h2><h3 id="encode-decode"><a href="#encode-decode" class="headerlink" title="encode/decode"></a>encode/decode</h3><p>JSON操作主要分为两个：</p>
<ol>
<li>encode，将Python对象转化为JSON文本，主要函数 dump( ) dumps( )</li>
<li>decode，将JSON文本转化为Python对象，主要函数 load( ) loads( )</li>
</ol>
<h3 id="转化规则"><a href="#转化规则" class="headerlink" title="转化规则"></a>转化规则</h3><table>
<thead>
<tr>
<th align="center">Python</th>
<th align="center">JSON</th>
</tr>
</thead>
<tbody><tr>
<td align="center">list/tuple</td>
<td align="center">array</td>
</tr>
<tr>
<td align="center">dict</td>
<td align="center">object</td>
</tr>
<tr>
<td align="center">None</td>
<td align="center">null</td>
</tr>
<tr>
<td align="center">True</td>
<td align="center">true</td>
</tr>
<tr>
<td align="center">False</td>
<td align="center">false</td>
</tr>
<tr>
<td align="center">‘abc’</td>
<td align="center">“abc”</td>
</tr>
</tbody></table>
<p>关于Python字典转JSON文本的说明：</p>
<ul>
<li>Keys in Python can be number, string, or tuple. </li>
<li>Number is also converted to string.</li>
<li>But tuple (with two or more components) is not acceptable by dumps()/dump()</li>
</ul>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json <span class="hljs-comment"># json库为Python自带的标准库</span><br><br><span class="hljs-comment"># JSON encoder Python object =&gt; JSON document   </span><br>json.dumps([<span class="hljs-number">3</span>, <span class="hljs-string">&#x27;abc&#x27;</span>, <span class="hljs-literal">True</span>, <span class="hljs-literal">None</span>])  <span class="hljs-comment"># &#x27;[3, &quot;abc&quot;, true, null]&#x27;</span><br><br><span class="hljs-comment"># Python object =&gt; JSON file</span><br>data = &#123;<span class="hljs-string">&#x27;name&#x27;</span>:<span class="hljs-string">&#x27;Sam&#x27;</span>,<span class="hljs-string">&#x27;age&#x27;</span>:<span class="hljs-number">23</span>&#125;<br><span class="hljs-keyword">with</span> open(<span class="hljs-string">&#x27;user_info.json&#x27;</span>,<span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> out_file:<br>    json.dump(data,out_file)<br><br><span class="hljs-comment">## JSON decoder </span><br>json.loads(<span class="hljs-string">&#x27;[&quot;foo&quot;, &#123;&quot;bar&quot;:[&quot;baz&quot;, null, 1.0, 2]&#125;]&#x27;</span>) <span class="hljs-comment"># [&#x27;foo&#x27;, &#123;&#x27;bar&#x27;: [&#x27;baz&#x27;, None, 1.0, 2]&#125;]</span><br><br><span class="hljs-comment"># JSON file =&gt; Python object</span><br><span class="hljs-keyword">with</span> open(<span class="hljs-string">&#x27;user_info.json&#x27;</span>,<span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> in_file:<br>    data = json.load(in_file)<br></code></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>Python</tag>
        <tag>File Format</tag>
      </tags>
  </entry>
  <entry>
    <title>ssh连接远程服务器中途断开</title>
    <url>/2020/10/23/Linux-screen/</url>
    <content><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>Mac机器使用ssh操作远程服务器的途中,由于网络波动,断开连接,休眠等一系列奇葩原因导致终端<code>client_loop: send disconnect: Broken pipe</code>,<br>在这样的情况下，远程服务器也跟着掉线。</p>
<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>即使本地Mac掉线，远程服务器仍旧保持工作</p>
<h2 id="解决方式-Screen"><a href="#解决方式-Screen" class="headerlink" title="解决方式: Screen"></a>解决方式: Screen</h2><ol>
<li><p>连接远程服务器<br><code>binzhang@MacBin ~$  ssh username@servername</code></p>
</li>
<li><p>创建名为cskg的screen,  此时screen被创建，之后的代码或者各种工作在该screen下操作<br><code>[servername ~]$ screen -S cskg </code></p>
</li>
<li><p>万一本地机器掉线，重新连接远程服务器，随后使用screen -ls 查看之前创建的screen信息</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><code class="hljs routeros">[servername ~]$<span class="hljs-built_in"> screen </span>-ls  <br>显示信息：<br>There is a<span class="hljs-built_in"> screen </span>on:<br>        157586.cskg  (Attached)<br>1 Socket <span class="hljs-keyword">in</span> /var/run/screen/xx<br></code></pre></td></tr></table></figure></li>
<li><p>使用-r回到之前服务器工作的screen<br><code>screen -r 157586</code></p>
</li>
</ol>
<h3 id="Screen常见问题"><a href="#Screen常见问题" class="headerlink" title="Screen常见问题"></a>Screen常见问题</h3><p>使用<code>screen -ls</code>, 显式当前状态为Attached，但并没有没有用户登陆该screen。screen此时正常状态应该为(Detached)使用 <code>screen -r &lt;session-id&gt;</code>无法连接，</p>
<p>解决方案:<code>screen -D  -r ＜session-id&gt;</code>,-D -r  表示先踢掉前用户，再登陆。</p>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Network File System</title>
    <url>/2020/09/11/NFS/</url>
    <content><![CDATA[<h3 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h3><p>The Network File System (NFS) is a client/server application that lets a computer user view and optionally store and update files on a remote computer as though they were on the user’s own computer. NFS is a distributed file system(DFS).</p>
<h4 id="Client-server-architecture"><a href="#Client-server-architecture" class="headerlink" title="Client/server architecture"></a>Client/server architecture</h4><p><img src="/image/C-S.png"></p>
<p>Advantage</p>
<ul>
<li>Easy sharing of data across clients</li>
<li>Centralized administration</li>
<li>Security</li>
</ul>
<p>Disadvantage:</p>
<ul>
<li>Network overhead</li>
<li>More components to fail</li>
</ul>
<h3 id="DFS"><a href="#DFS" class="headerlink" title="DFS"></a>DFS</h3><h4 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h4><p><img src="/image/DFS.png"></p>
<h4 id="Stateless-无状态协议"><a href="#Stateless-无状态协议" class="headerlink" title="Stateless  - 无状态协议"></a>Stateless  - 无状态协议</h4><ul>
<li>Server does not keep track of states of clients<ul>
<li>Which files are currently open at which clients</li>
<li>Current position/offset of file</li>
<li>Which clients have read/cached which blocks</li>
</ul>
</li>
<li>Requests from clients must make sure:<ul>
<li>the server can deliver all the information needed to complete the requests</li>
<li>do not rely on previous requests</li>
</ul>
</li>
</ul>
<h4 id="file-handle"><a href="#file-handle" class="headerlink" title="file handle"></a>file handle</h4><ul>
<li>Volume (file system) identifier<ul>
<li>Which volume? (e.g. partition C or D if NTFS)</li>
</ul>
</li>
<li>Inode number<ul>
<li>Which file in the volume?</li>
</ul>
</li>
<li>Generation number<ul>
<li>Needed since inode number may be reused at the server (e.g., after file has been deleted by other clients)</li>
</ul>
</li>
</ul>
<p><strong>Client uses file handle to communication with server</strong><br><img src="/image/file-handle.png"></p>
<h4 id="RPC-Remote-procedure-call"><a href="#RPC-Remote-procedure-call" class="headerlink" title="RPC - Remote procedure call"></a>RPC - Remote procedure call</h4><ul>
<li>Remote server publishes a set of procedures, for example f(args)<ul>
<li>example: NFSPROC_LOOKUP for lookup file handle ,read, write, create, remove, etc.</li>
</ul>
</li>
<li>In making RPC calls,(客户端调用)<ul>
<li>Client notifies remote server of executing f &amp; sends over arguments args for f</li>
<li>Server executes f(args) =&gt; results </li>
<li>Server sends back results</li>
</ul>
</li>
</ul>
<h3 id="Operations-on-remote-file"><a href="#Operations-on-remote-file" class="headerlink" title="Operations on remote file"></a>Operations on remote file</h3><p>All CRUD operations use <u>file handle</u></p>
<h4 id="Read"><a href="#Read" class="headerlink" title="Read"></a>Read</h4><p><u>First obtain file handle via lookup</u></p>
<ul>
<li>File handle for the root directory may be obtained via the mount protocol</li>
</ul>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql">Look up &quot;/foo/more/bar.txt&quot;<br>– First, use / file handle <span class="hljs-keyword">to</span> obtain foo<span class="hljs-string">&#x27;s handle  </span><br><span class="hljs-string">	File handle for the root directory may be obtained via the mount protocol</span><br><span class="hljs-string">– Next, use foo&#x27;</span>s handle <span class="hljs-keyword">to</span> obtain mor<span class="hljs-string">e&#x27;s handle</span><br><span class="hljs-string">– Finally, use more&#x27;</span>s handle <span class="hljs-keyword">to</span> obtain bar.txt handle<br></code></pre></td></tr></table></figure>

<p><u>Then use file handle to read data</u></p>
<figure class="highlight livecodeserver"><table><tr><td class="code"><pre><code class="hljs livecodeserver">NFSPROC_READ(<span class="hljs-built_in">file</span> handle, <span class="hljs-built_in">offset</span>, count)<br>- <span class="hljs-built_in">offset</span> here is explicit<br>– Return: data + <span class="hljs-built_in">file</span> attributes<br>– File attributes <span class="hljs-built_in">include</span> modification <span class="hljs-built_in">time</span>, useful <span class="hljs-keyword">for</span> client-size cache validation<br><br>Compared <span class="hljs-built_in">to</span> <span class="hljs-built_in">local</span> <span class="hljs-built_in">file</span> <span class="hljs-keyword">system</span><br>- n = <span class="hljs-built_in">read</span>(fd, buffer, size)<br>– n is <span class="hljs-keyword">the</span> <span class="hljs-built_in">number</span> <span class="hljs-keyword">of</span> <span class="hljs-keyword">bytes</span> actually <span class="hljs-built_in">read</span><br></code></pre></td></tr></table></figure>


<h4 id="Write"><a href="#Write" class="headerlink" title="Write"></a>Write</h4><p><u>First obtain file handle via lookup</u></p>
<p><u>Then use file handle to write data</u></p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><code class="hljs routeros">NFSPROC_WRITE(file handle, offset, count, data)<br>– Return: file attributes<br>–<span class="hljs-built_in"> Note </span>again explicit offset is specified <span class="hljs-keyword">in</span> the call<br><br>Compared <span class="hljs-keyword">to</span> local file system<br>– n = write(fd, buffer, size)<br>– Offset is again implicit (current position)<br></code></pre></td></tr></table></figure>

<h4 id="Create"><a href="#Create" class="headerlink" title="Create"></a>Create</h4><figure class="highlight reasonml"><table><tr><td class="code"><pre><code class="hljs reasonml"><span class="hljs-constructor">NFSPROC_CREATE(<span class="hljs-params">directory</span> <span class="hljs-params">file</span> <span class="hljs-params">handle</span>, <span class="hljs-params">name</span> <span class="hljs-params">of</span> <span class="hljs-params">file</span> <span class="hljs-params">in</span> <span class="hljs-params">the</span> <span class="hljs-params">directory</span>, <span class="hljs-params">attributes</span>)</span><br>– Return: file handle<br></code></pre></td></tr></table></figure>

<h4 id="Remove"><a href="#Remove" class="headerlink" title="Remove"></a>Remove</h4><figure class="highlight reasonml"><table><tr><td class="code"><pre><code class="hljs reasonml"><span class="hljs-constructor">NFSPROC_REMOVE(<span class="hljs-params">directory</span> <span class="hljs-params">file</span> <span class="hljs-params">handle</span>, <span class="hljs-params">name</span> <span class="hljs-params">of</span> <span class="hljs-params">file</span> <span class="hljs-params">to</span> <span class="hljs-params">be</span> <span class="hljs-params">removed</span>)</span><br>– Return: Nothing<br></code></pre></td></tr></table></figure>

<h3 id="Failures"><a href="#Failures" class="headerlink" title="Failures"></a>Failures</h3><h4 id="Cases"><a href="#Cases" class="headerlink" title="Cases"></a>Cases</h4><p>Case1: request lost<br>Case2: Server down<br>Case3: Reply lost on way back from Sever</p>
<h3 id="More-details"><a href="#More-details" class="headerlink" title="More details"></a>More details</h3><p>学习资源: <a href="https://tools.ietf.org/html/rfc1094">https://tools.ietf.org/html/rfc1094</a></p>
]]></content>
      <tags>
        <tag>Operating System</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch-BigGraph  批准备</title>
    <url>/2020/10/27/PBG-Batch-preparation/</url>
    <content><![CDATA[<h1 id="PBG-批准备"><a href="#PBG-批准备" class="headerlink" title="PBG 批准备"></a>PBG 批准备</h1><p>This section presents how the training data is prepared and organized in batches before the loss is calculated and optimized on each of them.<br>本节介绍了如何在计算和优化损失之前如何对训练数据进行准备和组织。</p>
<p>Training proceeds by iterating over the edges, through various nested loops. The outermost one walks through so-called epochs. Each epoch is independent and essentially equivalent to every other one. Their goal is to repeat the inner loop until convergence. Each epoch visits all the edges exactly once. The number of epochs is specified in the num_epochs configuration parameter.<br>训练过程通过不断嵌套循环各条边来进行。所有边都被训练过一次叫做一个epoch，每一个epoch都是互相独立且平等的。他们的目标都是为了重复训练边最终达到收敛的效果。每一个epoch只访问每一条一次。在配置文件中的num_epochs指定epoch数目。</p>
<p>The edges are partitioned into edge sets (one for each directory of the edge_paths configuration key) and, within each epoch, the edge sets are traversed in order.<br>图中的边被划分为边集合的形式（edge_paths目录下的每个文件表示一个边集合），在每一个epoch下，这些边集合按照顺序访问遍历。</p>
<p>When iterating over an edge set, each of its buckets is first divided into equally sized chunks: each chunk spans a contiguous interval of edges (in the order they are stored in the files) and the number of chunks can be tweaked using the num_edge_chunks configuration key. The training first operates on the all the first chunks of all buckets, then on all of their second chunks, and so on.<br>当在某一个边集合上进行迭代训练的时候，每一个桶（一个边集合）首先被划分成相同大小的chunks，每一个chunk跨度连续的边间隔（按它们存储在文件中的顺序），可以在配置文件中使用num_edge_chunks来设置chunk的大小。训练首先对桶中的第一个chunk进行训练，然后对第二个，以此类推。</p>
<p>Next, the algorithm iterates over the buckets. The order in which buckets are processed depends on the value of the bucket_order configuration key. In addition to a random permutation, there are methods that try to have successive buckets share a common partition: this allows for that partition to be reused, thus allowing it to be kept in memory rather than being unloaded and another one getting loaded in its place. (In distributed mode, the various trainer processes operate on the buckets at the same time, thus the iteration is managed differently).<br>接下来，该算法迭代训练每一个桶。桶训练顺序取决于配置文件中的bucket_order。除了随机排列之外，还有一些方法尝试使连续的存储桶共享一个公共分区，也就是说允许这个共享分区被重用。通过这样其允许这这些边数据保存在内存中而不是被卸载然后被其他的边数据取代。（在分布式模式下，各种训练器进程同时在存储桶上运行训练，因此对迭代的管理不同）。</p>
<p>Once the trainer has fixed a given chunk and a certain bucket, its edges are finally loaded from disk. When evaluating during training, a subset of these edges is withheld (such subset is the same for all epochs). The remaining edges are immediately uniformly shuffled and then split into equal parts. These parts are distributed among a pool of processes, so that the training can proceed in parallel on all of them at the same time. These subprocesses are “Hogwild!” workers, which do not synchronize their computations or memory accesses. The number of such workers is determined by the workers parameter.<br>一旦训练器获得了一个给定的chunk和bucket，其chuck的边最终从磁盘上加载到内存。在训练之前，边的一部分被保留下载作为之后的评估使用。（对于所有的epoch而言，这些边是一样的），其余的边都作为训练数据，他们被均匀地混洗，然后分成相等的部分分发到进程池里进行训练。因此训练是可以在所有进程上并行运行的。这些子进程被称为“Hogwild!” worker，他们的计算和内存使用并不同步进行。worker的数目在配置文件中的worfers参数指定。</p>
<blockquote>
<p>key point: 这里worker就可以当成是进程数目，几个worker表示可以同时几个进程并行训练数据。</p>
</blockquote>
<p>The way each worker trains on its set of edges depends on whether dynamic relations are in use. The simplest scenario is if they are, in which case the edges are split into contiguous batches (each one having the size specified in the batch_size configuration key, except possibly the last one which could be smaller). Training is then performed on that batch before moving on to the next one.<br>worker训练边集合的方式取决于是否使用动态关系。最简单的情况是，如果边被分成连续的批次，（每个批次具有一样的大小，并且其在配置文件的batch_size中指定，最后一个批次可能会小些。）训练分批次依次进行训练。</p>
<p>When dynamic relations are not in use, however, the loss can only be computed on a set of edges that are all of the same type. Thus the worker first randomly samples a relation type, with probability proportional to the number of edges of that type that are left in the pool. It then takes the first batch_size relations of that type (or fewer, if not enough of them are left), removes them from the pool and performs training on them.<br>当不使用动态关系的时候，损失函数将只能用于同种关系类型的边计算。因此worker首先需要随机的选出某一种关系类型的样本，（挑选概率和池子残留的边成正比）。随后，其采用该类型的第一个batch_size（如果剩余的不多的话，数目可能会很少）,从池子中移除然后进行训练。</p>
<p>源地址：<a href="https://torchbiggraph.readthedocs.io/en/latest/batch_preparation.html">https://torchbiggraph.readthedocs.io/en/latest/batch_preparation.html</a></p>
]]></content>
      <tags>
        <tag>PyTorch-BigGraph</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch-BigGraph 数据模型</title>
    <url>/2020/10/22/PBG-Data-model/</url>
    <content><![CDATA[<h1 id="PBG模型"><a href="#PBG模型" class="headerlink" title="PBG模型"></a>PBG模型</h1><p>PBG operates on directed multi-relation multigraphs, whose vertices are called entities. Each edge connects a source to a destination entity, which are respectively called its left- and right-hand side (shortened to LHS and RHS). Multiple edges between the same pair of entities are allowed. Loops, i.e., edges whose left- and right- hand sides are the same, are allowed as well.<br>PBG作用在有向多关系多图上，其中，图的顶点称为实体。每一条边将源实体连接到目标实体，源实体称为左侧，目标实体称为右侧(缩写为LHS和RHS)同一对实体之间可以有多个边。循环也是允许的，例如，LHS和RHS是相同的边.(A-&gt;B B-&gt;A)</p>
<blockquote>
<p>key point: directed, multi-relation, multigraph </p>
</blockquote>
<p>Each entity is of a certain entity type (one and only one type per entity). Thus, the types partition all the entities into disjoint groups. Similarly, each edge also belongs to exactly one relation type. All edges of a given relation type must have all their left-hand side entities of the same entity type and, similarly, all their right-hand side entities of the same entity type (possibly a different entity type than the left-hand side one). This property means that each relation type has a left-hand side entity type and a right-hand side entity type.<br>每一个实体都具有某种实体类型(每个实体都只有一种类型)。因此，实体的类型将所有的实体划分成不相交的组。类似的，每一条边也只属于个唯一的关系类型。给定关系类型的边的LHS/RHS实体都只属于一个实体类型（左右两侧的实体类型可以不同）。此属性意味着每种关系类型都具有左侧实体类型和右侧实体类型。</p>
<blockquote>
<p>key point: entity, relation, entity type, relation type</p>
</blockquote>
<p><img src="/image/PBG_entity_relation.png"><br>In this graph, there are 14 entities: 5 of the red entity type, 6 of the yellow entity type and 3 of the blue entity type; there are also 12 edges: 6 of the orange relation type (between red and yellow entities), 3 of the purple entity type (between red and blue entities) and 3 of the green entity type (between yellow and blue entities).<br>在该图中，存在14个实体：红色实体类型的5个，黄色实体类型的6个和蓝色实体类型的3个； 还有12条边：橙色关联类型的6个（在红色和黄色实体之间），紫色关联类型的3个（在红色和蓝色实体之间）和绿色关联类型的3个（在黄色和蓝色实体之间）。</p>
<p>In order for PBG to operate on large-scale graphs, the graph is broken up into small pieces, on which training can happen in a distributed manner. This is first achieved by further splitting the entities of each type into a certain number of subsets, called partitions. Then, for each relation type, its edges are divided into buckets: for each pair of partitions (one from the left- and one from the right-hand side entity types for that relation type) a bucket is created, which contains the edges of that type whose left- and right-hand side entities are in those partitions.<br>为了使得PBG在大型图形上运行，图形将被分解成小块，可以在这些小块上进行分布式训练。首先通过将每种类型的实体进一步拆分为一定数量的子集（称为分区）来实现。 然后，对于每种关系类型，它的边被划分成桶：对于每一对分区（对于该关系类型，一个分区是来源于LHS的实体类型，一个分区是来源于RHS的实体类型），一个存储桶被创建，这个存储桶中包含了特定类型的边及对应左、右侧实体类型的实体。</p>
<blockquote>
<p>key point: partition, bucket </p>
</blockquote>
<p><img src="/image/PBG_partition_bucket.png"><br>This graph shows a possible partition of the entities, with red having 3 partitions, yellow having 3, and blue having only one (hence blue is unpartitioned). The edges displayed are those of the orange bucket between the partitions 2 of the red entities and the partition 1 of the yellow entities.<br>此图显示了实体的可能分区，其中红色具有3个分区，黄色具有3个分区，蓝色仅具有1个分区（因此，蓝色是未分区的）。 显示的边是红色实体的分区2和黄色实体的分区1之间的橙色桶的边。</p>
<blockquote>
<p>Note<br>For technical reasons, at the current state all entity types that appear on the left-hand side of some relation type must be divided into the same number of partitions (except unpartitioned entities). The same must hold for all entity types that appear on the right-hand side. In numpy-speak, it means that the number of partitions of all entities must be broadcastable to the same value.<br>出于技术原因，当前出现在某种关系类型左侧的所有实体类型必须划分为相同数量的分区（未分区的实体除外）。对于出现在右侧的所有实体类型，必须保持相同的状态。在numpy中，这意味着所有实体的分区数必须可以扩展到相同的值。<br>举个例子，比如说LHS以及RHS实体类型只有一种，然后关系类型也就是一种，这个时候对LHS划分成2个partition，那么RHS也要划分成2个partition，此时得到的桶个数为2x2=4个</p>
</blockquote>
<p>An entity is identified by its type, its partition and its index within the partition (indices must be contiguous, meaning that if there are 𝑁 entities in a type’s partition, their indices lie in the half-open interval [0,𝑁)). An edge is identified by its type, its bucket (i.e., the partitions of its left- and right-hand side entity types) and the indices of its left- and right-hand side entities in their respective partitions. An edge doesn’t have to specify its left- and right-hand side entity types, because they are implicit in the edge’s relation type.<br>实体由其类型，分区和分区内的索引标识(索引必须是连续的，这意味着如果类型的分区中有N个实体，则其索引位于[0，𝑁)中)。边由其类型，其存储桶（即其左侧和右侧实体类型的分区对）以及其左侧和右侧实体在它们各自分区中的索引来标识。边不必指定其左侧和右侧实体类型，因为它们隐含在边的关系类型中。</p>
<p>Formally, each bucket can be identifies by a pair of integers (𝑖,𝑗), where 𝑖 and 𝑗 are respectively the left- and right-hand side partitions. Inside that bucket, each edge can be identified by a triplet of integers (𝑥,𝑟,𝑦), with 𝑥 and 𝑦 representing respectively the left- and right-hand side entities and 𝑟 representing the relation type. This edge is “interpreted” by first looking up relation type 𝑟 in the configuration, and finding out that it can only have entities of type 𝑒1 on its left-hand side and of type 𝑒2 on its right-hand side. One can then determine the left-hand side entity, which is given by (𝑒1,𝑖,𝑥) (its type, its partition and its index within the partition), and, similarly, the right-hand side one which is (𝑒2,𝑗,𝑦).<br>形式上，每个桶都可以由一对整数（𝑖，𝑗）标识，其中𝑖和𝑗分别是左侧和右侧分区的索引。 在该桶内，每条边都可以由三元组（𝑥，𝑟，𝑦）标识，其中𝑥和𝑦分别表示左侧和右侧实体在各自分区中的索引，而𝑟表示关系类型。 边的解译通过:首先在配置文件中查找关系类型𝑟，然后发现𝑟的左侧只能是具有𝑒1类型的实体，而在右侧则是具有𝑒2类型的实体， 然后，可以确定左侧实体，该实体由（𝑒1，𝑖，𝑥）（𝑒1=实体类型，i=左侧分区索引，x=该实体在分区内的索引）给出，类似地，右侧实体为（𝑒2 ，𝑗，𝑦）。</p>
<p>源地址：<br><a href="https://torchbiggraph.readthedocs.io/en/latest/data_model.html">https://torchbiggraph.readthedocs.io/en/latest/data_model.html</a></p>
]]></content>
      <tags>
        <tag>PyTorch-BigGraph</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch-BigGraph 分布式</title>
    <url>/2020/10/28/PBG-Distributed-mode/</url>
    <content><![CDATA[<h1 id="PyTorch-BigGraph-分布式"><a href="#PyTorch-BigGraph-分布式" class="headerlink" title="PyTorch-BigGraph 分布式"></a>PyTorch-BigGraph 分布式</h1><p>PBG can perform training across multiple machines which communicate over a network, in order to reduce training time on large graphs. Distributed training is able to concurrently utilize larger computing resources, as well as to keep the entire model stored in memory across all machines, avoiding the need to swap it to disk. On each machine, the training is further parallelized across multiple subprocesses.<br>PBG可以在通过网络进行通信的多台机器上执行训练，以减少大图上的训练时间。分布式训练能够利用巨大的计算资源，同时其可以将整个模型存储在所有机器的内存中而不需要进行磁盘交换。在每一台机器上，训练通过多个子进程进行。</p>
<h2 id="Set-up"><a href="#Set-up" class="headerlink" title="Set up"></a>Set up</h2><p>In order to perform distributed training, the configuration file must first be updated to contain the specification of the desired distributed setup. If training should be carried out on 𝑁 machines, then the num_machines key in the config must be set to that value. In addition, the distributed_init_method must describe a way for the trainers to discover each other and set up their communication. All valid values for the init_method argument of torch.distributed.init_process_group() are accepted here. Usually this will be a path to a shared network filesystem or the network address of one of the machines. See the PyTorch docs for more information and a complete reference.<br>为了执行分布式训练，配置文件必须首先进行相应配置，其需要包括分布式部署的相关配置。如果训练在N台机器上运行，num_machines必须要要配置成N。另外，distributed_init_method必须描述各个机器之间的通信方式。这里能够接受所有torch.distributed.init_process_group()里面的参数。通常来说，分布式训练将一台计算机的网络地址设置为文件共享系统的地址。 有关更多信息和完整参考，请参见PyTorch文档。</p>
<p>To launch distributed training, call torchbiggraph_train –rank rank config.py on each machine, with rank replaced by an integer between 0 and 𝑁−1 (inclusive), different for each machine. Each machine must have PBG installed and have a copy of the config file.<br>要启动分布式训练，首先在每台机器上调用torchbiggraph_train –rank rank config.py， 其中每台机器有各自的rank值（从1到N-1）。每台机器必须安装了PBG，并且具有配置文件的副本。</p>
<p>In some uncommon circumstances, one may want to store the embeddings on different machines than the ones that are performing training. In that case, one would set num_partition_servers to a positive value and manually launch some instances of torchbiggraph_partitionserver as well. See below for more information on this.<br>特定情况下，可能有将嵌入存储在不同机器而不是一台机器上的需求。在这种情况下，需要在配置文件中设置num_partition_servers为一个正数并且手动使用torchbiggraph_partitionserver来启动一些实例。详情见下文</p>
<blockquote>
<p>Tip<br> A good default setting is to set num_machines to half the number of partitions (see below why) and leave num_partition_servers unset.<br> 一个好的默认设置是将num_machines设置为分区数的一半（请参阅下面的原因），并保持num_partition_servers为未设置状态。</p>
</blockquote>
<blockquote>
<p>Warning<br> Unpartitioned entity types should not be used with distributed training. While the embeddings of partitioned entity types are only in use on one machine at a time and are swapped between machines as needed, the embeddings of unpartitioned entity types are communicated asynchronously through a poorly-optimized parameter server which was designed for sharing relation parameters, which are small. It cannot support synchronizing large amounts of parameters, e.g. an unpartitioned entity type with more than 1000 entities. In that case, the quality of the unpartitioned embeddings will likely be very poor.<br> 未分区的实体不应该进行分布式训练。实体类型的分区仅在同一时刻在某一台机器上进行训练，并且根据需要在不同机器间进行交换。未分区的实体类型的嵌入计算是通过欠优化的参数服务在机器之间进行异步交换的，这个服务只是设计成用来进行关系参数的共享并且很小。其不能支持大量参数的同步。例如：对于一个未分区的超过1000个实体的实体类型，在这种情况下，未分区的得到的嵌入质量会很差。</p>
</blockquote>
<h2 id="Communication-protocols"><a href="#Communication-protocols" class="headerlink" title="Communication protocols"></a>Communication protocols</h2><p> Distributed training requires the machines to coordinate and communicate in various ways for different purposes. These tasks are:<br> 分布式训练求机器以各种方式针对不同目的进行协调和通信。 这些任务是：</p>
<p> • synchronizing which trainer is operating on which bucket, assigning them so that there are no conflicts<br> • passing the embeddings of an entity partition from one trainer to the next one when needed (as this is data that is only accessed by one trainer at a time)<br> • sharing parameters that all trainers need access to simultaneously, by collecting and redistributing the updates to them.</p>
<p> • 同步哪个训练器在训练哪个桶，分配好任务以防产生冲突<br> • 将一个实体分区的嵌入从一个训练器传入到另一个训练器 （因为一个训练器只使用一次在一个epoch中）<br> • 通过收集和重新分发参数来实现各个训练器间的参数共享</p>
<p> Each of these is implemented by a separate “protocol”, and each trainer takes part in some or all of them by launching subprocesses that act as clients or servers for the different protocols. These protocols are explained below to provide insight into the system.</p>
<p> 这些中的每一个任务都由单独的“协议”实现，并且每个训练器都通过启动充当针对不同协议的客户端或服务器的子进程来参与整个过程的一部分或全部。下面说明这些协议以提供对系统的了解。</p>
<h3 id="Synchronizing-bucket-access"><a href="#Synchronizing-bucket-access" class="headerlink" title="Synchronizing bucket access"></a>Synchronizing bucket access</h3><p>PBG parallelizes training across multiple machines by having them all operate simultaneously on disjoint buckets (i.e., buckets that don’t have any partition in common). Therefore, each partition is in use by up to one machine at a time, and each machine uses up to two partitions (the only exception is for buckets “on the diagonal”, that have the same left- and right-hand side partition). This means that the number of buckets one can simultaneously train on is about half the total number of partitions.<br>PBG通过使它们同时在不相交的存储桶（即没有共同分区的存储桶）上同时运行来并行化多台机器的培训。（也就是所谓的数据分区）。因此，每个分区一次最多在一台机器上被使用，并且被个机器一次训练最多用两个分区（唯一的例外是“对角线”上的存储桶，它们具有相同的左侧和右侧分区））。这意味同时可以训练的桶的数量大概是一般的分区数量。</p>
<p>The way the machines agree on which one gets to operate on what bucket is through a “lock server”. The server is implicitly started by the trainer of rank 0. All other machines connect to it as clients, ask for a new bucket to operate on (when they need one), get a bucket assigned from the server (or none, if all buckets have already been trained on or are “locked” because their partitions are in use by another trainer), train on it, then report it as done and repeat. The lock server tries to optimize I/O by preferring, when a trainer asks for a bucket, to assign one that has as many partitions in common with the previous bucket that the trainer trained on, so that these partitions can be kept in memory rather than having to be unloaded and reloaded.<br>机器间通过“锁服务”的形式来表明哪个桶正在被操作。服务器由级别0（master）的训练器隐式启动，所有其他机器作为客户端连接到服务器，当它们需要新的桶的时候，他们向master请求并且得到一个新的桶数据（可能没拿到，因为可能存在所有的桶都被训练完一次了或者桶是锁住的状态）来进行训练、报告然后重复执行。当一个训练器请求新桶的时候，锁定服务器通过优先分配与该训练器上一次训练的数据有较多相同分区的桶来优化I/O，通过这样那些分区就可以被保留在内存上而不是被卸载然后又加载。</p>
<h3 id="Exchanging-partition-embeddings"><a href="#Exchanging-partition-embeddings" class="headerlink" title="Exchanging partition embeddings"></a>Exchanging partition embeddings</h3><p>When a trainer starts operating on a bucket it needs access to the embeddings of all entities (of all types) that belong to either the left- or the right-hand side partition of the bucket. The “locking” mechanism of the lock server ensures that at most one trainer is operating on a partition at any given time. This doesn’t hold for unpartitioned entity types, which are shared among all trainers; see below. Thus each trainer has exclusive hold of the partitions it’s training on.<br>当一个训练器开始在一个桶上执行训练的时候，他需要获取首先获取属于桶的左侧或右侧分区的所有实体（所有类型）的嵌入。 锁定服务器的“锁定”机制确保在任何给定时间最多有一个训练器在一个分区上运行。这个机制不适用于未分区的实体，因为他们被所有的训练器共享。见下文。 因此，每个训练器在训练时可以独占其训练的分区。</p>
<p>Once a trainer starts working on a new bucket it needs to acquire the embeddings of its partitions, and once it’s done it needs to release them and make them available, in their updated version, to the next trainer that needs them. In order to do this, there’s a system of so-called “partition servers” that store the embeddings, provide them upon request to the trainers who need them, receive back the updated embedding and store it.<br>当一个训练器开始在新的桶上上工作后，需要获取其分区的嵌入内容，完成后，需要释放它们并以更新版本将其提供给需要该训练的下一个训练器。 为此，有一个所谓的“分区服务器”系统，用于存储嵌入内容，并根据需要将其提供给需要它们的训练器，并接收更新的嵌入内容并进行存储。</p>
<p>This service is optional, and is disabled when num_partition_servers is set to zero. In that case the trainers “send” each other the embeddings simply by writing them to the checkpoint directory (which should reside on a shared disk) and then fetching them back from there.<br>此服务是可选的，并且在num_partition_servers设置为零时被禁用。 在这种情况下，训练器只需将嵌入内容写入“检查点”目录（应位于共享磁盘上），然后从那里取回它们，即可相互“发送”嵌入内容。</p>
<p>When this system is enabled, it can operate in two modes. The simplest mode is triggered when num_partition_servers is -1 (the default): in that case all trainers spawn a local process that acts as a partition server. If, on the other hand, num_partition_servers is a positive value then the trainers will not spawn any process, but will instead connect to the partition servers that the user must have provisioned manually by launching the torchbiggraph_partitionserver command on the appropriate number of machines.<br>启用此系统后，它可以在两种模式下运行。 当num_partition_servers为-1（默认值）时，将触发最简单的模式：在这种情况下，所有训练器都会生成一个充当分区服务器的本地进程。 另一方面，如果num_partition_servers为正值，那么训练器将不会产生任何进程，而是将会连接到用户手动设置的提供的分区服务器上，用户通过在适当数量的机器上运行torchbiggraph_partitionserver来实现预先供给。</p>
<h3 id="Updating-shared-parameters"><a href="#Updating-shared-parameters" class="headerlink" title="Updating shared parameters"></a>Updating shared parameters</h3><p>Some parameters of the model need to be used by all trainers at the same time (this includes the operator weights, the global embeddings of each entity type, the embeddings of the unpartitioned entities). These are parameters that don’t depend on what bucket the trainer is operating on, and therefore are always present on all trainers (as opposed to the entity embeddings, which are loaded and unloaded as needed). These parameters are synchronized using a series of “parameter servers”. Each trainer starts a local parameter server (in a separate subprocess) and connects to all other parameter servers. Each parameter that is shared between trainers is then stored in a parameter server (possibly sharded across several of them, if too large). Each trainer also has a loop (also in a separate subprocess) which, at regular intervals, goes over each shared parameter, computes the difference between its current local value and the value it had when it was last synced with the server where the parameter is hosted and sends that delta to that server. The server, in turn, accumulates all the deltas it receives from all trainers, updates the value of the parameter and sends this new value back to the trainers. The parameter server performs throttling to 100 updates/s or 1GB/s, in order to prevent the parameter server from starving the other communication.<br>所有训练器必须同时使用模型的某些参数（这包括操作员权重，每种实体类型的全局嵌入，未分区实体的嵌入）。这些参数并不取决于那些训练器正在操作的桶，因此其始终存在于所有训练器上（与实体嵌入情况相仿，实体嵌入根据需要进行加载和卸载）这些参数使用一系列“参数服务器”进行同步。每一个训练器开启一个本地的参数服务器（以一个独自的子进程）并且和其他所有的参数服务器进行连接。每一个被训练器共享的参数被存储在参数服务器中（如果参数太大的话，则可能将其分散在其中的几个服务器中）。每一个训练器还有一个loop（同样也在一个子进程中），该循环以固定的时间间隔遍历每个共享参数，计算其当前本地值与上次与该服务器上次与服务器同步时所具有的值之间的差，那些服务器的参数值是被托管并且发送到本地的子进程中。反过来，服务器会累积从所有训练器那里收到的所有差，更新参数的值，然后将此新值发送回训练器。参数服务器执行限制至100个更新每秒或者1GB每秒，以防止参数服务器使其他通信中断。</p>
<p>源地址：<a href="https://torchbiggraph.readthedocs.io/en/latest/distributed_training.html">https://torchbiggraph.readthedocs.io/en/latest/distributed_training.html</a></p>
]]></content>
      <tags>
        <tag>PyTorch-BigGraph</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch-BigGraph 评估</title>
    <url>/2020/11/04/PBG-Evaluation/</url>
    <content><![CDATA[<h1 id="PyTorch-BigGraph-评估"><a href="#PyTorch-BigGraph-评估" class="headerlink" title="PyTorch-BigGraph 评估"></a>PyTorch-BigGraph 评估</h1><p>During training, the average loss is reported for each edge bucket at each pass. Evaluation metrics can be computed on held-out data during or after training to measure the quality of trained embeddings.<br>在训练过程中，每一次epoch中的每一个桶的的平均损失会被汇报。可以在训练期间或之后根据保留的数据计算评估指标，以测量训练后嵌入的质量。</p>
<h2 id="Offline-evaluation¶"><a href="#Offline-evaluation¶" class="headerlink" title="Offline evaluation¶"></a>Offline evaluation¶</h2><p>The torchbiggraph_eval command will perform an offline evaluation of trained PBG embeddings on a validation dataset. This dataset should contain held-out data not included in the training dataset. It is invoked in the same way as the training command and takes the same arguments.<br>torchbiggraph_eval命令将会对验证数据集执行训练后嵌入的离线评估。该数据集应包含训练数据集中未包含的保留数据。它以与训练命令相同的方式调用，并采用相同的参数。</p>
<p>It is generally advisable to have two versions of the config file, one for training and one for evaluation, with the same parameters except for the edge paths, in order to evaluate a separate (and often smaller) set of edges. (It’s also possible to use a single config file and have it produce different output based on environment variables or other context). Training-specific config parameters (e.g., the learning rate, loss function, …) will be ignored during evaluation.<br>通常建议配置文件有两个版本，一个用于训练，一个用于评估，除了edge paths参数外，它们具有相同的参数，以便评估一组单独的（通常是较小的）边。（也可以使用单个配置文件，并根据环境变量或其他上下文产生不同的输出）。 评估过程中将忽略训练专用的配置参数（例如，学习率，损失函数等）。</p>
<p>The metrics are first reported on each bucket, and a global average is computed at the end. (If multiple edge paths are in use, metrics are computed separately for each of them but still ultimately averaged).<br>首先在每个存储桶上报告指标，最后计算全局平均值。 （如果正在使用多个edge paths ，则将针对每个边缘路径分别计算指标，但最终仍将其平均）。</p>
<p>Many metrics are statistics based on the “ranks” of the edges of the validation set. The rank of a positive edge is determined by the rank of its score against the scores of a certain number of negative edges. A rank of 1 is the “best” outcome as it means that the positive edge had a higher score than all the negatives. Higher values are “worse” as they indicate that the positive didn’t stand out.<br>许多指标是基于验证集边的“rank”的统计信息。正边的rank由得分相对于一定数量的负边缘的得分的rank确定。rank=1表示“最佳”结果，因为这意味着正边比所有负边的得分都高。较高的rank值表示性能较差，因为其表明正边并没有突出显示出来。</p>
<p>It may happen that some of the negative samples used in the rank computation are in fact other positive samples, which are expected to have a high score and may thus cause adverse effects on the rank. This effect is especially visible on smaller graphs, in particular when all other entities are used to construct the negatives. To fix it, and to match what is typically done in the literature, a so-called “filtered” rank is used in the FB15k demo script (and there only), where positive samples are filtered out when computing the rank of an edge. It is hard to scale this technique to large graphs, and thus it is not enabled globally. However, filtering is less important on large graphs as it’s less likely to see a training edge among the sampled negatives.<br>在计算rank的过程中，某些负样本实际上可能是其他正样本，这些样本可能会得到较高的得分因此会对rank的结果造成不利影响。在较小的图上，尤其是在使用所有其他实体构造负样本时，这种效果尤其明显。为了解决该问题并使其与文献中通常所做的事情相匹配，FB15k演示脚本（仅在此处）使用了所谓的“过滤后”等级，在计算边rank时会滤除正样本。很难将此技术适用于到大图中，因此无法全局启用。但是，过滤在大型图上的重要性较弱，因为它不太可能在负样本中使用训练的边。</p>
<p>The metrics are:<br>相关的指标有</p>
<p>• Mean Rank: the average of the ranks of all positives (lower is better, best is 1).<br>• Mean Reciprocal Rank (MRR): the average of the reciprocal of the ranks of all positives (higher is better, best is 1).<br>• Hits@1: the fraction of positives that rank better than all their negatives, i.e., have a rank of 1 (higher is better, best is 1).<br>• Hits@10: the fraction of positives that rank in the top 10 among their negatives (higher is better, best is 1).<br>• Hits@50: the fraction of positives that rank in the top 50 among their negatives (higher is better, best is 1).<br>• Area Under the Curve (AUC): an estimation of the probability that a randomly chosen positive scores higher than a randomly chosen negative (any negative, not only the negatives constructed by corrupting that positive).</p>
<h2 id="Evaluation-during-training"><a href="#Evaluation-during-training" class="headerlink" title="Evaluation during training"></a>Evaluation during training</h2><p>Offline evaluation is a slow process that is intended to be run after training is complete to evaluate the final model on a held-out set of edges constructed by the user. However, it’s useful to be able to monitor overfitting as training progresses. PBG offers this functionality, by calculating the same metrics as the offline evaluation before and after each pass on a small set of training edges. These stats are printed to the logs.<br>离线评估是一个缓慢的过程，其目的在于边训练完成后，对保留的边进行评估来评估训练的模型。但是，训练过程中监控过度拟合是很有用的。PBG通过在每次经过一小组训练边之前和之后计算与脱机评估相同的指标来实现训练时评估。 这些统计信息将打印到日志中。</p>
<p>The metrics are computed on a set of edges that is held out automatically from the training set. To be more explicit: using this feature means that training happens on fewer edges, as some are excluded and reserved for this evaluation. The holdout fraction is controlled by the eval_fraction config parameter (setting it to zero thus disables this feature). The evaluations before and after each training iteration happen on the same set of edges, thus are comparable. Moreover, the evaluations for the same edge chunk, edge path and bucket at different epochs also use the same set of edges.<br>指标是在一组边上计算的，这些边会从训练集中自动保留下来（不被训练，仅作为评估）。更明确地说：使用此功能意味着会训练较少的边，因为某些边被排除并保留用于此评估。保持比例由eval_fraction配置参数控制（将其设置为零将禁用此功能）。每次训练迭代之前和之后的评估都在同一组边上进行，因此具有可比性。此外，在不同时期对相同边缘块，边缘路径和存储桶的评估也使用相同的边缘集。</p>
<p>Evaluation metrics are computed both before and after training each edge bucket because it provides insight into whether the partitioned training is working. If the partitioned training is converging, then the gap between the “before” and “after” statistics should go to zero over time. On the other hand, if the partitioned training is causing the model to overfit on each edge bucket (thus decreasing performance for other edge buckets) then there will be a persistent gap between the “before” and “after” statistics.<br>评估指标在训练每个边缘存储块之前和之后都会进行计算，因为它可以洞悉分区的训练是否有效。如果分区训练正在收敛，则“之前”和“之后”统计量之间的差距应随时间推移变为零。另一方面，如果分区训练导致模型在每个边缘存储区上过拟合（从而降低了其他边缘存储区的性能），则“之前”和“之后”统计量之间将存在持续的差距。</p>
<p>It’s possible to use different batch sizes for same-batch and uniform negative sampling by tuning the eval_num_batch_negs and the eval_num_uniform_negs config parameters.<br>通过调整eval_num_batch_negs和eval_num_uniform_negs配置参数，可以对同一批次和统一负采样使用不同的批次大小。</p>
]]></content>
      <tags>
        <tag>PyTorch-BigGraph</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch-BigGraph I/O 格式</title>
    <url>/2020/10/27/PBG-I-O-format/</url>
    <content><![CDATA[<h1 id="PBG-I-O格式"><a href="#PBG-I-O格式" class="headerlink" title="PBG I/O格式"></a>PBG I/O格式</h1><h2 id="Entity-and-relation-types"><a href="#Entity-and-relation-types" class="headerlink" title="Entity and relation types"></a>Entity and relation types</h2><p>The list of entity types (each identified by a string), plus some information about each of them, is given in the <code>entities</code> dictionary in the configuration file. The list of relation types (each identified by its index in that list), plus some data like what their left- and right-hand side entity types are, is in the <code>relations</code> key of the configuration file.<br>实体类型的列表(每个实体类型由一个字符串标识)以及有关于他们的一些信息由配置文件中的实体字典提供。关系类型的列表（每个都由该列表中的索引标识）以及一些数据（如其左侧和右侧实体类型）在配置文件的relation key中指示。</p>
<h2 id="Entities"><a href="#Entities" class="headerlink" title="Entities"></a>Entities</h2><p>The only information that needs to be provided about entities is how many there are in each entity type’s partition. This is done by putting a file named <code>entity_count_type_part.txt</code> for each entity type identified by <code>type</code> and each partition <code>part</code> in the directory specified by the <code>entity_path</code> config parameter. These files must contain a single integer (as text), which is the number of entities in that partition. The directory where all these files reside must be specified as the <code>entity_path</code> key of the configuration file.<br>对于实体而言，其只需要提供每种实体类型的分区中有多少个实体这样一个信息即可。他通过将文件名称为<code>entity_count_type_part.txt</code>(<code>type</code>表示实体类型，<code>part</code>表示分区id)放到配置文件指定的文件夹来实现，这个文件夹在配置文件中的key为<code>entity_path</code>。 这些文件(entity_count_type1_part1.txt, entity_count_type2_part2.txt…)必须包含一个整数（作为文本），该整数表示在该分区中实体的数量。这些文件所在的目录名称必须指定为在配置文件下的<code>entity_path</code>。</p>
<p>It is possible to provide an initial value for the embeddings, by specifying a value for the <code>init_path</code> configuration key, which is the name of a directory that contains files in a format similar to the output format detailed in Checkpoint (possibly without the optimizer state dicts).<br>通过为<code>init_path</code>配置键指定一个值，可以为嵌入提供一个初始值。<code>init_path</code>值的目录下包含和Checkpoint类似的文件格式。（详情见checkpoint）</p>
<p>If no initial value is provided, it will be auto-generated, with each dimension sampled from the centered normal distribution whose standard deviation can be configured using the <code>init_scale</code> configuration key. For performance reasons the samples of all the entities of a certain type will not be independent.<br>如果为嵌入提供初始值，它将会自动生成，其每个维的值都是从正态分布中采样得到。可以通过配置文件中的init_scale来调整方差范围。出于性能原因，对于同一个类型下的所有实体的样本将不是独立的。</p>
<h2 id="Edges"><a href="#Edges" class="headerlink" title="Edges"></a>Edges</h2><p>For each bucket there must be a file that stores all the edges that fall in that bucket, of all relation types. This means that such a file is only identified by two integers, the partitions of its left- and right-hand side entities. It must be named <code>edges_lhs_rhs.h5</code> (where lhs and rhs are the above integers), it must be a HDF5 file containing three one-dimensional datasets of the same length, called rel, lhs and rhs. The elements in the 𝑖-th positions in each of them define the 𝑖-th edge: rel identifies the relation type (and thus the left- and right-hand side entity types), lhs and rhs given the indices of the left- and right-hand side entities within their respective partitions.<br>对于每一个桶，必须要有一个文件来存储该桶下的所有关系类型的所有边信息。这意味这样的一个文件仅由两个整数来进行标识，即它的左侧和右侧实体的分区id。 必须将其命名为edges_lhs_rhs.h5（其中lhs和rhs是左侧分区id和右侧分区id），它必须是HDF5文件，其中包含三个相同长度的一维数据集，分别称为rel，lhs和rhs。<br>在这三个一维数据集中，第𝑖个位置的元素定义了第i条边: rel定义了关系类型（并因此指定左侧和右侧实体类型），lhs和rhs定义了左右侧的实体在其各自分区下的索引。</p>
<p>To ease future updates to this format, each file must contain the format version in the format_version attribute of the top-level group. The current version is 1.<br>为了简化以后对该格式的更新，每个文件都必须在顶级组的format_version属性中包含格式版本。 当前版本是1。（这个是H5的格式问题）</p>
<p>If an entity type is unpartitioned (that is, all its entities belong to the same partition), then the edges incident to these entities must still be uniformly spread across all buckets.<br>如果某一个实体类型是未分区的（即，其所有实体都属于同一分区），则这些实体连接的的边必须仍然均匀地分布在所有存储桶中。</p>
<p>These files, for all buckets, must be stored in the same directory, which must be passed as the edge_paths configuration key. That key can actually contain a list of paths, each pointing to a directory of the format described above: in that case the graph will contain the union of all their edges.<br>对于所有桶对应的文件都必须存储在同一个目录下。该目录将会被配置到配置文件的<code>edge_paths</code>key下，该key实际上可以包含一个路径列表，每个路径都指向上述格式的目录：在这种情况下，图将包含其所有边的并集。</p>
<h2 id="Checkpoint"><a href="#Checkpoint" class="headerlink" title="Checkpoint"></a>Checkpoint</h2><p>The training’s checkpoints are also its output, and they are written to the directory given as the checkpoint_path parameter in the configuration. Checkpoints are identified by successive positive integers, starting from 1, and all the files belonging to a certain checkpoint have an extra component .vversion between their name and extension (e.g., something.v42.h5 for version 42).<br>训练的检查点也是其输出，它们被写入配置中指定为checkpoint_path参数的目录中。检查点由从1开始的连续正整数标识，并且属于某个检查点的所有文件的名称和扩展名之间都有一个额外的组件.vversion（例如，版本42的something.v42.h5）。</p>
<p>The latest complete checkpoint version is stored in an additional file in the same directory, called <code>checkpoint_version.txt</code>, which contains a single integer number, the current version.<br>最新的完整检查点版本存储在同一目录中的另一个文件中，该文件称为checkpoint_version.txt，其中包含一个整数，即当前版本。</p>
<p>Each checkpoint contains a JSON dump of the config that was used to produce it stored in the config.json file.<br>每一个检查点都为配置文件生成了对应json文件，并将其存储在config.json文件中。</p>
<p>After a new checkpoint version is saved, the previous one will automatically be deleted. In order to periodically preserve some of these versions, set the <code>checkpoint_preservation_interval</code> config flag to the desired period (expressed in number of epochs).<br>当一个新的检查点版本被保存时，上一个将会自动被删除。如果想要周期性的保存一些检查点，可以在配置文件中设置checkpoint_preservation_interval（以训练轮次数的形式）</p>
<h2 id="Model-parameters"><a href="#Model-parameters" class="headerlink" title="Model parameters"></a>Model parameters</h2><p>The model parameters are stored in a file named model.h5, which is a HDF5 file containing one dataset for each parameter, all of which are located within the model group. Currently, the parameters that are provided are:<br>• model/relations/idx/operator/side/param with the parameters of each relation’s operator.<br>• model/entities/type/global_embedding with the per-entity type global embedding.<br>模型参数存储在名为model.h5的文件中，该文件是一个HDF5文件，其中包含每个参数的一个数据集，所有数据集都位于模型组下。 当前，提供的参数有：<br>• model/relations/idx/operator/side/param -&gt; 对应每个算子的参数<br>• model/entities/type/global_embedding -&gt;带有每个实体类型全局嵌入</p>
<blockquote>
<p>🤔 这里好像和实际有点出入</p>
</blockquote>
<p>Each of these datasets also contains, in the state_dict_key attribute, the key it was stored inside the model state dict. An additional dataset may exist, optimizer/state_dict, which contains the binary blob (obtained through torch.save()) of the state dict of the model’s optimizer.</p>
<blockquote>
<p>🤔 H5格式</p>
</blockquote>
<p>Finally, the top-level group of the file contains a few attributes with additional metadata. This mainly includes the format version, a JSON-dump of the config and some information about the iteration that produced the checkpoint.<br>最后，文件的顶级组包含一些带有其他元数据的属性。 主要包括格式版本，配置的JSON转储以及有关生成检查点的迭代的一些信息。</p>
<h2 id="Embeddings"><a href="#Embeddings" class="headerlink" title="Embeddings"></a>Embeddings</h2><p>Then, for each entity type and each of its partitions, there is a file embeddings_type_part.h5 (where type is the type’s name and part is the 0-based index of the partition), which is a HDF5 file with two datasets. One two-dimensional dataset, called embeddings, contains the embeddings of the entities, with the first dimension being the number of entities and the second being the dimension of the embedding.<br>对于每种实体类型及其每个分区，都有一个embeddings_type_part.h5文件（其中type是类型的名称，part是分区的从0开始的索引），它是一个包含两个数据集的HDF5文件。 一个称为嵌入的二维数据集包含实体的嵌入，第一个维度是实体的数量，第二个维度是嵌入的维度。（其实就是实体嵌入的矩阵）（数据集1）</p>
<p>Just like for the model parameters file, the optimizer state dict and additional metadata is also included.<br>就像model.h5一样，embeedings.h5中还包含了一些关于优化状态等额外的元数据。（数据集2）</p>
<p>源地址：<a href="https://torchbiggraph.readthedocs.io/en/latest/input_output.html">https://torchbiggraph.readthedocs.io/en/latest/input_output.html</a></p>
]]></content>
      <tags>
        <tag>PyTorch-BigGraph</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch-BigGraph 概要</title>
    <url>/2020/10/22/PBG-overview/</url>
    <content><![CDATA[<h1 id="PBG概要"><a href="#PBG概要" class="headerlink" title="PBG概要"></a>PBG概要</h1><p>PyTorch-BigGraph (PBG) is a distributed system for learning graph embeddings for large graphs, particularly big web interaction graphs with up to billions of entities and trillions of edges.PBG now supports GPU training.<br>PyTorch-BigGraph（PBG）是一个用于学习学习大图（节点、关系众多），特别是那些具有多达数十亿个实体和数万亿条边的大型Web交互图嵌入的分布式系统。如今PBG也支持了GPU训练的过程。</p>
<blockquote>
<p>key point: PBG, distributed, big graph, embedding, GPU</p>
</blockquote>
<p>PBG was introduced in the PyTorch-BigGraph: A Large-scale Graph Embedding Framework paper, presented at the SysML conference in 2019.<br>PBG是于2019年发表在SysML会议上的一篇论文 PYTORCH-BIGGRAPH: A LARGE-SCALE GRAPH EMBEDDING SYSTEM被首次提出<br>论文地址: <a href="https://mlsys.org/Conferences/2019/doc/2019/71.pdf">https://mlsys.org/Conferences/2019/doc/2019/71.pdf</a></p>
<p>PBG trains on an input graph by ingesting its list of edges, each identified by its source and target entities and, possibly, a relation type. It outputs a feature vector (embedding) for each entity, trying to place adjacent entities close to each other in the vector space, while pushing unconnected entities apart. Therefore, entities that have a similar distribution of neighbors will end up being nearby.<br>PBG通过提取图的边列表来进行embedding训练，每条边由其源实体,目标实体以及一个可能的关系类型作为标识。PBG对每个实体输出一个特征向量（嵌入）。它通过将相邻的实体在向量空间中彼此靠近放置，不相邻的实体彼此远离放置。通过这样的手段，相似的实体最终的嵌入互相靠近。</p>
<blockquote>
<p>key point: 结点向量间的距离能够衡量原图中的邻接关系强弱。</p>
</blockquote>
<p>It is possible to configure each relation type to calculate this “proximity score” in a different way, with the parameters (if any) learned during training. This allows the same underlying entity embeddings to be shared among multiple relation types.<br>在训练过程中，可以通过配置每种关系类型来计算接近程度以及参数学习。如此一来一些相同的实体嵌入可以被多种关系类型进行共享。</p>
<p>The generality and extensibility of its model allows PBG to train a number of models from the knowledge graph embedding literature, including TransE, RESCAL, DistMult and ComplEx.<br>模型的通用性和可扩展性使得PBG可以从知识图嵌入文献中训练许多模型，包括TransE，RESCAL，DistMult和ComplEx。</p>
<blockquote>
<p>key point: embedding algorithm: TransE, RESCAL, DistMult, ComplEx</p>
</blockquote>
<p>PBG is designed with scale in mind, and achieves it through:<br>PBG的设计考虑了规模，并通过以下方法实现了规模：</p>
<p>1.graph partitioning, so that the model does not have to be fully loaded into memory<br>2.multi-threaded computation on each machine<br>3.distributed execution across multiple machines (optional), all simultaneously operating on disjoint parts of the graph<br>4.batched negative sampling, allowing for processing &gt;1 million edges/sec/machine with 100 negatives per edge</p>
<p>1.图形分区，因此模型不必完全加载到内存中<br>2.每台机器上的多线程计算<br>3.在多台计算机上分布执行（可选），所有这些操作同时在图的不连续部分上运行(数据分布)<br>4.批量负采样，可处理 &gt;1,000,00 个边/秒/机器, 每条边100个负采样</p>
<blockquote>
<p>key point: partition, distributed,  batched negative sampling</p>
</blockquote>
<p>PBG is not optimized for small graphs. If your graph has fewer than 100,000 nodes, consider using KBC with the ComplEx model and N3 regularizer. KBC produces state-of-the-art embeddings for graphs that can fit on a single GPU. Compared to KBC, PyTorch-BigGraph enables learning on very large graphs whose embeddings wouldn’t fit in a single GPU or a single machine, but may not produce high-quality embeddings for small graphs without careful tuning.<br>PBG并未针对小型图形进行优化。如果图少于100,000个节点，请考虑将KBC与ComplEx模型和N3正则化器一起使用。 KBC为可放在单个GPU上的图形生成最先进的嵌入。与KBC相比，PyTorch-BigGraph支持在非常大的图上进行学习，这些图的嵌入无法在单个GPU或单个机器中完成，但是PBG存在的缺点是其可能无法为小型图生成高质量的嵌入如果没有很好的调参的话。</p>
<p>源地址：<br><a href="https://github.com/facebookresearch/PyTorch-BigGraph">https://github.com/facebookresearch/PyTorch-BigGraph</a></p>
]]></content>
      <tags>
        <tag>PyTorch-BigGraph</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱识别</title>
    <url>/2020/10/11/Probabilistic-Models-for-KG-Construction/</url>
    <content><![CDATA[<h1 id="前情提要"><a href="#前情提要" class="headerlink" title="前情提要"></a>前情提要</h1><p>在知识图谱中，主要需要解决的有三个问题:  (节点，属性，关系)</p>
<ol>
<li>Who are the entities (nodes) in the graph?</li>
<li>What are their attributes and types (labels)?</li>
<li>How are they related (edges)?<br><img src="/image/KG-problem.png"></li>
</ol>
<p>知识图谱的构建主要就是通过IE将互联网上各式各样的资源进行提取，整合得到结构化的数据(说起来容易，做起来难！)</p>
<p><img src="/image/build-KG.png"></p>
<p>提取到的知识可能存在的一些问题:</p>
<ol>
<li>ambiguous (一般我们说ambiguous 指的是多个entity指向一个名字， variant指的是一个entity有多个名字)<ul>
<li>Beetles, beetles, Beatles 这三个name可能都是指向一个实体甲壳虫，但是名字不同</li>
<li>citizenOf, livedIn, bornIn 这三个property可能都是指向一个实体，表达的意思为该实体住在某一个地方</li>
</ul>
</li>
<li>incomplete<ul>
<li>missing relationship; missing labels; missing entities</li>
</ul>
</li>
<li>inconsistent<ul>
<li>exclusive labels (alive, dead), 提取到的知识表明一个人既已经去世又还在人世</li>
<li>domain-range constraints，提取到一个relationship例如人的年龄为一朵花的名字(age属性一般为int)</li>
<li>一个人的配偶有多个存在(common sense: 一般认为配偶为一对)</li>
</ul>
</li>
</ol>
<p>例子: NELL在知识提取中存在的问题:<br>NELL: Never-Ending Language Learner CMU研究的一个人工智能语言学习程序,从Web文本中获取知识，并将其添加到内部知识库内 ,使用机器学习算法学习新入库的知识，巩固对知识的理解。<br>特点: Large-scale IE project; Lifelong learning: aims to “read the web”; Ontology of known labels and relations; Knowledge base contains millions of facts</p>
<p>存在问题:</p>
<ol>
<li>Entity co-reference errors, 例如对于Kyrgyzstan(吉尔吉斯斯坦,中亚的一个国家)这样一个实体，他具有许多的variants(Kyrgystan, Kyrgistan, Kyrghyzstan, Kyrgzstan, Kyrgyz Republic), NELL不能很好将这些variant完全映射到同一个实体上。</li>
<li>Missing and spurious labels, 例如Kyrgyzstan被标注为鸟或者国家</li>
<li>Missing and spurious relations, 例如Kyrgyzstan的位置，通过IE可能得到不同的地区归属。</li>
<li>Violations of ontological knowledge </li>
</ol>
<blockquote>
<p>由于知识提取过程中的诸多问题，想要很好的解决这些问题需要jointly considering multiple extractions.</p>
</blockquote>
<p>图谱构建方法:</p>
<ol>
<li>Clean and complete extraction graph</li>
<li>Incorporate ontological constraints and relational patterns</li>
<li>Discover statistical relationships within knowledge graph</li>
</ol>
<h1 id="PSL-Probabilistic-soft-logic-概率软逻辑"><a href="#PSL-Probabilistic-soft-logic-概率软逻辑" class="headerlink" title="PSL (Probabilistic soft logic) 概率软逻辑"></a>PSL (Probabilistic soft logic) 概率软逻辑</h1><blockquote>
<p>It’s not a black-and-white issue.</p>
</blockquote>
<h2 id="从一个例子说起-美国选民派别分类-Voter-Party-Classification"><a href="#从一个例子说起-美国选民派别分类-Voter-Party-Classification" class="headerlink" title="从一个例子说起, 美国选民派别分类 (Voter Party Classification)"></a>从一个例子说起, 美国选民派别分类 (Voter Party Classification)</h2><p><img src="/image/vote-classification.png"></p>
<p>各个角度的信息来分析一名美国选民的政治态度:</p>
<p><img src="/image/voter_attitude.png"></p>
<p>IDEA: <strong>Collective Classification</strong></p>
<p>假设分析A的政治态度，可以通过其配偶的政治态度，其tweet粉丝的观点，个人的行为来进行分析，通过制定一些规则来确定这个人是支持共和党还是民主党。<br><img src="/image/vote_opinion.png"></p>
<figure class="highlight livescript"><table><tr><td class="code"><pre><code class="hljs livescript">Collective Classification <span class="hljs-keyword">with</span> PSL<br>规则: 如果A<span class="hljs-function"> -&gt;</span> 那么B <br><br><span class="hljs-comment">/* Local rules */</span><br><span class="hljs-number">5.0</span>: Donates<span class="hljs-function"><span class="hljs-params">(A, P)</span> -&gt;</span> Votes(A, P)<br><span class="hljs-number">0.3</span>: Mentions<span class="hljs-function"><span class="hljs-params">(A, “Affordable Health”)</span> -&gt;</span> Votes(A, “Democrat”)<br><span class="hljs-number">0.3</span>: Mentions<span class="hljs-function"><span class="hljs-params">(A, “Tax Cuts”)</span> -&gt;</span> Votes(A, “Republican”)<br><br><span class="hljs-comment">/* Relational rules */</span><br><span class="hljs-number">1.0</span>: Votes<span class="hljs-function"><span class="hljs-params">(A,P)</span> &amp; <span class="hljs-title">Spouse</span><span class="hljs-params">(B,A)</span> -&gt;</span> Votes(B,P)<br><span class="hljs-number">0.3</span>: Votes<span class="hljs-function"><span class="hljs-params">(A,P)</span> &amp; <span class="hljs-title">Friend</span><span class="hljs-params">(B,A)</span> -&gt;</span> Votes(B,P)<br><span class="hljs-number">0.1</span>: Votes<span class="hljs-function"><span class="hljs-params">(A,P)</span> &amp; <span class="hljs-title">Colleague</span><span class="hljs-params">(B,A)</span> -&gt;</span> Votes(B,P)<br><br><span class="hljs-comment">/* Range constraint */</span><br>Votes(A, “Republican”) + Votes(A, “Democrat”) = <span class="hljs-number">1.0</span> .<br></code></pre></td></tr></table></figure>

<h2 id="为什么需要PSL"><a href="#为什么需要PSL" class="headerlink" title="为什么需要PSL"></a>为什么需要PSL</h2><p>通过上面的例子，可以发现，通过制定一些比较好的规则，可以为提取的信息进行一些比较高质量的标注。</p>
<p>存在问题: </p>
<ol>
<li><p>规则的制定依赖于提取到的信息，如果提取到的知识存在问题，那么一定是garbage in - garbage out.<br> 例子:<br> Lbl(Socrates, Man) &amp; Sub(Man, Mortal) -&gt; Lbl(Socrates, Mortal)<br> 假如提取到的知识没有正确的将Socrates标注为男性，那么永远也得不到Lbl(Socrates, Mortal)这样的知识。</p>
<p> 解决措施: probabilistic models  =&gt; P(new facts|extraction infos)<br> P(Lbl(Socrates, Mortal)|Lbl(Socrates,Man)=0.9)</p>
</li>
<li><p>多条规则之间可能产生冲突<br> 例子:<br> B的正确性无法确定:<br> A -&gt; B 符合<br> C -&gt; B 不符合<br> D -&gt; B 符合</p>
<p> 解决措施: Soft Logic<br> A -&gt; B 符合 0.7<br> C -&gt; B 不符合 0.2<br> D -&gt; B 符合 0.9</p>
</li>
</ol>
<h2 id="Soft-Probability"><a href="#Soft-Probability" class="headerlink" title="Soft Probability"></a>Soft Probability</h2><figure class="highlight properties"><table><tr><td class="code"><pre><code class="hljs properties"><span class="hljs-attr">P</span> <span class="hljs-string">-&gt; Q</span><br><span class="hljs-meta">/*</span> <span class="hljs-string">Soft Logic Penalty */</span><br><span class="hljs-attr">if</span> <span class="hljs-string">P &lt; Q</span><br>	<span class="hljs-attr">return</span> <span class="hljs-string">satisfication</span><br><span class="hljs-attr">else</span>:<span class="hljs-string"></span><br>	<span class="hljs-attr">return</span> <span class="hljs-string">P-Q</span><br><br><span class="hljs-attr">Closed</span> <span class="hljs-string">Form 封闭世界原则</span><br><span class="hljs-comment">!Q = 1-Q</span><br><span class="hljs-attr">P</span> <span class="hljs-string">-&gt; Q = max(0, P-Q)</span><br><span class="hljs-attr">P</span> <span class="hljs-string">&amp;  Q = max(0, P+Q-1)</span><br><span class="hljs-attr">P</span> <span class="hljs-string">|  Q = min(1, P+Q)</span><br><br></code></pre></td></tr></table></figure>

<h2 id="PSL-model"><a href="#PSL-model" class="headerlink" title="PSL model"></a>PSL model</h2><ul>
<li>PSL finds optimal assignment for all unknowns</li>
<li>Optimal = minimizes the soft-logic penalty</li>
<li>Fast, joint convex optimization using ADMM</li>
<li>Supports learning rule weights and latent variables</li>
</ul>
<h1 id="图谱模型的建立"><a href="#图谱模型的建立" class="headerlink" title="图谱模型的建立"></a>图谱模型的建立</h1><p>步骤:<br>Define joint probability distribution on knowledge graphs<br>Each candidate fact in the knowledge graph is a variable<br>Statistical signals, ontological knowledge and rules parameterize the dependencies between variables<br>Find most likely knowledge graph by optimization/sampling</p>
<h2 id="Knowledge-Graph-Identification-KGI"><a href="#Knowledge-Graph-Identification-KGI" class="headerlink" title="Knowledge Graph Identification (KGI)"></a>Knowledge Graph Identification (KGI)</h2><p>Knowledge Graph Identification (KGI): 解决图谱中存在的一系列问题的方案</p>
<ul>
<li>Performs graph identification:<ul>
<li>entity resolution</li>
<li>collective classification</li>
<li>link prediction</li>
</ul>
</li>
<li>Enforces ontological constraints</li>
<li>Incorporates multiple uncertain sources</li>
</ul>
<p>P(Who, What, How | Extractions)</p>
<h2 id="图谱中probability的获得"><a href="#图谱中probability的获得" class="headerlink" title="图谱中probability的获得"></a>图谱中probability的获得</h2><p>Statistical signals from text extractors and classifiers<br>    ex:<br>    P(R(John,Spouse,Yoko))=0.75; P(R(John,Spouse,Cynthia))=0.25<br>    LevenshteinSimilarity(Beatles, Beetles) = 0.9</p>
<p>Ontological knowledge about domain<br>    ex:<br>    Functional(Spouse) &amp; R(A,Spouse,B) -&gt; !R(A,Spouse,C)<br>    Range(Spouse, Person) &amp; R(A,Spouse,B) -&gt; Type(B, Person)</p>
<p>Rules and patterns mined from data<br>    ex:<br>    R(A, Spouse, B) &amp; R(A, Lives, L) -&gt; R(B, Lives, L)<br>    R(A, Spouse, B) &amp; R(A, Child, C) -&gt; R(B, Child, C)</p>
<h2 id="定义一个graphical-models"><a href="#定义一个graphical-models" class="headerlink" title="定义一个graphical models"></a>定义一个graphical models</h2><p>有许多种方式去定一个图模型，这里使用PSL(使用规则)<br>PSL infers a “truth value” for each fact via optimization</p>
<h3 id="Rules-for-KG-Model"><a href="#Rules-for-KG-Model" class="headerlink" title="Rules for KG Model"></a>Rules for KG Model</h3><figure class="highlight livescript"><table><tr><td class="code"><pre><code class="hljs livescript"><br><span class="hljs-number">100</span>: 	Subsumes<span class="hljs-function"><span class="hljs-params">(L1,L2)</span>   &amp; <span class="hljs-title">Label</span><span class="hljs-params">(E,L1)</span>      -&gt;</span>  Label(E,L2)<br><span class="hljs-number">100</span>:	Exclusive<span class="hljs-function"><span class="hljs-params">(L1,L2)</span>  &amp; <span class="hljs-title">Label</span><span class="hljs-params">(E,L1)</span>      -&gt;</span> !Label(E,L2)<br><br><span class="hljs-number">100</span>:	Inverse<span class="hljs-function"><span class="hljs-params">(R1,R2)</span>    &amp; <span class="hljs-title">Relation</span><span class="hljs-params">(R1,E,O)</span> -&gt;</span>  Relation(R2,O,E)<br><span class="hljs-number">100</span>:	Subsumes<span class="hljs-function"><span class="hljs-params">(R1,R2)</span>   &amp; <span class="hljs-title">Relation</span><span class="hljs-params">(R1,E,O)</span> -&gt;</span>  Relation(R2,E,O)<br><span class="hljs-number">100</span>:	Exclusive<span class="hljs-function"><span class="hljs-params">(R1,R2)</span>  &amp; <span class="hljs-title">Relation</span><span class="hljs-params">(R1,E,O)</span> -&gt;</span> !Relation(R2,E,O)<br><br><span class="hljs-number">100</span>:	Domain<span class="hljs-function"><span class="hljs-params">(R,L)</span>       &amp; <span class="hljs-title">Relation</span><span class="hljs-params">(R,E,O)</span>  -&gt;</span>  Label(E,L)<br><span class="hljs-number">100</span>:	Range<span class="hljs-function"><span class="hljs-params">(R,L)</span>        &amp; <span class="hljs-title">Relation</span><span class="hljs-params">(R,E,O)</span>  -&gt;</span>  Label(O,L)<br><br><span class="hljs-number">10</span>:	SameEntity<span class="hljs-function"><span class="hljs-params">(E1,E2)</span> &amp; <span class="hljs-title">Label</span><span class="hljs-params">(E1,L)</span>      -&gt;</span>  Label(E2,L)<br><span class="hljs-number">10</span>: 	SameEntity<span class="hljs-function"><span class="hljs-params">(E1,E2)</span> &amp; <span class="hljs-title">Relation</span><span class="hljs-params">(R,E1,O)</span> -&gt;</span>  Relation(R,E2,O)<br><br><span class="hljs-number">1</span>:	Label_OBIE<span class="hljs-function"><span class="hljs-params">(E,L)</span>                      -&gt;</span>  Label(E,L)<br><span class="hljs-number">1</span>:	Label_OpenIE<span class="hljs-function"><span class="hljs-params">(E,L)</span>                    -&gt;</span>  Label(E,L)<br><span class="hljs-number">1</span>:	Relation_Pattern<span class="hljs-function"><span class="hljs-params">(R,E,O)</span>              -&gt;</span>  Relation(R,E,O)<br><span class="hljs-number">1</span>:	                                        !Relation(R,E,O)<br><span class="hljs-number">1</span>:	                                        !Label(E,L)<br><br></code></pre></td></tr></table></figure>

<h3 id="Rules-to-Distributions"><a href="#Rules-to-Distributions" class="headerlink" title="Rules to Distributions"></a>Rules to Distributions</h3><p>Rules are grounded by substituting literals into formulas<br>Ground rules provide a joint probability distribution over knowledge graph facts, conditioned on the extractions</p>
]]></content>
      <tags>
        <tag>Knowledge Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>Scrapy学习</title>
    <url>/2020/09/22/Scrapy-learning/</url>
    <content><![CDATA[<h3 id="Scrapy-架构"><a href="#Scrapy-架构" class="headerlink" title="Scrapy 架构"></a>Scrapy 架构</h3><p><img src="/image/scrapy.png"></p>
<ol>
<li>The Engine gets the initial Requests to crawl from the Spider.(获取要爬的url)</li>
<li>The Engine schedules the Requests in the Scheduler and asks for the next Requests to crawl.（调度request到调度器中，形成一些列的url调度队列）</li>
<li>The Scheduler returns the next Requests to the Engine.（调度器调度完成，发送第一个request给引擎）</li>
<li>The Engine sends the Requests to the Downloader, passing through the Downloader Middlewares (see process_request()). （引擎发送第一个request给下载器）</li>
<li>Once the page finishes downloading the Downloader generates a Response (with that page) and sends it to the Engine, passing through the Downloader Middlewares (see process_response()).（下载器得到该request的response，返回给引擎）</li>
<li>The Engine receives the Response from the Downloader and sends it to the Spider for processing, passing through the Spider Middleware (see process_spider_input()).（引擎将response发给spider进行解析）</li>
<li>The Spider processes the Response and returns scraped items and new Requests (to follow) to the Engine, passing through the Spider Middleware (see process_spider_output()).（spider解析好需要的数据：包括：提取需要的内容+提取新的url请求，将处理好的数据发给引擎）</li>
<li>The Engine sends processed items to Item Pipelines, then send processed Requests to the Scheduler and asks for possible next Requests to crawl.（一方面引擎将得到的内容发给item，将得到的url请求发给调度器）</li>
<li>The process repeats (from step 1) until there are no more requests from the Scheduler.</li>
</ol>
<p>摘自:<a href="https://docs.scrapy.org/en/latest/topics/architecture.html">https://docs.scrapy.org/en/latest/topics/architecture.html</a></p>
<h3 id="Scrapy-使用"><a href="#Scrapy-使用" class="headerlink" title="Scrapy 使用"></a>Scrapy 使用</h3><h4 id="第一步-创建爬虫项目；"><a href="#第一步-创建爬虫项目；" class="headerlink" title="第一步: 创建爬虫项目；"></a>第一步: 创建爬虫项目；</h4><p>$scrapy startproject project_name</p>
<h4 id="第二步-设置settings"><a href="#第二步-设置settings" class="headerlink" title="第二步: 设置settings"></a>第二步: 设置settings</h4><p>settings关闭robots =》 POBOTSOXT_OBEY = True<br>…</p>
<h4 id="第三步-定义item信息"><a href="#第三步-定义item信息" class="headerlink" title="第三步: 定义item信息"></a>第三步: 定义item信息</h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">import</span> scrapy<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Test1Item</span>(<span class="hljs-params">scrapy.Item</span>):</span> <span class="hljs-comment"># ==&gt;&gt;&gt;修改item名称</span><br>    <span class="hljs-comment"># define the fields for your item here like:</span><br>    <span class="hljs-comment"># name = scrapy.Field()</span><br>    <span class="hljs-keyword">pass</span><br><br>    <span class="hljs-comment"># Items clearly define the common output data format in a separate file ,有的时候可能需要对传过来的数据进行进一步处理</span><br><br>    <span class="hljs-comment"># example:</span><br>    quote_content = Field(input_processor=MapCompose(remove_quotes),output_processor=TakeFirst())<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">remove_quotes</span>(<span class="hljs-params">text</span>):</span><br>	<span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure>

<h4 id="第四步-编写代码逻辑"><a href="#第四步-编写代码逻辑" class="headerlink" title="第四步:编写代码逻辑"></a>第四步:编写代码逻辑</h4><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><br>首先创建爬虫文件<br>cd project_name<br>scrapy genspider spider_name url<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse</span>(<span class="hljs-params">self, response</span>):</span><br>	<span class="hljs-comment">## 编写代码逻辑</span><br>	self.logger.info(<span class="hljs-string">&#x27;test...&#x27;</span>)<br><br>    <span class="hljs-comment">## 一般逻辑:</span><br>    res = response.xpath(<span class="hljs-string">&quot;xxx&quot;</span>).getall() <br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> res:<br>    	<span class="hljs-keyword">yield</span>&#123;<br>    		<span class="hljs-comment"># 放入item中，更加复杂的写法见item itemloader</span><br>    	&#125;<br>     <span class="hljs-comment">##如果需要深度搜索，具有下一页之类的</span><br>     next_page = response.xpath(xxx).get()<br>    <span class="hljs-keyword">if</span> next_page <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        next_page = response.urljoin(next_page)<br>        <span class="hljs-keyword">yield</span> scrapy.Request(next_page, callback=self.parse)  <span class="hljs-comment"># 这里的callback 也可以重新写个函数进行回调 crapy.Request(next_page, callback=self.parse2)</span><br>        <span class="hljs-comment"># 如果不进行url拼接的话，也可以直接</span><br>        <span class="hljs-keyword">yield</span> response.follow(a, callback=self.parse)<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse2</span>(<span class="hljs-params">self,response</span>):</span><br>	<span class="hljs-keyword">pass</span><br><br><span class="hljs-comment">#Ite/Itemloader</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> res:<br>	loader = ItemLoader(item=Test1Item(), selector=i)<br>	...<br><br>	item = loader.load_item() <span class="hljs-comment"># 此时item便得到了提取的数据</span><br>	<span class="hljs-keyword">yield</span> item<br></code></pre></td></tr></table></figure>

<h4 id="第五步-调试-输出"><a href="#第五步-调试-输出" class="headerlink" title="第五步:调试 输出"></a>第五步:调试 输出</h4><p>$ scrapy shell URL<br>response.xpath(xxx).getall()</p>
<p>$ scrapy crawl quotes -o xx.json</p>
]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Web Crawler</tag>
        <tag>library</tag>
      </tags>
  </entry>
  <entry>
    <title>社交网络</title>
    <url>/2020/10/23/Social-network/</url>
    <content><![CDATA[<h2 id="总览"><a href="#总览" class="headerlink" title="总览"></a>总览</h2><p>社交网络与图, 社交网络的聚类, 社区发现</p>
<h2 id="社交网络与图"><a href="#社交网络与图" class="headerlink" title="社交网络与图"></a>社交网络与图</h2><p>1.What is a network? a network can be defined as a graph in which nodes and/or edges have attributes (e.g. names). 带有属性信息的图</p>
<p>2.Network’s properties:</p>
<ul>
<li>size: # of nodes/edges</li>
<li>node degree: # of links to other nodes</li>
<li>degree distribution: probability that a randomly selected node has degree k<ul>
<li>scale-free network: network whose degree distribution follows a power law, at least asymptotically. 网络中的大部分节点只和很少节点连接，而有极少的节点与非常多的节点连接,幂律分布/长尾效应</li>
<li>Real networks are scale-free</li>
</ul>
</li>
<li>path,</li>
<li>shortest path </li>
<li>diameter: shortest path between most distant nodes/maximal shortest path<br>…</li>
</ul>
<p>3.Mathematical representation<br>Adjacency matrix : A<br>Path matrix: 可以通过邻接矩阵计算路径矩阵, A^l 结果表示为路径矩阵</p>
<p>3.Phenomena arise as a result of properties: </p>
<ul>
<li><p>Friendship paradox: on average, your friends have more friends than you do, 友谊悖论</p>
</li>
<li><p>Small world: most nodes are not neighbors of one another,but most nodes can be reached from every other by a small number of hops or steps. 小世界理论，六度空间</p>
<ul>
<li>The typical distance L between two randomly chosen nodes grows proportionally to the logarithm of the number of nodes N in the network, 两个节点的距离和整个网络的节点数的对数成正比<br><img src="/image/small_world.png"></li>
</ul>
</li>
<li><p>Perception bias in networks,网络中存在感知误差</p>
</li>
</ul>
<h2 id="社交网络的聚类"><a href="#社交网络的聚类" class="headerlink" title="社交网络的聚类"></a>社交网络的聚类</h2><p>方式:<br>1.Hierarchical</p>
<ul>
<li>Agglomerative (bottom-up):<br>  Initially, each point is a cluster，每个点都是一个cluster<br>  Repeatedly combine the two “nearest” clusters into one，不断往上去合并点，最终得到结果。<br>  Use distance metric</li>
<li>Divisive (top-down):  ==&gt;&gt; 本文关注的算法<br>  Start with one cluster and recursively split it 整个图为一个cluster，不断去细分clusters</li>
</ul>
<p>2.Point assignment<br>    Maintain a set of clusters<br>    Points belong to “nearest” cluster<br>    Use distance metric</p>
<p>社交网络中距离指标</p>
<p>1.d(x, y) is 0 if there is an edge and 1 if there is no such edge.<br>限制🚫: violate the triangle inequality<br>例子: 假设A,B,C三点,AC一边,那么 d(A,B) = d(B,C) = 0 and d(A,C) = 1 &gt; d(A,B) + d(B,C) </p>
<p>====&gt;</p>
<p>2.Shortest path distance: Minimum # of edges connecting to nodes:<br>例子: 假设A,B,C三点，边AB，BC，那么d(A,B) = d(B,C) = 1   and  d(A,C) = 2</p>
<p>The problem is more complex because the distance between data points (nodes) is not measured by Euclidian<br>由此可以看到传统的聚类方法（KNN，Kmeans）不太适用于社交网络的聚类，因为距离指标计算很复杂</p>
<p>====&gt;<br>3. Betweenness 中介性/居间性</p>
<p>Edge betweenness: # of shortest paths passing over the edge<br>边的中介性，简单理解就是经过该条边的最短路径数目</p>
<p>example:Betweenness of edge (a, b):<br>number of pairs of nodes x and y -&gt; x<br>edge (a,b) lies on the shortest path between x and y</p>
<p>However,if there are several shortest paths between x and y, edge (a,b) is credited with the fraction of those shortest paths that include edge (a,b)</p>
<p>但是!!!y-&gt;x的最短路径不只有一条，假设y-&gt;x的最短路径有3条，其中有一条进过了边(a,b),那么这一条最短路径为（a,b）的中介性贡献了1/3</p>
<p>A high score is bad: suggests that edge (a,b) runs between two different communities<br>我们可以发现如果一条边的中介性很高的话，表明a和b是属于两个不同的社区&lt;&lt;== 聚类实现</p>
<p><img src="/image/edge_betweeness.png"></p>
<h2 id="社区发现"><a href="#社区发现" class="headerlink" title="社区发现"></a>社区发现</h2><h3 id="利用betweeness的社区发现算法：Girvan-Newman-Algorithm"><a href="#利用betweeness的社区发现算法：Girvan-Newman-Algorithm" class="headerlink" title="利用betweeness的社区发现算法：Girvan-Newman Algorithm"></a>利用betweeness的社区发现算法：Girvan-Newman Algorithm</h3><p>Idea: discover communities using divisive hierarchical clustering (Start with one cluster (the social network) and recursively split it)</p>
<p>Strategy: edge betweenness(# of shortest paths passing through the edge)</p>
<p>Algorithm: Girvan-Newman Algorithm</p>
<p>Repeat until no edges are left:<br>    Calculate betweenness of edges<br>    Remove edges with highest betweenness</p>
<p>Result: Connected components are communities<br><img src="/image/GN.png"></p>
<p>但是通过上述的描述可以发现，通过不断的移除最大的edge betweeness，最后得到结果是一个个孤立的点，所以需要明确以下两点:<br>1.How to compute betweenness?<br>2.How to select the number of clusters?</p>
<h4 id="Compute-betweenness"><a href="#Compute-betweenness" class="headerlink" title="Compute betweenness"></a>Compute betweenness</h4><p>Strategy: BFS</p>
<p>1.Perform a breadth-first search (BFS) of the graph, starting at node X<br>首先对于每一个点 X，得到一个BFS tree，这个BFS tree可以反映出该点到其他所有点的最短路径</p>
<p>2.Label each node by the number of shortest paths that reach it from the root node<br>标记处每个点到root node X的最短路径数目<br><img src="/image/label_sp_num.png"></p>
<p>3.Calculate for each edge e, the sum over all nodes Y (of the fraction) of the shortest paths from the root X to Y that go through edge e<br>为每条边e计算从root nodeX到其他所有点Y的最短路径中经过e的总和(包括分数部分)</p>
<p>具体计算:<br><img src="/image/step1.png"><br>第一步: 从底部开始<br>• A and C are leaves: get credit = 1;<br>• Each of these nodes has only one parent, so their credit=1 is given to edges (B,A) and (B,C)<br>• At level 2, G is a leaf: gets credit = 1<br>• B gets credit 1 + credit of DAG edges entering from below = 1 + 1 +1 = 3<br>• B has only one parent, so edge (D,B) gets entire credit of node B = 3</p>
<p>第二步:碰见需要拆分的情况 G has 2 parents<br><img src="/image/step2.png"><br>• In this case, both D and F have just one shortest path from E to each of those nodes<br>    • So, give half credit of node G to each of those edges<br>    • Credit = 1/(1 + 1) = 0.5<br>• In general, how we distribute credit of a node to its edges depends on number of shortest paths<br>    • Say there were 5 shortest paths to D and only 3 to F<br>    • Then credit of edge (D,G) = 5/8 and credit of edge (F,G) = 3/8<br>• Node D gets credit = 1 + credits of edges below it = 1 + 3 + 0.5 = 4.5<br>• Node F gets credit = 1 + 0.5 = 1.5<br>• D has only one parent, so Edge (E,D) gets credit = 4.5 from D<br>• Likewise for F: Edge (E,F) gets credit = 1.5 from F.</p>
<p>To complete betweenness calculation, must: </p>
<ul>
<li>Repeat this for every node as root</li>
<li>Sum the contributions on each edge</li>
<li>Divide by 2 to get true betweenness<ul>
<li>since every shortest path will be counted twice, once for each of its endpoints</li>
</ul>
</li>
</ul>
<p>以上仅仅是对针对一个节点计算的各个边的betweeness，需要对每个节点重复这样的计算随后累加除以2得到最终结果。从这里也可以发现GN是针对无向图的，如果是有向图的话，就不需要除以2了。</p>
<h4 id="Select-the-number-of-clusters"><a href="#Select-the-number-of-clusters" class="headerlink" title="Select the number of clusters"></a>Select the number of clusters</h4><p>Communities: sets of tightly connected nodes</p>
<p>Modularity Q: A measure of how well a network is partitioned into communities<br>模块：衡量一个网络中社区聚类好坏的指标</p>
<blockquote>
<p>Modularity compares the number of edges inside a cluster with the expected number of edges that one would find in the cluster if the network were a random network with the same number of nodes and where each node keeps its degree, but edges are otherwise randomly attached.<br> Networks with high modularity have dense connections between the nodes within groups but sparse connections between nodes in different groups.<br> The null model is a graph which matches one specific graph in some of its structural features, but which is otherwise taken to be an instance of a random graph. The null model is used as a term of comparison, to verify whether the graph in question displays some feature, such as community structure, or not.</p>
</blockquote>
<p><img src="/image/modularity.png"></p>
<p>rclone mount GD: /nas/home/binzhang/data –allow-other –allow-non-empty –vfs-cache-mode writes</p>
<p>rclone mount GD: /nas/home/binzhang/GoogleDrive –allow-other –allow-non-empty –vfs-cache-mode writes</p>
<p> rclone copy /home/backup gdrive:backu</p>
]]></content>
      <tags>
        <tag>Data mining</tag>
      </tags>
  </entry>
  <entry>
    <title>Storage systems</title>
    <url>/2020/09/08/Storage-systems/</url>
    <content><![CDATA[<h2 id="Storage-Device"><a href="#Storage-Device" class="headerlink" title="Storage Device"></a>Storage Device</h2><h3 id="Hierarchy"><a href="#Hierarchy" class="headerlink" title="Hierarchy"></a>Hierarchy</h3><p><img src="/image/storage_hierarchy.png"></p>
<h3 id="Characters"><a href="#Characters" class="headerlink" title="Characters"></a>Characters</h3><ul>
<li>Capacity(bytes) - How much data it can hold</li>
<li>Cost($) -  Price per byte of storage</li>
<li>Bandwidth(bytes/sec) - Number of bytes that can be transferred per second;  read and write bandwidth may be different</li>
<li>Latency(sec) - Time elapsed, waiting for response/delivery of data</li>
</ul>
<h3 id="Basic-Function-CRUD"><a href="#Basic-Function-CRUD" class="headerlink" title="Basic Function : CRUD"></a>Basic Function : CRUD</h3><ul>
<li>C(reate)/write</li>
<li>R(ead)</li>
<li>U(pdate)/overwrite</li>
<li>D(elete)</li>
</ul>
<h3 id="Some-terms"><a href="#Some-terms" class="headerlink" title="Some terms"></a>Some terms</h3><ul>
<li>Access times: Time taken before drive is ready to transfer data 一般来说，物理设备（硬盘，内存..）在进行数据的转换前需要索引到目标位置， 内存-纳秒 SSD-微秒 HDD-毫秒</li>
<li>Access pattern: how storage read/write data <ul>
<li>Sequential: Data to be accessed are located next to each other or sequentially on the device</li>
<li>Random: Access data located randomly on storage device</li>
</ul>
</li>
<li>Completion Time：Time to complete an read/write operation <ul>
<li><strong>CompletionTime = Latency + Size/Bandwidth</strong></li>
<li>Depends on lots of factors(device, operation type, access pattern…)</li>
</ul>
</li>
</ul>
<p>Note: 这里主要讨论HDD和SSD</p>
<h2 id="Hard-Disk-Drive"><a href="#Hard-Disk-Drive" class="headerlink" title="Hard Disk Drive"></a>Hard Disk Drive</h2><h3 id="Organization"><a href="#Organization" class="headerlink" title="Organization"></a>Organization</h3><p><img src="/image/hdd.png"></p>
<ul>
<li>One or more spinning magnetic platters, typically two surface per platter</li>
<li>Data stored in tracks</li>
<li>Disk arm positions over the radial positon - swings across tracks but don’t extend</li>
<li>Data is read/written by disk head as platter spins</li>
</ul>
<p>Hard disk head movement while copying files between two folders：<br><a href="https://www.youtube.com/watch?v=BlB49F6ExkQ">https://www.youtube.com/watch?v=BlB49F6ExkQ</a></p>
<h3 id="Physical-characteristics"><a href="#Physical-characteristics" class="headerlink" title="Physical characteristics"></a>Physical characteristics</h3><ul>
<li>3.5” (diameter, common in desktops), 2.5” (common in laptops)</li>
<li>Rotational Speed: 4800/5400/7200/10000 RPM (rotations per minute)</li>
<li>Between 5-7 platters</li>
<li>Current capacity up to 10TB</li>
</ul>
<h3 id="Data-Storage"><a href="#Data-Storage" class="headerlink" title="Data Storage"></a>Data Storage</h3><ul>
<li>1 platter  is divided into a number of tracks</li>
<li>1 tracker is divided into N fixed size sectors<ul>
<li>sector size: 4KB</li>
<li>Entire sector is written “atomically” -&gt; sector为最小的操作单元，所以不论读写都首先进行sector的寻址</li>
</ul>
</li>
</ul>
<h3 id="Address-Method-CHS-cylinder-head-sector"><a href="#Address-Method-CHS-cylinder-head-sector" class="headerlink" title="Address Method - CHS(cylinder-head-sector)"></a>Address Method - CHS(cylinder-head-sector)</h3><h4 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h4><p>CHS is an early way to address a sector. (LBA(Logical Block Addressing) is  more common now.) </p>
<p><img src="/image/CHS.png"></p>
<p>举个例子：<br>#cylinders: 256<br>#heads: 16 (i.e., 8 platters, 2 heads/platter)<br>#sectors/track: 64<br>sector size = 4KB<br>=&gt; capacity of the drive:  2^8 * 2^6 * 2^2* 2^10 * 2^4 = 2^30 = 1GB</p>
<h4 id="address-step"><a href="#address-step" class="headerlink" title="address step"></a>address step</h4><p><strong>According to CHS, data can be located before transferring, then data can be transferred</strong></p>
<ol>
<li>Wait for the disk haed on right track    - seek time <ol>
<li>On average seek time is about  1⁄3  max seek time</li>
<li><img src="/image/seek-time.png"></li>
</ol>
</li>
<li>wait for the right sector to rotate under the head. - rotational latency <ol>
<li>On average: about 1⁄2 of time of a full rotation</li>
<li>example:  Assume 10,000 RPM (rotations per minute) 60000 ms/ 10000 rotations  = 6ms / rotation</li>
</ol>
</li>
</ol>
<h3 id="Data-Operation"><a href="#Data-Operation" class="headerlink" title="Data Operation"></a>Data Operation</h3><blockquote>
<p>T = T_seek + T_rotation + T_transfer<br>T_seek : Time to get the disk head on right track<br>T_rotation :Time to wait for the right sector to rotate under the head<br>T_transfer: Time to actually transfer data</p>
</blockquote>
<h4 id="T-transfer"><a href="#T-transfer" class="headerlink" title="T_transfer"></a>T_transfer</h4><p>Assume that data will be transferred:  512KB, 128 MB/sec transmission bandwidth<br>Transfer time:  512KB/128MB * 1000ms = 4ms</p>
<h4 id="Actual-Bandwidth"><a href="#Actual-Bandwidth" class="headerlink" title="Actual Bandwidth"></a>Actual Bandwidth</h4><p>Actual Bandwidth = data / actual time ，所以一般情况下实际带宽会小</p>
<h4 id="数据传输中的block和sector区分"><a href="#数据传输中的block和sector区分" class="headerlink" title="数据传输中的block和sector区分"></a>数据传输中的block和sector区分</h4><ul>
<li>Sector is the  basic unit of hard disk dirve</li>
<li>Block is the basic unit of file system</li>
<li>Block has 1 or more sectors  （in this course, assuming one block = one sector）</li>
</ul>
<p>硬盘本身没有block的概念，block概念存在于文件系统的概念中，文件系统是一个块一个块的读取数据，如果是按照一个sector一个sector的来读数据，太慢了，所以才有了block这样一个逻辑块的概念。</p>
<h4 id="不同access-pattern对读写的影响"><a href="#不同access-pattern对读写的影响" class="headerlink" title="不同access pattern对读写的影响"></a>不同access pattern对读写的影响</h4><ul>
<li>Sequential operation:<ul>
<li>May assume all sectors involved are on the same track<br>– need to seek to the right track or rotate to the first sector （一次seektime）<br>– But no rotation/seeking needed afterwardSSD</li>
</ul>
</li>
<li>Random operation:  <ul>
<li>May assume all sectors are on different tracks and sectors （多次seektime）</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">example: 7ms avg seek,  10,000 RPM  50 MB/sec transfer rate 4KB/block<br>Sequential access of 10 MB:<br>– Completion time = 7ms + 60*1000/10000/2 ms + 10/50 *1000 ms = 210ms<br>– Actual bandwidth = 10MB/210ms = 47.62 MB/s<br><br>Random access of 10 MB <br>– block numbers: 10*1000/4 = 2500  (assume 1 block = 1 sector)<br>– Completion time = 2500 * (7 + 3 + 4/50) = 25.2s<br>– Actual bandwidth = 10MB / 25.2s = 0.397 MB/s<br></code></pre></td></tr></table></figure>

<h2 id="Soild-State-Drive"><a href="#Soild-State-Drive" class="headerlink" title="Soild State Drive"></a>Soild State Drive</h2><h3 id="Organization-1"><a href="#Organization-1" class="headerlink" title="Organization"></a>Organization</h3><p><img src="/image/ssd.png"></p>
<p>SSD contains a number of flash memory chips</p>
<p><img src="/image/chip.png"></p>
<p>chip -&gt; dies -&gt; planes -&gt; blocks -&gt; pages (rows) -&gt; cells</p>
<h3 id="Characteristics"><a href="#Characteristics" class="headerlink" title="Characteristics"></a>Characteristics</h3><ul>
<li>All electronic, made from flash memory</li>
<li>Limited lifetime, can only write a limited number of times.</li>
<li>More expensive, less capacity  - 3 times or more expensive</li>
<li>Significantly better latency: no seek or rotational delay</li>
<li>Much better performance on random (however, write has much higher latency than read )</li>
</ul>
<h3 id="Data-Storage-1"><a href="#Data-Storage-1" class="headerlink" title="Data Storage"></a>Data Storage</h3><ul>
<li>Cells are made of floating-gate transistors : By applying high positive/negative voltage to control gate, electrons can be attracted to or repelled from floating gate<ul>
<li>State = 1, if no electrons in the floating gate</li>
<li>State = 0, if there are electrons (negative charges)<br>– Electrons stuck there even when power is off<br>– So state is retained</li>
</ul>
</li>
<li>Data in SSD are represented by the ‘01010…’ formats, that is the state of the electrons</li>
</ul>
<p><img src="/image/floating-gate.png"></p>
<h3 id="Operations"><a href="#Operations" class="headerlink" title="Operations"></a>Operations</h3><h4 id="Read"><a href="#Read" class="headerlink" title="Read"></a>Read</h4><ul>
<li>Electrons on the floating gate affect the threshold voltage for the floating gate transistor to conduct</li>
<li>Higher voltage needed when gate has electrons</li>
</ul>
<p><img src="/image/ssd_read.png"></p>
<p>Steps: </p>
<ol>
<li>Apply Vint (intermediate voltage) </li>
<li>If the current is detected, gate has no electrons=&gt; bit = 1</li>
<li>If no current, gate must have electrons =&gt; bit = 0</li>
</ol>
<p><u><strong>Page is the smallest unit that can be read</strong></u></p>
<h4 id="Write-and-Erase"><a href="#Write-and-Erase" class="headerlink" title="Write and Erase"></a>Write and Erase</h4><h4 id="Write-and-erase"><a href="#Write-and-erase" class="headerlink" title="Write and erase"></a>Write and erase</h4><ul>
<li>Write: 1 =&gt; 0 (get electron)<ul>
<li>Apply high positive voltage (&gt;&gt; voltage for read) to the control gate</li>
<li>Attract electrons from channel to floating gate (through quantum tunneling)</li>
<li>Page is the smallest unit for write</li>
</ul>
</li>
<li>Erase: 0 =&gt; 1 (make electrons empty)<ul>
<li>Need to apply much higher negative voltage to the control gate</li>
<li>Get rid of electrons from floating gate</li>
<li>May stress surrounding cells(dangerous to do on individual pages)</li>
<li>Block is the smallest unit for erase</li>
</ul>
</li>
</ul>
<h3 id="P-E-cycle"><a href="#P-E-cycle" class="headerlink" title="P/E cycle"></a>P/E cycle</h3><blockquote>
<p>P/E cycle: Data is written to cells (P) and then erased (E)<br>Every write &amp; erase damages oxide layer surrounding the floating-gate to some extent</p>
</blockquote>
<ul>
<li>Page is the smallest unit for read and write (write is also called program, 1-&gt;0)</li>
<li>Block is the smallest unit for erase (0-&gt;1) – i.e., make cells “empty” (i.e., no electrons)  (关于为什么使用block作为最小擦除单元：SSD的物理结构导致，擦除过程会作用到整个block施加高电压，将电子吸引出来)</li>
</ul>
]]></content>
      <tags>
        <tag>Operating System</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统总览</title>
    <url>/2020/10/03/recommendation-system/</url>
    <content><![CDATA[<h1 id="效用矩阵-utility-matrix"><a href="#效用矩阵-utility-matrix" class="headerlink" title="效用矩阵 (utility matrix)"></a>效用矩阵 (utility matrix)</h1><p>一般的推荐系统中有两类元素，一类是user，一类是item(例如电影、音乐…)，可以用一个矩阵表示这些数据<br>每一行表示每一个user，每一列表示一个item, <code>M[i][j]</code>表示第i个用户对第j项的评分。</p>
<p>一般效用矩阵都是稀疏的，需要做的就是填充这些缺失值</p>
<ul>
<li>Gathering known ratings for matrix</li>
<li>Extrapolate unknown ratings from the known ones</li>
<li>Evaluating extrapolation methods</li>
</ul>
<h1 id="长尾效应-The-Long-tail"><a href="#长尾效应-The-Long-tail" class="headerlink" title="长尾效应 (The Long tail)"></a>长尾效应 (The Long tail)</h1><p>物理世界和在线世界的差别称为长尾现象，长尾现象要求互联网必须对每个用户进行推荐。</p>
<ul>
<li>实际的物理实体店中受限于空间，只能推荐畅销项目。</li>
<li>互联网推荐系统不受空间限制，可以推荐所有项目。</li>
</ul>
<p>关于在线世界与物理世界差异的例子:<br>What percentage of the top 10,000 titles in any online media store (Netflix, iTunes, Amazon, or any other) will rent or sell at least once a month?<br>Most people guess 20 percent.(80-20 rule, also known as Pareto’s principle (1896))<br>The right answer: 99 percent.(Demand for nearly every one of those top 10,000 titles.)</p>
<h1 id="推荐算法分类"><a href="#推荐算法分类" class="headerlink" title="推荐算法分类"></a>推荐算法分类</h1><h2 id="基于内容-Content-based"><a href="#基于内容-Content-based" class="headerlink" title="基于内容(Content based)"></a>基于内容(Content based)</h2><blockquote>
<p>思想:关注item的属性，计算各个item之间的相似度来进行推荐。 Recommend items to customer x that are similar to previous items rated highly by x.</p>
</blockquote>
<p><img src="/image/content-based.png"></p>
<h3 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h3><p>基于内容的推荐系统是建立在用户给出的数据之上的，用户给出的数据要么是对物品的直接评分或者是点击或者浏览物品，通过这些数据可以建立出基于这个用户的基本信息(user profile),后续对该用户物品推荐是基于其user profile，user profile越完善，推荐系统的效果越好。</p>
<h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><ul>
<li>Construct item profiles  (构建描述item的向量)<ul>
<li>特征可以是明确的属性值，也可以是从文档中提取出来的特征</li>
</ul>
</li>
<li>Construct user profiles （构建描述user喜好的向量）<ul>
<li>将user的偏好也表示成同一空间下的向量</li>
</ul>
</li>
<li>Recommend items to users based on content</li>
</ul>
<h3 id="优劣"><a href="#优劣" class="headerlink" title="优劣"></a>优劣</h3><p>优点：</p>
<ol>
<li>不需要其他user的数据</li>
<li>可以根据user的独特喜好来推荐item，也就是说可以推荐一些小众的item</li>
<li>推荐的原因可以很容易被解释<br>缺点：</li>
<li>内容的特征提取的问题，如何才能提取出最有用的特征（requires a lot of domain knowledge.）</li>
<li>只能推荐与用户历史数据中相似的item（model has limited ability to expand on the users’ existing interests）</li>
<li>无法利用其他用户的数据</li>
</ol>
<h2 id="协同过滤-Collaborative-filtering"><a href="#协同过滤-Collaborative-filtering" class="headerlink" title="协同过滤(Collaborative filtering)"></a>协同过滤(Collaborative filtering)</h2><blockquote>
<p>思想:关注item与user之间的关系，先识别相似用户，然后基于相似用户进行相似项目进行推荐。 Suggestions made to a user utilizing information across the entire user base</p>
</blockquote>
<p><img src="/image/CF.png"></p>
<h3 id="工作原理-1"><a href="#工作原理-1" class="headerlink" title="工作原理"></a>工作原理</h3><p>基于协同过滤的推荐系统是建立在许多用户的数据之上的，通过分析这些用户之间的相似度，发掘相似的用户,（假设用户A和B相似），推荐A一些B买过的东西。其主要精髓在于:物以类聚(item-based)，人以群分(user-based)。<br>It’s based on the idea that people who agree in their evaluations of certain items in the past are likely to agree again in the future.</p>
<p>关于一点思考：Users only see what they are expected to like.</p>
<h3 id="协同过滤的两类算法"><a href="#协同过滤的两类算法" class="headerlink" title="协同过滤的两类算法"></a>协同过滤的两类算法</h3><h4 id="Memory-based-approach"><a href="#Memory-based-approach" class="headerlink" title="Memory-based approach"></a>Memory-based approach</h4><p><img src="/image/CF-2.png"></p>
<h5 id="User-based-CF-k-nearest-neighbor-collaborative-filtering"><a href="#User-based-CF-k-nearest-neighbor-collaborative-filtering" class="headerlink" title="User-based CF (k-nearest neighbor collaborative filtering)"></a>User-based CF (k-nearest neighbor collaborative filtering)</h5><ul>
<li>思想: 为用户A找到相似用户，然后基于相似用户进行相似项目推荐</li>
<li>步骤:<ul>
<li>根据与A的相似度来为A的相似用户分配权重 </li>
<li>选择topk相似用户 </li>
<li>基于topk相似用户对于item的评分去预测A对item的评分 </li>
</ul>
</li>
<li>例子: 假设A有3个相似用户，B(sim=0.8, rating(h)=5),C(sim=0.3,rating(h)=4),D(sim=0.5,rating(h)=3) 预测A对于h的评分<ul>
<li>r_a(h) = 0.8x5+0.3x4+0.5x3 / (0.8+0.3+0.5) = 4.2</li>
</ul>
</li>
</ul>
<h5 id="Item-based-CF"><a href="#Item-based-CF" class="headerlink" title="Item-based CF"></a>Item-based CF</h5><ul>
<li>思想: 为物品x找相似物品，然后基于用户之前喜欢/买过的物品为其推荐最相似的物品</li>
<li>例子: 为电影进行评分时, similar items will be rated similarly by the same user</li>
</ul>
<h4 id="Model-based-approaches"><a href="#Model-based-approaches" class="headerlink" title="Model-based approaches"></a>Model-based approaches</h4><p><img src="/image/model-based-CF.png"></p>
<h5 id="矩阵分解"><a href="#矩阵分解" class="headerlink" title="矩阵分解"></a>矩阵分解</h5><p><img src="/image/MF.png"></p>
<h3 id="CF优劣"><a href="#CF优劣" class="headerlink" title="CF优劣"></a>CF优劣</h3><p>优点：</p>
<ol>
<li>利用了别的user的数据</li>
<li>不需要特征选择（不需要domain knowledge）</li>
</ol>
<p>缺点：</p>
<ol>
<li>需要足够的数据支持</li>
<li>稀疏，构建出的utility matrix稀疏且庞大</li>
<li>无法推荐从未被评级的item</li>
<li>不能根据某个用户的特有品位进行推荐</li>
</ol>
<h2 id="混合方法-Hybrid"><a href="#混合方法-Hybrid" class="headerlink" title="混合方法(Hybrid)"></a>混合方法(Hybrid)</h2>]]></content>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>Scrapy 实例 - TripAdvisor信息抓取</title>
    <url>/2020/10/15/scrapy-example/</url>
    <content><![CDATA[<h1 id="TripAdvisor-Scrapy-实例"><a href="#TripAdvisor-Scrapy-实例" class="headerlink" title="TripAdvisor Scrapy 实例"></a>TripAdvisor Scrapy 实例</h1><h2 id="爬虫需求"><a href="#爬虫需求" class="headerlink" title="爬虫需求"></a>爬虫需求</h2><p>爬取洛杉矶地区的餐厅和酒店的相关信息</p>
<p>餐厅信息包括:</p>
<ul>
<li>餐厅名称</li>
<li>餐厅评星（几颗星）</li>
<li>餐厅排名（针对洛杉矶而言）</li>
<li>餐厅价格区间</li>
<li>餐厅菜品种类</li>
<li>餐厅地点</li>
<li>附近旅馆</li>
<li>附近餐厅</li>
<li>附近景点</li>
<li>用户评价</li>
</ul>
<p>酒店信息包括：</p>
<ul>
<li>酒店名称</li>
<li>酒店价格区间</li>
<li>酒店房间数</li>
<li>酒店评星</li>
<li>酒店设施</li>
<li>酒店特色</li>
<li>酒店星级</li>
<li>酒店房间类型</li>
<li>酒店风格</li>
<li>酒店地点</li>
<li>附近餐厅</li>
<li>附近景点</li>
<li>用户评价</li>
</ul>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>Scrapy + MongoDB<br>代码: xxxx</p>
<h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><ol>
<li>使用scrapd + spiderkeeper 进行可视化部署</li>
<li>log文件保存(settings.py中设置LOG_FILE  = “debug.log”)</li>
<li>使用 scrapy shell 进行xpath语法规则的调试以确保得到想要的结果</li>
</ol>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>10522 条餐厅数据 (50M)<br>1000 条酒店数据  (100M)</p>
]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Web Crawler</tag>
      </tags>
  </entry>
  <entry>
    <title>spaCy学习</title>
    <url>/2020/09/10/spacy-learning/</url>
    <content><![CDATA[<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><blockquote>
<p>spaCy (/speɪˈsiː/) is an open-source software library for advanced natural language processing written in the programming languages Python and Cython.</p>
</blockquote>
<p>学习资源: <a href="https://spacy.io/">https://spacy.io/</a>  <a href="https://course.spacy.io/en">https://course.spacy.io/en</a></p>
<h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><ul>
<li>nlp   : contains the processing pipeline; includes language-specific rules for tokenization etc.</li>
<li>doc   : access information about the text in a structured way, and no information is lost.</li>
<li>token : tokens in a document – for example, a word or a punctuation character.</li>
<li>span  : a slice of the document consisting of one or more tokens</li>
<li>lexical attributes : <code>token.i</code>, <code>token.text</code>, <code>token.is_alpha</code>, <code>token.is_punct</code>, <code>token.like_num</code>… They refer to the entry in the vocabulary and don’t depend on the token’s context.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入对应语言类 Note: Some language tokenizers require external dependencies. </span><br><span class="hljs-keyword">from</span> spacy.lang.zh <span class="hljs-keyword">import</span> Chinese   <span class="hljs-comment"># https://spacy.io/usage/models 查看lang后面对应的字符</span><br><br><span class="hljs-comment"># 创建nlp实例</span><br>nlp = Chinese() 	<br><br><span class="hljs-comment"># 使用nlp对象处理一段文本并生成doc实例		</span><br><span class="hljs-comment"># When you call nlp on a string, spaCy first tokenizes the text and creates a document object.	  </span><br>doc = nlp(<span class="hljs-string">&quot;这是一个句子。&quot;</span>) <span class="hljs-comment"># 底层调用 __call__方法					</span><br>  <br><span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> doc:  <span class="hljs-comment"># 遍历doc实例中的token							</span><br>    print(token.text)  <br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">这是</span><br><span class="hljs-string">一个</span><br><span class="hljs-string">句子</span><br><span class="hljs-string">。</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment"># 使用索引获得某一个token</span><br>specific_token = doc[<span class="hljs-number">1</span>]	<br><br><span class="hljs-comment"># 使用切片获得doc片段					</span><br>span = doc[<span class="hljs-number">1</span>:<span class="hljs-number">3</span>]								<br><br><span class="hljs-comment"># token的一些属性</span><br>doc = nlp(<span class="hljs-string">&quot;我花了20¥买了一个汉堡包。&quot;</span>)<br>print(<span class="hljs-string">&quot;Index:   &quot;</span>, [token.i <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> doc])<br>print(<span class="hljs-string">&quot;Text:    &quot;</span>, [token.text <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> doc])<br>print(<span class="hljs-string">&quot;is_alpha:&quot;</span>, [token.is_alpha <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> doc])<br>print(<span class="hljs-string">&quot;is_punct:&quot;</span>, [token.is_punct <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> doc])<br>print(<span class="hljs-string">&quot;like_num:&quot;</span>, [token.like_num <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> doc])<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">Index:    [0, 1, 2, 3, 4, 5, 6, 7, 8]</span><br><span class="hljs-string">Text:     [&#x27;我花&#x27;, &#x27;了&#x27;, &#x27;20&#x27;, &#x27;¥&#x27;, &#x27;买&#x27;, &#x27;了&#x27;, &#x27;一个&#x27;, &#x27;汉堡包&#x27;, &#x27;。&#x27;]</span><br><span class="hljs-string">is_alpha: [True, True, False, False, True, True, True, True, False]</span><br><span class="hljs-string">is_punct: [False, False, False, False, False, False, False, False, True]</span><br><span class="hljs-string">like_num: [False, False, True, False, False, False, False, False, False]</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure>

<h3 id="统计模型"><a href="#统计模型" class="headerlink" title="统计模型"></a>统计模型</h3><p>来源: Models are trained on large datasets of labeled example texts.<br>作用: 词性标注 (Part-of-speech tags), 依存关系解析 (Syntactic dependencies), 命名实体识别 (Named entities)<br>优化: Can be updated with more examples to fine-tune predictions</p>
<h3 id="模型包"><a href="#模型包" class="headerlink" title="模型包"></a>模型包</h3><p>spaCy提供了很多预训练好的模型包，可以使用<code>spacy download</code>进行下载 <a href="https://spacy.io/usage/models">https://spacy.io/usage/models</a></p>
<p>The package provides </p>
<ul>
<li>binary weights that enable spaCy to make predictions;</li>
<li>vocabulary, and meta information to tell spaCy which language class to use;</li>
<li>how to configure the processing pipeline;</li>
</ul>
<p>导入模型包和通过初始化语言类的nlp实例区别？</p>
<ul>
<li>spacy.lang.xxx中包含其实是特定语言的代码和规则，包括停用词，数字之类,简单认为就是最基础的一些规则</li>
<li>模型包是建立在已经对大量的标记数据进行训练后得到的统计模型，简单认为是基于spacy.lang.xxx中某一种语言最基础规则之上,对大量数据进行训练学习得到的更多规则</li>
</ul>
<h3 id="词性标注-Part-of-speech-tags"><a href="#词性标注-Part-of-speech-tags" class="headerlink" title="词性标注 (Part-of-speech tags)"></a>词性标注 (Part-of-speech tags)</h3><p>通过标注每一个token的词性，为后于文本处理提供基础保证，<code>.pos_</code>属性返回词性标注的结果</p>
<h3 id="依存关系解析-Syntactic-dependencies"><a href="#依存关系解析-Syntactic-dependencies" class="headerlink" title="依存关系解析 (Syntactic dependencies)"></a>依存关系解析 (Syntactic dependencies)</h3><p>由于已经对于token做了词性标注，由此可以进一步来进行词与词之间的关系的预测。比如一个词是某一个句子或者物体的主语。<code>.dep_</code>属性返回预测的依存关系标注,<code>.head</code>属性返回当前token的parent token(也就是依存的那一个token)</p>
<p>关于依存关系:<br>20世纪70年代，Robinson提出依存语法中关于依存关系的四条公理：</p>
<ul>
<li>一个句子中只有一个成分是独立的；</li>
<li>其它成分直接依存于某一成分；</li>
<li>任何一个成分都不能依存与两个或两个以上的成分；</li>
<li>如果A成分直接依存于B成分，而C成分在句中位于A和B之间，那么C或者直接依存于B，或者直接依存于A和B之间的某一成分；</li>
</ul>
<p>依存结构是加标签的有向图，箭头从中心词指向从属，具体来说，箭头是从head指向child,举个例子： I love you,中 I 依存于 love,所以会有一条有向边从love指向I<br><img src="/image/dependency_scapy.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 可视化依存关系图</span><br><span class="hljs-keyword">from</span> spacy <span class="hljs-keyword">import</span> displacy<br>options = &#123;<span class="hljs-string">&quot;distance&quot;</span>: <span class="hljs-number">120</span>&#125;<br>displacy.render(doc, style=<span class="hljs-string">&quot;dep&quot;</span>, options=options)<br></code></pre></td></tr></table></figure>

<p>常用的依存标签：<br>root:  中心词,一般为动词<br>nsubj: nominal subject,名词性主语<br>dobj:  direct object, 直接宾语<br>prep: 介词<br>pobj: 介词宾语<br>cc: 连词<br>compound: 复合词<br>advmod: 状语<br>det: 限定词<br>amod: 形容词修饰语</p>
<h3 id="命名实体识别-Named-entities"><a href="#命名实体识别-Named-entities" class="headerlink" title="命名实体识别 (Named entities)"></a>命名实体识别 (Named entities)</h3><p><code>doc.ents</code>读取模型预测出的所有命名实体,例如Bill Gates,使用<code>.label_</code>属性打印出实体标签，例如Person</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python">$ python -m spacy download zh_core_web_sm <br><br><span class="hljs-keyword">import</span> spacy<br>nlp = spacy.load(<span class="hljs-string">&quot;en_core_web_md&quot;</span>)  <span class="hljs-comment"># load a model package by name and returns an nlp object.</span><br><br><span class="hljs-comment"># Process a text</span><br>doc = nlp(<span class="hljs-string">&quot;I love you!&quot;</span>)<br><br><span class="hljs-comment"># Iterate over the tokens</span><br><span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> doc:<br>    <span class="hljs-comment"># Print the text and the predicted part-of-speech tag</span><br>    print(token.text, token.pos_)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">I PRON</span><br><span class="hljs-string">love VERB</span><br><span class="hljs-string">you PRON</span><br><span class="hljs-string">! PUNCT</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> doc:<br>    print(token.text, token.pos_, token.dep_, token.head.text)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">I PRON nsubj love</span><br><span class="hljs-string">love VERB ROOT love</span><br><span class="hljs-string">you PRON dobj love</span><br><span class="hljs-string">! PUNCT punct love</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment"># Process a text</span><br>doc = nlp(<span class="hljs-string">&quot;Apple is looking at buying U.K. startup for $1 billion&quot;</span>)<br><br><span class="hljs-comment"># Iterate over the predicted entities</span><br><span class="hljs-keyword">for</span> ent <span class="hljs-keyword">in</span> doc.ents:<br>    <span class="hljs-comment"># Print the entity text and its label</span><br>    print(ent.text, ent.label_)<br><br><span class="hljs-comment"># Process a text</span><br>doc = nlp(<span class="hljs-string">&quot;Microsoft Corporation is an American multinational technology company with headquarters in Redmond, Washington.&quot;</span>)<br><br><span class="hljs-comment"># Iterate over the predicted entities</span><br><span class="hljs-keyword">for</span> ent <span class="hljs-keyword">in</span> doc.ents:<br>    <span class="hljs-comment"># Print the entity text and its label</span><br>    print(ent.text, ent.label_)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">Microsoft Corporation ORG</span><br><span class="hljs-string">American NORP</span><br><span class="hljs-string">Redmond GPE</span><br><span class="hljs-string">Washington GPE</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment"># Get quick definitions of the most common tags and labels.</span><br>spacy.explain(<span class="hljs-string">&#x27;NORP&#x27;</span>)<br><span class="hljs-string">&#x27;Nationalities or religious or political groups&#x27;</span><br></code></pre></td></tr></table></figure>


<h3 id="基于规则的匹配抽取"><a href="#基于规则的匹配抽取" class="headerlink" title="基于规则的匹配抽取"></a>基于规则的匹配抽取</h3><p>使用预训练模型的缺陷：模型的预测是否正确取决于训练数据和处理的文本，如果处理的文本中许多规则没有被模型包含，则预测的精度不会很高。<br>解决方案: 使用预训练的模型 + 添加相应的规则</p>
<p>Match patterns</p>
<ul>
<li>Lists of dictionaries, one per token  [{},{},{}]</li>
<li>Match exact token texts</li>
<li>Match lexical attributes</li>
<li>Match any token attributes </li>
</ul>
<p>Match step:</p>
<ol>
<li>Import the <code>Matcher</code> from <code>spacy.matcher</code>.</li>
<li>Initialize it with the <code>nlp</code> object’s shared <code>vocab</code>.</li>
<li>Create patterns</li>
<li>Use the <code>matcher.add</code> to add the pattern to the matcher.</li>
<li>Call the matcher on the <code>doc</code> and store the result in the variable matches.</li>
<li>Iterate over the matches and get the matched span from the <code>start</code> to the <code>end</code> index.</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> spacy<br><span class="hljs-keyword">from</span> spacy.matcher <span class="hljs-keyword">import</span> Matcher <span class="hljs-comment"># # Import the Matcher</span><br><br>nlp = spacy.load(<span class="hljs-string">&quot;en_core_web_md&quot;</span>)<br><br><span class="hljs-comment"># Initialize the matcher with the shared vocab</span><br>matcher = Matcher(nlp.vocab)<br><br><span class="hljs-comment"># Add the pattern to the matcher</span><br>pattern = [&#123;<span class="hljs-string">&quot;TEXT&quot;</span>: <span class="hljs-string">&quot;iPhone&quot;</span>&#125;, &#123;<span class="hljs-string">&quot;TEXT&quot;</span>: <span class="hljs-string">&quot;X&quot;</span>&#125;]<br>matcher.add(<span class="hljs-string">&quot;IPHONE_PATTERN&quot;</span>, <span class="hljs-literal">None</span>, pattern)<br><br><br><span class="hljs-comment"># 其实一个pattern就是一个或者多个token的组合，然后&#123;&#125;表示这个token需要满足的条件</span><br><span class="hljs-comment">#例子：</span><br>pattern_example = [<br>    &#123;<span class="hljs-string">&quot;LEMMA&quot;</span>: <span class="hljs-string">&quot;like&quot;</span>, <span class="hljs-string">&quot;POS&quot;</span>: <span class="hljs-string">&quot;VERB&quot;</span>&#125;,<br>    &#123;<span class="hljs-string">&quot;POS&quot;</span>: <span class="hljs-string">&quot;NOUN&quot;</span>&#125;<br>]<br><br><span class="hljs-comment"># Process some text</span><br>doc = nlp(<span class="hljs-string">&quot;Upcoming iPhone X release date leaked&quot;</span>)<br><br><span class="hljs-comment"># Call the matcher on the doc</span><br>matches = matcher(doc)<br><br><span class="hljs-comment"># Iterate over the matches</span><br><span class="hljs-keyword">for</span> match_id, start, end <span class="hljs-keyword">in</span> matches:<br>    <span class="hljs-comment"># Get the matched span</span><br>    matched_span = doc[start:end]<br>    print(matched_span.text)<br>    print(nlp.vocab.strings[match_id])  <span class="hljs-comment"># 通过match_id 获取当初加入pattern的名称)</span><br><br><span class="hljs-string">&#x27;iPhone X&#x27;</span><br><span class="hljs-string">&#x27;IPHONE_PATTERN&#x27;</span><br><br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">&#123;&quot;OP&quot;: &quot;!&quot;&#125;	Negation: match 0 times</span><br><span class="hljs-string">&#123;&quot;OP&quot;: &quot;?&quot;&#125;	Optional: match 0 or 1 times</span><br><span class="hljs-string">&#123;&quot;OP&quot;: &quot;+&quot;&#125;	Match 1 or more times</span><br><span class="hljs-string">&#123;&quot;OP&quot;: &quot;*&quot;&#125;	Match 0 or more times</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure>

<h2 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h2><h3 id="Vocab-共享词汇表"><a href="#Vocab-共享词汇表" class="headerlink" title="Vocab:共享词汇表"></a>Vocab:共享词汇表</h3><ul>
<li>spaCy把所有共享数据都存在一个词汇表Vocab中</li>
<li>所有的字符串都被编码成哈希ID</li>
<li>Vocab库为一个双向的查询表，字符串-&gt;hash值； hash值-&gt;字符串</li>
<li>哈希不能逆求解,通过hash值检索某一个不在Vocab中的字符串会报错</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python">doc = nlp(<span class="hljs-string">&quot;I love you&quot;</span>)<br>print(<span class="hljs-string">&quot;hash value:&quot;</span>, nlp.vocab.strings[<span class="hljs-string">&quot;love&quot;</span>])<br>print(<span class="hljs-string">&quot;string value:&quot;</span>, nlp.vocab.strings[<span class="hljs-number">3702023516439754181</span>])<br><br><span class="hljs-comment">#一个Doc实例也可以暴露出它的词汇表和字符串</span><br><span class="hljs-comment">#另一种方式</span><br>print(<span class="hljs-string">&quot;string value:&quot;</span>, doc.vocab.strings[<span class="hljs-number">3702023516439754181</span>])<br></code></pre></td></tr></table></figure>

<h3 id="Lexeme-语素"><a href="#Lexeme-语素" class="headerlink" title="Lexeme:语素"></a>Lexeme:语素</h3><p>Lexeme（语素）是词汇表中和语境无关的元素,代表着一个词的和语境无关的信息，比如文本本身，或者是这个词是否包含了英文字母,Lexeme中没有词性标注、依存关系或者实体标签这些和语境关联的信息。<br>在词汇表中查询一个字符串或者一个哈希ID就会获得一个lexeme。<code>lexeme.text</code>表示其文本, <code>lexeme.orth</code>表示其hash值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python">doc = nlp(<span class="hljs-string">&quot;I love you&quot;</span>)<br>lexeme = nlp.vocab[<span class="hljs-string">&#x27;love&#x27;</span>]<br><br><span class="hljs-comment"># 打印词汇的属性</span><br>print(lexeme.text, lexeme.orth, lexeme.is_alpha)<br></code></pre></td></tr></table></figure>

<p>一句话总结就是：doc中的每一个token对应一个lexeme,其中保存着对应的文本信息和hash值,要拿到文本信息，首先去Vocab中查找对应的hash值。</p>
<h3 id="Doc-Span的手动创建"><a href="#Doc-Span的手动创建" class="headerlink" title="Doc, Span的手动创建"></a>Doc, Span的手动创建</h3><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> spacy.tokens <span class="hljs-keyword">import</span> Doc,Span<br>words = [<span class="hljs-string">&quot;Hello&quot;</span>, <span class="hljs-string">&quot;world&quot;</span>, <span class="hljs-string">&quot;!&quot;</span>]<br>spaces = [<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>]<br>doc = Doc(nlp.vocab, words=words, spaces=spaces) <br><span class="hljs-comment"># 三个参数：共享的词汇表，词汇和空格</span><br><br>span = Span(doc, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br> <br><span class="hljs-comment"># 创建一个带标签的span  这里的带标签很有用！</span><br>span_with_label = Span(doc, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, label=<span class="hljs-string">&quot;GREETING&quot;</span>)<br><span class="hljs-comment"># 把span加入到doc.ents中  doc.ents是可写的,可以人为添加</span><br>doc.ents = [span_with_label]<br></code></pre></td></tr></table></figure>

<h3 id="语义相似度"><a href="#语义相似度" class="headerlink" title="语义相似度"></a>语义相似度</h3><p><code>Doc.similarity()</code>、<code>Span.similarity()</code>和<code>Token.similarity()</code></p>
<p>相似度的计算方式:</p>
<ul>
<li>通过词向量计算的，词向量是一个词汇的多维度的语义表示</li>
<li>词向量是用诸如Word2Vec算法在大规模语料上面生成的</li>
<li>spaCy默认返回两个向量的余弦相似度</li>
<li>Doc和Span的向量默认是由其token的平均值得到的</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#查看词向量</span><br>doc = nlp(<span class="hljs-string">&quot;I have a banana&quot;</span>)<br><span class="hljs-comment"># 通过token.vector属性获取向量</span><br>print(doc[<span class="hljs-number">3</span>].vector)<br><br><span class="hljs-comment">## 文档和文档相似度</span><br>doc1 = nlp(<span class="hljs-string">&quot;I like fast food&quot;</span>)<br>doc2 = nlp(<span class="hljs-string">&quot;I like pizza&quot;</span>)<br>print(doc1.similarity(doc2))<br><br><span class="hljs-comment">## token和token相似度</span><br>doc = nlp(<span class="hljs-string">&quot;I like pizza and pasta&quot;</span>)<br>token1 = doc[<span class="hljs-number">2</span>]<br>token2 = doc[<span class="hljs-number">4</span>]<br>print(token1.similarity(token2))<br><br><span class="hljs-comment">## 文档和token相似度</span><br>doc = nlp(<span class="hljs-string">&quot;I like pizza&quot;</span>)<br>token = nlp(<span class="hljs-string">&quot;soap&quot;</span>)[<span class="hljs-number">0</span>]<br>print(doc.similarity(token))<br><br><span class="hljs-comment">## 文档和span相似度</span><br>span = nlp(<span class="hljs-string">&quot;I like pizza and pasta&quot;</span>)[<span class="hljs-number">2</span>:<span class="hljs-number">5</span>] <span class="hljs-comment"># span.text = pizza and pasta</span><br>doc = nlp(<span class="hljs-string">&quot;McDonalds sells burgers&quot;</span>)<br>print(span.similarity(doc))<br></code></pre></td></tr></table></figure>

<h3 id="模型-规则"><a href="#模型-规则" class="headerlink" title="模型 + 规则"></a>模型 + 规则</h3><blockquote>
<p>将统计模型与规则系统结合使用，是自然语言处理工具箱里面最强大的方法之一</p>
</blockquote>
<p>模型: 根据训练语料库得到一系列规则的集合,spaCy功能一般包括:实体识别器、依存句法识别器、词性标注器<br>规则: 人为制定的模式匹配，适用于例子较少的情况, spaCy功能一般包括:分词器, Matcher, PhraseMatcher</p>
<p>PhraseMatcher:传进一个Doc实例而不是字典列表作为模板</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> spacy.matcher <span class="hljs-keyword">import</span> PhraseMatcher<br><br>matcher = PhraseMatcher(nlp.vocab)<br><br>pattern = nlp(<span class="hljs-string">&quot;Golden Retriever&quot;</span>)  <span class="hljs-comment"># 和Matcher不同的地方</span><br>matcher.add(<span class="hljs-string">&quot;DOG&quot;</span>, <span class="hljs-literal">None</span>, pattern)<br>doc = nlp(<span class="hljs-string">&quot;I have a Golden Retriever&quot;</span>)<br><br><span class="hljs-comment"># 遍历匹配结果</span><br><span class="hljs-keyword">for</span> match_id, start, end <span class="hljs-keyword">in</span> matcher(doc):<br>    <span class="hljs-comment"># 获取匹配到的span</span><br>    span = doc[start:end]<br>    print(<span class="hljs-string">&quot;Matched span:&quot;</span>, span.text)<br><br><span class="hljs-comment"># 多个pattern如何进行add</span><br><span class="hljs-comment"># 下面的代码比这样的表达方式更快： [nlp(country) for country in COUNTRIES]</span><br>patterns = list(nlp.pipe(COUNTRIES))<br>matcher.add(<span class="hljs-string">&quot;COUNTRY&quot;</span>, <span class="hljs-literal">None</span>, *patterns)<br><br><span class="hljs-comment">### 例子： 找符合pattern的一个span 并且获取这个span中的root，随后找到这个root的head</span><br>span_root = span.root<br>span_root_head = span.root.head<br><span class="hljs-comment"># 打印这个span的根头词符的文本及span的文本</span><br>print(span_root_head.text, <span class="hljs-string">&quot;--&gt;&quot;</span>, span.text)<br></code></pre></td></tr></table></figure>

<h2 id="处理流程"><a href="#处理流程" class="headerlink" title="处理流程"></a>处理流程</h2><p><img src="/image/nlp_process.png"></p>
<p>词性标注: token.tag, token.pos<br>依存关系: token.dep, token.head<br>命名实体: 将检测到的实体添加到doc.ents<br>文本分类器: 适用于整个文本的类别，将其加入doc.cats</p>
<p>所有预训练的模型都包含了一些文件和一个meta.json。这个元数据定义了语种和流程等等，告诉spaCy应该去初始化那些组件。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python">print(nlp.pipe_names)  <span class="hljs-comment"># 流程组件名的列表</span><br>print(nlp.pipeline)    <span class="hljs-comment"># (name, component)元组的列表</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">[(&#x27;tagger&#x27;, &lt;spacy.pipeline.Tagger&gt;),</span><br><span class="hljs-string"> (&#x27;parser&#x27;, &lt;spacy.pipeline.DependencyParser&gt;),</span><br><span class="hljs-string"> (&#x27;ner&#x27;, &lt;spacy.pipeline.EntityRecognizer&gt;)]</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure>

<h2 id="定制化流程组件"><a href="#定制化流程组件" class="headerlink" title="定制化流程组件"></a>定制化流程组件</h2><p>当一个文本已经被分词且Doc实例被创建后，流程组件会依次被调用。 spaCy支持一系列的原生组件，但是也可以在其中加入自己设计的组件，通过自己定制可以给doc和token添加一些属性。</p>
<blockquote>
<p>一个流程组件就是一个函数或者callable，它读取一个doc，修改和返回这个doc，作为下一个流程组件的输入。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">custom_component</span>(<span class="hljs-params">doc</span>):</span><br>    <span class="hljs-comment"># 对doc做一些处理</span><br>    <span class="hljs-keyword">return</span> doc<br><br>nlp.add_pipe(custom_component)<br><br>nlp.add_pipe(component, last=<span class="hljs-literal">True</span>) <span class="hljs-comment"># 组件顺序最后 first=True 组件顺序最前</span><br>nlp.add_pipe(component, before=<span class="hljs-string">&quot;ner&quot;</span>) <span class="hljs-comment"># 指定组件顺序之前 after=&quot;tagger&quot; 指定组件顺序之后</span><br></code></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>Python</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch-BigGraph 损失计算</title>
    <url>/2020/10/30/PBG-Loss-calculation/</url>
    <content><![CDATA[<h1 id="PyTorch-BigGraph-损失计算"><a href="#PyTorch-BigGraph-损失计算" class="headerlink" title="PyTorch-BigGraph 损失计算"></a>PyTorch-BigGraph 损失计算</h1><p>The training process aims at finding the embeddings for the entities so that the scores of the positive edges are higher than the scores of the negative edges. When unpacking what this means, three different aspects come into play:<br>训练过程旨在找到实体的嵌入，使得正边实例的得分高于负边实例的得分。 具体的说，分为以下三个部分：</p>
<p>• One must first determine which edges are to be considered as positive and negative samples.<br>• Then, once the scores of all the samples have been determined, one must decide how to aggregate them in a single loss value.<br>• Finally, one must decide how to go about optimizing that loss.</p>
<p>• 首先必须确定哪些边被视为正样本和负样本。<br>• 然后，一旦确定了所有样本的得分，就必须决定如何将它们汇总为一个损失值。<br>• 最后，必须决定如何优化这一损失。</p>
<p>This chapter will dive into each of these issues.<br>本章将深入探讨这些问题。</p>
<h2 id="Negative-sampling"><a href="#Negative-sampling" class="headerlink" title="Negative sampling"></a>Negative sampling</h2><p>The edges provided in the input data are known to be positives but, as PBG operates under the open-world assumption, the edges that are not in the input are not necessarily negatives. However, as PBG is designed to perform on large sparse graphs, it relies on the approximation that any random edge is a negative with very high probability.<br>已知输入数据中提供的边为正实例，但是，由于PBG是基于开放世界假设的，因此不在输入中的边不一定为负实例。但是，由于PBG设计是在大型稀疏图运行的，因此它依赖于近似值，即任何随机边为负实例的可能性很高。</p>
<p>The goal of sampling negatives is to produce a set of negative edges for each positive edge of a batch. Usual downstream applications (ranking, classification, …) are interested in comparing the score of an edge (𝑥,𝑟,𝑦1) with the score of an edge (𝑥,𝑟,𝑦2). Therefore, PBG produces negative samples for a given positive edge by corrupting the entity on one of its sides, keeping the other side and the relation type intact. This makes the sampling more suited to the task.<br>负采样的目的是在一个批次上的为每条正边实例子产生一组负边实例。通常的之后的操作（排名，分类等）都希望将边（𝑥，𝑟，𝑦1）的得分与边（𝑥，𝑟，𝑦2）的得分进行比较。因此，PBG通过破坏其一侧的实体，保持另一侧和关联类型的完整性来生成给定正边实例的负样本。这使得采样更加适合任务。</p>
<p>For performance reasons, the set of entities used to corrupt the positive edges in order to produce the negative samples may be shared across several positive edges. The way this usually happens is that positive edges are split into “chunks”, a single set of entities is sampled for each chunk, and all edges in that chunk are corrupted using that set of entities.<br>出于性能原因，可用于破坏正边以生成负样本的实体集可在多个正边之间共享。这种情况通常发生的方式是，将正边分成“块”，为每个块采样一组实体，然后使用这样一组实体来作用到这个块中的所有边来完成负采样。</p>
<p>PBG supports several ways of sampling negatives:<br>PBG支持多种负采样：</p>
<h3 id="All-negatives"><a href="#All-negatives" class="headerlink" title="All negatives"></a>All negatives</h3><p>The most straightforward way is to use, for each positive edge, all its possible negatives. What this means is that for a positive (𝑥,𝑟,𝑦) (where 𝑥 and 𝑦 are the left- and right-hand side negatives respectively and 𝑟 is the relation type), its negatives will be (𝑥′,𝑟,𝑦) for all 𝑥′ of the same entity type as 𝑥 and (𝑥,𝑟,𝑦′) for 𝑦′ of the same entity type as 𝑦. (Due to technical reasons this is in fact restricted to only the 𝑥′ in the same partition as 𝑥, and similarly for 𝑦′, as negative sampling always operates within the current bucket.)<br>最直接的方式是对每个正边得到所有可能的负边。  这意味着对于一个正边（𝑥，𝑟，𝑦），它的负边为（𝑥’，𝑟，𝑦 ）表示与𝑥具有相同实体类型的所有𝑥’以及（𝑥，𝑟，𝑦’）表示与𝑦具有相同实体类型的所有𝑦’。（出于技术原因，一般就一种，要不就x’,要不就y‘）</p>
<p>As one can imagine, this method generates a lot of negatives and thus doesn’t scale to graphs of any significant size. It should not be used in practice, and is provided in PBG mainly for “academic” reasons. It is mainly useful to get more accurate results during evaluation on small graphs.<br>可以设想，这种方法会产生很多负样本，因此无法适用于任何大小的图形。因此不应在实践中使用它，而在PBG中提供它主要是出于“学术”原因。 起作用主要是在小图上进行评估时获得更准确的结果。</p>
<p>This method is activated on a per-relation basis, by turning on the all_negs config flag. When it’s enabled, this mode takes precedence and overrides any other mode.<br>通过打开all_negs config标志，将基于每个关系激活此方法。 启用后，此模式将优先并覆盖其他任何模式。</p>
<h3 id="Same-batch-negatives"><a href="#Same-batch-negatives" class="headerlink" title="Same-batch negatives"></a>Same-batch negatives</h3><p>This negative sampling method produces negatives for a given positive edge of a batch by sampling from the other edges of the same batch. This is done by first splitting the batch into so-called chunks (beware that the name “chunks” is overloaded, and these chunks are different than the edge chunks explained in Batch preparation). Then the set of negatives for a positive edge (𝑥,𝑟,𝑦) contains the edges (𝑥′,𝑟,𝑦) for all entities 𝑥′ that are on left-hand side of another edge in the chunk, and the edges (𝑥,𝑟,𝑦′) with 𝑦′ satisfying the same condition for the right-hand side.<br>通过从同一批次的其他边进行采样，此负采样方法可为批次的给定正边生成负样本。这是通过首先将批处理拆分为所谓的块来完成的（请注意，“块”的名称已超载（有多重意思），并且这些块与“批处理”中介绍的边缘块不同），然后，一个正边（𝑥，𝑟，𝑦）得到的负样本为，这个块中的符合条件的(𝑥′,𝑟,𝑦)以及(𝑥,𝑟,𝑦′)。</p>
<p>For a single positive edge, this means that the entities used to construct its negatives are sampled from the current partition proportionally to their degree, a.k.a., according to the data distribution. This helps counteract the effects of a very skewed distribution of degrees, which might cause the embeddings to just capture that distribution.<br>对于一个正边来说，根据其生成的负边的实体来源于当前的分区，并根据数据的分布来按比例生成的。这有助于抵消度的非常偏斜的分布的影响，可以实现嵌入恰好捕获该分布。</p>
<p>The size of the chunks is controlled by the global num_batch_negs parameter. To disable this sampling mode, set that parameter to zero.<br>块的大小由全局num_batch_negs参数控制。 要禁用此采样模式，请将该参数设置为零。</p>
<h3 id="Uniformly-sampled-negatives"><a href="#Uniformly-sampled-negatives" class="headerlink" title="Uniformly-sampled negatives"></a>Uniformly-sampled negatives</h3><p>This last method is perhaps the most natural approximation of the “all negatives” method that scales to arbitrarily large graphs. Instead of using all the entities on either side to produce negative edges (thus having the number of negatives scale linearly withe the size of the graph), a fixed given number of these entities is sampled uniformly with replacement. Thus the set of negatives remains of constant size no matter how large the graph is. As with the “all negatives” method, the sampling here is restricted to the entities that have the same type and that belong to the same partition as the entity of the positive edge.<br>最后一种方法与“全负样本”的方法类似，但是该方法可适用于到任意大图。该方法对固定数量的这些实体进行均匀采样并替换而不是像“全负样本”一样完全生成负样本（负边的数量与图的大小成线性比例）。因此无论图形有多大，负样本数目保持恒定大小。 与“全负样本”方法一样，此处的采样仅限于具有相同类型且与正边实体属于同一分区的实体。</p>
<p>This method interacts with the same-batch method, as all the edges in a chunk receive the same set of uniformly sampled negatives. This caveat means that the uniform negatives of two different positives are independent and uncorrelated only if they belong to different chunks.<br>该方法与same-batch方法相互影响，因为块中的所有边都接收同一组均匀采样的负样本。这表明了两个正边生成的负样本只有在其属于不同组块时才是独立且不相关的。</p>
<p>This method is controlled by the num_uniform_negs parameter, which controls how many negatives are sampled for each chunk. If num_batch_negs is zero, the batches will be split into chunks of size num_uniform_negs.<br>此方法由num_uniform_negs参数控制，该参数控制为每个块采样多少个负边。 如果num_batch_negs为零，则将批次拆分为num_uniform_negs大小的块。</p>
<blockquote>
<p>🤔 需要再琢磨琢磨</p>
</blockquote>
<h2 id="Loss-functions"><a href="#Loss-functions" class="headerlink" title="Loss functions"></a>Loss functions</h2><p>Once positive and negative samples have been determined and their scores have been computed by the model, the scores’ suitability for a certain application must be assessed, which is done by aggregating them into a single real value, the loss. What loss function is most appropriate depends on what operations the embeddings will be used for.<br>一旦确定了正负样本并通过模型计算了它们的得分，就必须评估得分对特定应用程序的适用性。一般来说，将这些得分汇总为一个实际值即损失。 哪种损失函数最合适取决于嵌入之后的的应用场景。</p>
<p>In all cases, the loss function’s input will be a series of scores for positive samples and, for each of them, a set of scores for corresponding negative samples. For simplicity, suppose all these sets are of the same size (if they are not, they can be padded with “negative infinity” values, as these are the “ideal” scores for negative edges and should thus induce no loss).<br>在所有情况下，损失函数的输入将是一系列正样本的得分，对于每个正样本，会有一组是其对应负样本的得分。 为简单起见，假设所有这些集合的大小都相同（如果不是，则可以用“负无穷大”值填充，因为这些是负边的“理想”分数，因此不应引起任何损失）。</p>
<h3 id="Ranking-loss"><a href="#Ranking-loss" class="headerlink" title="Ranking loss"></a>Ranking loss</h3><p>The ranking loss compares each positive score with each of its corresponding negatives. For each such pair, no loss is introduced if the positive score is greater than the negative one by at least a given margin. Otherwise the incurred loss is the amount by which that inequality is violated. This is the hinge loss on the difference between the positive and negative score. Formally, for a margin 𝑚, a positive score 𝑠𝑖 and a negative score 𝑡𝑖,𝑗, the loss is max(0,𝑚−𝑠𝑖+𝑡𝑖,𝑗). The total loss is the sum of the losses over all pairs of positive and negative scores, i.e., over all 𝑖 and 𝑗.<br>排名损失将每个正得分与其对应的负得分进行比较。对于每一个这样的配对，如果正值至少比负值大一个给定的余量，则不会造成任何损失，否则，其贡献的损失为超过的值。这其实是一种正负得分差的hinge loss。形式上，对于余量𝑚，正分数𝑠𝑖和负分数𝑡𝑖𝑗，损失为max（0，𝑚-𝑠𝑖+𝑡𝑖𝑗）。总损失是所有正负分数对（即所有𝑖和𝑗）的损失总和。</p>
<p>This loss function is chosen by setting the loss_fn parameter to ranking, and the target margin is specified by the margin parameter.<br>通过将loss_fn参数设置为等级来选择此损失函数，并且margin参数指定余量的大小。</p>
<p>This loss function is suitable when the setting requires to rank some entities by how likely they are to be related to another given entity.<br>当设置需要根据某些实体与另一个给定实体相关的可能性来对某些实体进行排名时，此损失函数适用。</p>
<h3 id="Logistic-loss"><a href="#Logistic-loss" class="headerlink" title="Logistic loss"></a>Logistic loss</h3><p>The logistic loss instead interprets the scores as the probabilities that the edges exist. It does so by first passing each score (whose domain is the entire real line) through the logistic function (𝑥↦1/(1+𝑒−𝑥), which maps it to a value between 0 and 1). This value is taken as the probability 𝑝 and the loss will be its binary cross entropy with the “target” probability, i.e., 1 for positive edges and 0 for negative ones. In formulas, the loss for positives is −log𝑝 whereas for negatives it’s −log(1−𝑝). The total loss of due to the negatives is renormalized so it compares with the one of the positives.<br>逻辑损失将分数解释为边存在的概率。首先要通过逻辑函数（𝑥↦1/（1 + 𝑒−𝑥），将其映射到0到1之间的值）传递每个得分（其域是整个实线）。 将该值作为概率𝑝，并且损失将是具有“目标”概率的二进制交叉熵。正边为1，负边为0。 在公式中，正值的损失为-log𝑝，负数的损失为-log（1-𝑝)。 由于负数导致的总损失被重新归一化，因此可以与正数之一进行比较。</p>
<p>One can see this as the cross entropy between two distributions on the values “edge exists” and “edge doesn’t exist”. One is given by the score (passed through the logistic function), the other has all the mass on “exists” for positives or all the mass on “doesn’t exist” for negatives.<br>通过使用交叉熵，可以得到边是否存在的分布。一个由得分（通过逻辑函数传递）给出，另一个由阳性存在的“存在”代表所有否定，或由负存在所有质量的“不存在”代表。</p>
<p>This loss function is parameterless and is enabled by setting loss_fn to logistic.<br>此损失函数是无参的，可以通过将loss_fn设置为logistic来启用。</p>
<h3 id="Softmax-loss"><a href="#Softmax-loss" class="headerlink" title="Softmax loss"></a>Softmax loss</h3><p>The last loss function is designed for when one wants a distribution on the probabilities of some entities being related to a given entity (contrary to just wanting a ranking, as with the ranking loss). For a certain positive 𝑖, its score 𝑠𝑖 and the score 𝑡𝑖,𝑗 of all the corresponding negatives 𝑗 are first converted to probabilities by performing a softmax: 𝑝𝑖∝𝑒𝑠𝑖 and 𝑞𝑖,𝑗∝𝑒𝑡𝑖,𝑗, normalized so that they sum up to 1. Then the loss is the cross entropy between this distribution and the “target” one, i.e., the one that puts all the mass on the positive sample. So, in full, the loss for a single 𝑖 is −log𝑝𝑖, i.e., −𝑠𝑖+log∑𝑗𝑒𝑡𝑖,𝑗.<br>最后一个损失函数设计用于当人们希望通过给定一个实体，得到其他实体和其相关性的概率的场景。（与仅需要排名，与排名损失相反）。对于某个正例𝑖，首先通过执行softmax：𝑝𝑖∝𝑒𝑠𝑖和𝑞𝑖，𝑗∝𝑒𝑡𝑖，𝑗进行归一化，使得概率和为1。此时损失就是这种分布与“目标”分布之间的交叉熵。即将所有质量置于正样本上的交叉熵。 因此，总的来说，单个𝑖的损失为-log𝑝𝑖，即-𝑠𝑖+ log∑𝑗𝑒𝑡𝑖，𝑗。</p>
<p>This loss is activated by setting loss_fn to softmax.<br>通过在配置参数中设置loss_fn=softmax来实现配置。</p>
<h2 id="Optimizers"><a href="#Optimizers" class="headerlink" title="Optimizers"></a>Optimizers</h2><p>The Adagrad optimization method is used to update all model parameters. Adagrad performs stochastic gradient descent with an adaptive learning rate applied to each parameter inversely proportional to the inverse square magnitude of all previous updates. In practice, Adagrad updates lead to an order of magnitude faster convergence for typical PBG models.</p>
<p>Adagrad优化方法用于更新所有模型参数。Adagrad使用与所有先前更新的平方反比成反比的自适应学习率，以自适应学习率执行随机梯度下降。 实际上，Adagrad更新使典型PBG模型的收敛速度提高了一个数量级。</p>
<p>The initial learning rate for Adagrad is specified by the lr config parameter.A separate learning rate can also be set for non-embeddings using the relation_lr parameter.<br>Adagrad的初始学习率由config文件里的lr参数指定。还可以使用related_lr参数为非嵌入设置单独的学习率。</p>
<p>Standard Adagrad requires an equal amount of memory for optimizer state as the size of the model, which is prohibitive for the large models targeted by PBG. To reduce optimizer memory usage, a modified version of Adagrad is used that uses a common learning rate for each entity embedding. The learning rate is proportional to the inverse sum of the squared gradients from each element of the embedding, divided by the dimension. Non-embedding parameters (e.g. relation operator parameters) use standard Adagrad.<br>标准Adagrad要求优化程序状态的内存量与模型大小相同，这对于PBG定位的大型模型是不允许的。为了减少优化器的内存使用，使用了Adagrad的修改版本，该版本对每个实体嵌入使用通用的学习率。学习率与嵌入元素的梯度平方的倒数之和成比例，再除以尺寸。 非嵌入参数（例如关系运算符参数）使用标准Adagrad。</p>
<p>Adagrad parameters are updated asynchronously across worker threads with no explicit synchronization. Asynchronous updates to the Adagrad state (the total squared gradient) appear stable, likely because each element of the state tensor only accumulates positives updates. Optimization is further stabilized by performing a short period of training with a single thread before beginning Hogwild! training, which is tuned by the hogwild_delay parameter.<br>Adagrad参数在工作线程之间异步更新，而没有显式同步。异步更新使得Adagrad状态(总平方梯度)的更新显得稳定,这可能是因为状态张量的每个元素仅累积正更新。在开始Hogwild之前，可以通过单线程进行短期培训来进一步优化。 培训，具体取决于hogwild_delay参数。</p>
<p>In distributed training, the Adagrad state for shared parameters (e.g. relation operator parameters) are shared via the parameter server using the same asynchronous gradient update as the parameters themselves. Similar to inter-thread synchronization, these asynchronous updates are stable after an initial burn-in period because the total squared gradient strictly accumulates positive values.<br>分布式训练中，共享参数（例如关系运算符参数）的Adagrad状态通过参数服务器使用与参数本身相同的异步梯度更新进行共享。与线程间同步类似，这些异步更新在初始阶段会不稳定，之后会保持稳定，因为总平方梯度严格累加正值。</p>
<blockquote>
<p>🤔 </p>
</blockquote>
<p>源地址：<a href="https://torchbiggraph.readthedocs.io/en/latest/loss_optimization.html">https://torchbiggraph.readthedocs.io/en/latest/loss_optimization.html</a></p>
]]></content>
      <tags>
        <tag>PyTorch-BigGraph</tag>
      </tags>
  </entry>
</search>
