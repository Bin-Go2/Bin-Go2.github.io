<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.1.1">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.ico">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="PyTorch-BigGraph æŸå¤±è®¡ç®—The training process aims at finding the embeddings for the entities so that the scores of the positive edges are higher than the scores of the negative edges. When unpacking what">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch-BigGraph æŸå¤±è®¡ç®—">
<meta property="og:url" content="http://example.com/2020/10/30/PBG-Loss-calculation/index.html">
<meta property="og:site_name" content="Bin&#39;s blog">
<meta property="og:description" content="PyTorch-BigGraph æŸå¤±è®¡ç®—The training process aims at finding the embeddings for the entities so that the scores of the positive edges are higher than the scores of the negative edges. When unpacking what">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-10-30T23:05:30.000Z">
<meta property="article:modified_time" content="2020-10-31T01:23:13.422Z">
<meta property="article:author" content="Bin">
<meta property="article:tag" content="PyTorch-BigGraph">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2020/10/30/PBG-Loss-calculation/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>PyTorch-BigGraph æŸå¤±è®¡ç®— | Bin's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Bin's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">ä»Šæ—¥æ‘¸é±¼è¿›åº¦ï¼š 1/ âˆ</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/30/PBG-Loss-calculation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Bin">
      <meta itemprop="description" content="æˆ‘ç†Šäººæ—æ°¸ä¸ä¸ºå¥´ï¼ï¼å¼å¼å¼ï¼ï¼ï¼">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bin's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          PyTorch-BigGraph æŸå¤±è®¡ç®—
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-10-30 16:05:30 / Modified: 18:23:13" itemprop="dateCreated datePublished" datetime="2020-10-30T16:05:30-07:00">2020-10-30</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2020/10/30/PBG-Loss-calculation/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/10/30/PBG-Loss-calculation/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="PyTorch-BigGraph-æŸå¤±è®¡ç®—"><a href="#PyTorch-BigGraph-æŸå¤±è®¡ç®—" class="headerlink" title="PyTorch-BigGraph æŸå¤±è®¡ç®—"></a>PyTorch-BigGraph æŸå¤±è®¡ç®—</h1><p>The training process aims at finding the embeddings for the entities so that the scores of the positive edges are higher than the scores of the negative edges. When unpacking what this means, three different aspects come into play:<br>è®­ç»ƒè¿‡ç¨‹æ—¨åœ¨æ‰¾åˆ°å®ä½“çš„åµŒå…¥ï¼Œä½¿å¾—æ­£è¾¹å®ä¾‹çš„å¾—åˆ†é«˜äºè´Ÿè¾¹å®ä¾‹çš„å¾—åˆ†ã€‚ å…·ä½“çš„è¯´ï¼Œåˆ†ä¸ºä»¥ä¸‹ä¸‰ä¸ªéƒ¨åˆ†ï¼š</p>
<p>â€¢ One must first determine which edges are to be considered as positive and negative samples.<br>â€¢ Then, once the scores of all the samples have been determined, one must decide how to aggregate them in a single loss value.<br>â€¢ Finally, one must decide how to go about optimizing that loss.</p>
<p>â€¢ é¦–å…ˆå¿…é¡»ç¡®å®šå“ªäº›è¾¹è¢«è§†ä¸ºæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ã€‚<br>â€¢ ç„¶åï¼Œä¸€æ—¦ç¡®å®šäº†æ‰€æœ‰æ ·æœ¬çš„å¾—åˆ†ï¼Œå°±å¿…é¡»å†³å®šå¦‚ä½•å°†å®ƒä»¬æ±‡æ€»ä¸ºä¸€ä¸ªæŸå¤±å€¼ã€‚<br>â€¢ æœ€åï¼Œå¿…é¡»å†³å®šå¦‚ä½•ä¼˜åŒ–è¿™ä¸€æŸå¤±ã€‚</p>
<p>This chapter will dive into each of these issues.<br>æœ¬ç« å°†æ·±å…¥æ¢è®¨è¿™äº›é—®é¢˜ã€‚</p>
<h2 id="Negative-sampling"><a href="#Negative-sampling" class="headerlink" title="Negative sampling"></a>Negative sampling</h2><p>The edges provided in the input data are known to be positives but, as PBG operates under the open-world assumption, the edges that are not in the input are not necessarily negatives. However, as PBG is designed to perform on large sparse graphs, it relies on the approximation that any random edge is a negative with very high probability.<br>å·²çŸ¥è¾“å…¥æ•°æ®ä¸­æä¾›çš„è¾¹ä¸ºæ­£å®ä¾‹ï¼Œä½†æ˜¯ï¼Œç”±äºPBGæ˜¯åŸºäºå¼€æ”¾ä¸–ç•Œå‡è®¾çš„ï¼Œå› æ­¤ä¸åœ¨è¾“å…¥ä¸­çš„è¾¹ä¸ä¸€å®šä¸ºè´Ÿå®ä¾‹ã€‚ä½†æ˜¯ï¼Œç”±äºPBGè®¾è®¡æ˜¯åœ¨å¤§å‹ç¨€ç–å›¾è¿è¡Œçš„ï¼Œå› æ­¤å®ƒä¾èµ–äºè¿‘ä¼¼å€¼ï¼Œå³ä»»ä½•éšæœºè¾¹ä¸ºè´Ÿå®ä¾‹çš„å¯èƒ½æ€§å¾ˆé«˜ã€‚</p>
<p>The goal of sampling negatives is to produce a set of negative edges for each positive edge of a batch. Usual downstream applications (ranking, classification, â€¦) are interested in comparing the score of an edge (ğ‘¥,ğ‘Ÿ,ğ‘¦1) with the score of an edge (ğ‘¥,ğ‘Ÿ,ğ‘¦2). Therefore, PBG produces negative samples for a given positive edge by corrupting the entity on one of its sides, keeping the other side and the relation type intact. This makes the sampling more suited to the task.<br>è´Ÿé‡‡æ ·çš„ç›®çš„æ˜¯åœ¨ä¸€ä¸ªæ‰¹æ¬¡ä¸Šçš„ä¸ºæ¯æ¡æ­£è¾¹å®ä¾‹å­äº§ç”Ÿä¸€ç»„è´Ÿè¾¹å®ä¾‹ã€‚é€šå¸¸çš„ä¹‹åçš„æ“ä½œï¼ˆæ’åï¼Œåˆ†ç±»ç­‰ï¼‰éƒ½å¸Œæœ›å°†è¾¹ï¼ˆğ‘¥ï¼Œğ‘Ÿï¼Œğ‘¦1ï¼‰çš„å¾—åˆ†ä¸è¾¹ï¼ˆğ‘¥ï¼Œğ‘Ÿï¼Œğ‘¦2ï¼‰çš„å¾—åˆ†è¿›è¡Œæ¯”è¾ƒã€‚å› æ­¤ï¼ŒPBGé€šè¿‡ç ´åå…¶ä¸€ä¾§çš„å®ä½“ï¼Œä¿æŒå¦ä¸€ä¾§å’Œå…³è”ç±»å‹çš„å®Œæ•´æ€§æ¥ç”Ÿæˆç»™å®šæ­£è¾¹å®ä¾‹çš„è´Ÿæ ·æœ¬ã€‚è¿™ä½¿å¾—é‡‡æ ·æ›´åŠ é€‚åˆä»»åŠ¡ã€‚</p>
<p>For performance reasons, the set of entities used to corrupt the positive edges in order to produce the negative samples may be shared across several positive edges. The way this usually happens is that positive edges are split into â€œchunksâ€, a single set of entities is sampled for each chunk, and all edges in that chunk are corrupted using that set of entities.<br>å‡ºäºæ€§èƒ½åŸå› ï¼Œå¯ç”¨äºç ´åæ­£è¾¹ä»¥ç”Ÿæˆè´Ÿæ ·æœ¬çš„å®ä½“é›†å¯åœ¨å¤šä¸ªæ­£è¾¹ä¹‹é—´å…±äº«ã€‚è¿™ç§æƒ…å†µé€šå¸¸å‘ç”Ÿçš„æ–¹å¼æ˜¯ï¼Œå°†æ­£è¾¹åˆ†æˆâ€œå—â€ï¼Œä¸ºæ¯ä¸ªå—é‡‡æ ·ä¸€ç»„å®ä½“ï¼Œç„¶åä½¿ç”¨è¿™æ ·ä¸€ç»„å®ä½“æ¥ä½œç”¨åˆ°è¿™ä¸ªå—ä¸­çš„æ‰€æœ‰è¾¹æ¥å®Œæˆè´Ÿé‡‡æ ·ã€‚</p>
<p>PBG supports several ways of sampling negatives:<br>PBGæ”¯æŒå¤šç§è´Ÿé‡‡æ ·ï¼š</p>
<h3 id="All-negatives"><a href="#All-negatives" class="headerlink" title="All negatives"></a>All negatives</h3><p>The most straightforward way is to use, for each positive edge, all its possible negatives. What this means is that for a positive (ğ‘¥,ğ‘Ÿ,ğ‘¦) (where ğ‘¥ and ğ‘¦ are the left- and right-hand side negatives respectively and ğ‘Ÿ is the relation type), its negatives will be (ğ‘¥â€²,ğ‘Ÿ,ğ‘¦) for all ğ‘¥â€² of the same entity type as ğ‘¥ and (ğ‘¥,ğ‘Ÿ,ğ‘¦â€²) for ğ‘¦â€² of the same entity type as ğ‘¦. (Due to technical reasons this is in fact restricted to only the ğ‘¥â€² in the same partition as ğ‘¥, and similarly for ğ‘¦â€², as negative sampling always operates within the current bucket.)<br>æœ€ç›´æ¥çš„æ–¹å¼æ˜¯å¯¹æ¯ä¸ªæ­£è¾¹å¾—åˆ°æ‰€æœ‰å¯èƒ½çš„è´Ÿè¾¹ã€‚  è¿™æ„å‘³ç€å¯¹äºä¸€ä¸ªæ­£è¾¹ï¼ˆğ‘¥ï¼Œğ‘Ÿï¼Œğ‘¦ï¼‰ï¼Œå®ƒçš„è´Ÿè¾¹ä¸ºï¼ˆğ‘¥â€™ï¼Œğ‘Ÿï¼Œğ‘¦ ï¼‰è¡¨ç¤ºä¸ğ‘¥å…·æœ‰ç›¸åŒå®ä½“ç±»å‹çš„æ‰€æœ‰ğ‘¥â€™ä»¥åŠï¼ˆğ‘¥ï¼Œğ‘Ÿï¼Œğ‘¦â€™ï¼‰è¡¨ç¤ºä¸ğ‘¦å…·æœ‰ç›¸åŒå®ä½“ç±»å‹çš„æ‰€æœ‰ğ‘¦â€™ã€‚ï¼ˆå‡ºäºæŠ€æœ¯åŸå› ï¼Œä¸€èˆ¬å°±ä¸€ç§ï¼Œè¦ä¸å°±xâ€™,è¦ä¸å°±yâ€˜ï¼‰</p>
<p>As one can imagine, this method generates a lot of negatives and thus doesnâ€™t scale to graphs of any significant size. It should not be used in practice, and is provided in PBG mainly for â€œacademicâ€ reasons. It is mainly useful to get more accurate results during evaluation on small graphs.<br>å¯ä»¥è®¾æƒ³ï¼Œè¿™ç§æ–¹æ³•ä¼šäº§ç”Ÿå¾ˆå¤šè´Ÿæ ·æœ¬ï¼Œå› æ­¤æ— æ³•é€‚ç”¨äºä»»ä½•å¤§å°çš„å›¾å½¢ã€‚å› æ­¤ä¸åº”åœ¨å®è·µä¸­ä½¿ç”¨å®ƒï¼Œè€Œåœ¨PBGä¸­æä¾›å®ƒä¸»è¦æ˜¯å‡ºäºâ€œå­¦æœ¯â€åŸå› ã€‚ èµ·ä½œç”¨ä¸»è¦æ˜¯åœ¨å°å›¾ä¸Šè¿›è¡Œè¯„ä¼°æ—¶è·å¾—æ›´å‡†ç¡®çš„ç»“æœã€‚</p>
<p>This method is activated on a per-relation basis, by turning on the all_negs config flag. When itâ€™s enabled, this mode takes precedence and overrides any other mode.<br>é€šè¿‡æ‰“å¼€all_negs configæ ‡å¿—ï¼Œå°†åŸºäºæ¯ä¸ªå…³ç³»æ¿€æ´»æ­¤æ–¹æ³•ã€‚ å¯ç”¨åï¼Œæ­¤æ¨¡å¼å°†ä¼˜å…ˆå¹¶è¦†ç›–å…¶ä»–ä»»ä½•æ¨¡å¼ã€‚</p>
<h3 id="Same-batch-negatives"><a href="#Same-batch-negatives" class="headerlink" title="Same-batch negatives"></a>Same-batch negatives</h3><p>This negative sampling method produces negatives for a given positive edge of a batch by sampling from the other edges of the same batch. This is done by first splitting the batch into so-called chunks (beware that the name â€œchunksâ€ is overloaded, and these chunks are different than the edge chunks explained in Batch preparation). Then the set of negatives for a positive edge (ğ‘¥,ğ‘Ÿ,ğ‘¦) contains the edges (ğ‘¥â€²,ğ‘Ÿ,ğ‘¦) for all entities ğ‘¥â€² that are on left-hand side of another edge in the chunk, and the edges (ğ‘¥,ğ‘Ÿ,ğ‘¦â€²) with ğ‘¦â€² satisfying the same condition for the right-hand side.<br>é€šè¿‡ä»åŒä¸€æ‰¹æ¬¡çš„å…¶ä»–è¾¹è¿›è¡Œé‡‡æ ·ï¼Œæ­¤è´Ÿé‡‡æ ·æ–¹æ³•å¯ä¸ºæ‰¹æ¬¡çš„ç»™å®šæ­£è¾¹ç”Ÿæˆè´Ÿæ ·æœ¬ã€‚è¿™æ˜¯é€šè¿‡é¦–å…ˆå°†æ‰¹å¤„ç†æ‹†åˆ†ä¸ºæ‰€è°“çš„å—æ¥å®Œæˆçš„ï¼ˆè¯·æ³¨æ„ï¼Œâ€œå—â€çš„åç§°å·²è¶…è½½ï¼ˆæœ‰å¤šé‡æ„æ€ï¼‰ï¼Œå¹¶ä¸”è¿™äº›å—ä¸â€œæ‰¹å¤„ç†â€ä¸­ä»‹ç»çš„è¾¹ç¼˜å—ä¸åŒï¼‰ï¼Œç„¶åï¼Œä¸€ä¸ªæ­£è¾¹ï¼ˆğ‘¥ï¼Œğ‘Ÿï¼Œğ‘¦ï¼‰å¾—åˆ°çš„è´Ÿæ ·æœ¬ä¸ºï¼Œè¿™ä¸ªå—ä¸­çš„ç¬¦åˆæ¡ä»¶çš„(ğ‘¥â€²,ğ‘Ÿ,ğ‘¦)ä»¥åŠ(ğ‘¥,ğ‘Ÿ,ğ‘¦â€²)ã€‚</p>
<p>For a single positive edge, this means that the entities used to construct its negatives are sampled from the current partition proportionally to their degree, a.k.a., according to the data distribution. This helps counteract the effects of a very skewed distribution of degrees, which might cause the embeddings to just capture that distribution.<br>å¯¹äºä¸€ä¸ªæ­£è¾¹æ¥è¯´ï¼Œæ ¹æ®å…¶ç”Ÿæˆçš„è´Ÿè¾¹çš„å®ä½“æ¥æºäºå½“å‰çš„åˆ†åŒºï¼Œå¹¶æ ¹æ®æ•°æ®çš„åˆ†å¸ƒæ¥æŒ‰æ¯”ä¾‹ç”Ÿæˆçš„ã€‚è¿™æœ‰åŠ©äºæŠµæ¶ˆåº¦çš„éå¸¸åæ–œçš„åˆ†å¸ƒçš„å½±å“ï¼Œå¯ä»¥å®ç°åµŒå…¥æ°å¥½æ•è·è¯¥åˆ†å¸ƒã€‚</p>
<p>The size of the chunks is controlled by the global num_batch_negs parameter. To disable this sampling mode, set that parameter to zero.<br>å—çš„å¤§å°ç”±å…¨å±€num_batch_negså‚æ•°æ§åˆ¶ã€‚ è¦ç¦ç”¨æ­¤é‡‡æ ·æ¨¡å¼ï¼Œè¯·å°†è¯¥å‚æ•°è®¾ç½®ä¸ºé›¶ã€‚</p>
<h3 id="Uniformly-sampled-negatives"><a href="#Uniformly-sampled-negatives" class="headerlink" title="Uniformly-sampled negatives"></a>Uniformly-sampled negatives</h3><p>This last method is perhaps the most natural approximation of the â€œall negativesâ€ method that scales to arbitrarily large graphs. Instead of using all the entities on either side to produce negative edges (thus having the number of negatives scale linearly withe the size of the graph), a fixed given number of these entities is sampled uniformly with replacement. Thus the set of negatives remains of constant size no matter how large the graph is. As with the â€œall negativesâ€ method, the sampling here is restricted to the entities that have the same type and that belong to the same partition as the entity of the positive edge.<br>æœ€åä¸€ç§æ–¹æ³•ä¸â€œå…¨è´Ÿæ ·æœ¬â€çš„æ–¹æ³•ç±»ä¼¼ï¼Œä½†æ˜¯è¯¥æ–¹æ³•å¯é€‚ç”¨äºåˆ°ä»»æ„å¤§å›¾ã€‚è¯¥æ–¹æ³•å¯¹å›ºå®šæ•°é‡çš„è¿™äº›å®ä½“è¿›è¡Œå‡åŒ€é‡‡æ ·å¹¶æ›¿æ¢è€Œä¸æ˜¯åƒâ€œå…¨è´Ÿæ ·æœ¬â€ä¸€æ ·å®Œå…¨ç”Ÿæˆè´Ÿæ ·æœ¬ï¼ˆè´Ÿè¾¹çš„æ•°é‡ä¸å›¾çš„å¤§å°æˆçº¿æ€§æ¯”ä¾‹ï¼‰ã€‚å› æ­¤æ— è®ºå›¾å½¢æœ‰å¤šå¤§ï¼Œè´Ÿæ ·æœ¬æ•°ç›®ä¿æŒæ’å®šå¤§å°ã€‚ ä¸â€œå…¨è´Ÿæ ·æœ¬â€æ–¹æ³•ä¸€æ ·ï¼Œæ­¤å¤„çš„é‡‡æ ·ä»…é™äºå…·æœ‰ç›¸åŒç±»å‹ä¸”ä¸æ­£è¾¹å®ä½“å±äºåŒä¸€åˆ†åŒºçš„å®ä½“ã€‚</p>
<p>This method interacts with the same-batch method, as all the edges in a chunk receive the same set of uniformly sampled negatives. This caveat means that the uniform negatives of two different positives are independent and uncorrelated only if they belong to different chunks.<br>è¯¥æ–¹æ³•ä¸same-batchæ–¹æ³•ç›¸äº’å½±å“ï¼Œå› ä¸ºå—ä¸­çš„æ‰€æœ‰è¾¹éƒ½æ¥æ”¶åŒä¸€ç»„å‡åŒ€é‡‡æ ·çš„è´Ÿæ ·æœ¬ã€‚è¿™è¡¨æ˜äº†ä¸¤ä¸ªæ­£è¾¹ç”Ÿæˆçš„è´Ÿæ ·æœ¬åªæœ‰åœ¨å…¶å±äºä¸åŒç»„å—æ—¶æ‰æ˜¯ç‹¬ç«‹ä¸”ä¸ç›¸å…³çš„ã€‚</p>
<p>This method is controlled by the num_uniform_negs parameter, which controls how many negatives are sampled for each chunk. If num_batch_negs is zero, the batches will be split into chunks of size num_uniform_negs.<br>æ­¤æ–¹æ³•ç”±num_uniform_negså‚æ•°æ§åˆ¶ï¼Œè¯¥å‚æ•°æ§åˆ¶ä¸ºæ¯ä¸ªå—é‡‡æ ·å¤šå°‘ä¸ªè´Ÿè¾¹ã€‚ å¦‚æœnum_batch_negsä¸ºé›¶ï¼Œåˆ™å°†æ‰¹æ¬¡æ‹†åˆ†ä¸ºnum_uniform_negså¤§å°çš„å—ã€‚</p>
<blockquote>
<p>ğŸ¤” éœ€è¦å†ç¢ç£¨ç¢ç£¨</p>
</blockquote>
<h2 id="Loss-functions"><a href="#Loss-functions" class="headerlink" title="Loss functions"></a>Loss functions</h2><p>Once positive and negative samples have been determined and their scores have been computed by the model, the scoresâ€™ suitability for a certain application must be assessed, which is done by aggregating them into a single real value, the loss. What loss function is most appropriate depends on what operations the embeddings will be used for.<br>ä¸€æ—¦ç¡®å®šäº†æ­£è´Ÿæ ·æœ¬å¹¶é€šè¿‡æ¨¡å‹è®¡ç®—äº†å®ƒä»¬çš„å¾—åˆ†ï¼Œå°±å¿…é¡»è¯„ä¼°å¾—åˆ†å¯¹ç‰¹å®šåº”ç”¨ç¨‹åºçš„é€‚ç”¨æ€§ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå°†è¿™äº›å¾—åˆ†æ±‡æ€»ä¸ºä¸€ä¸ªå®é™…å€¼å³æŸå¤±ã€‚ å“ªç§æŸå¤±å‡½æ•°æœ€åˆé€‚å–å†³äºåµŒå…¥ä¹‹åçš„çš„åº”ç”¨åœºæ™¯ã€‚</p>
<p>In all cases, the loss functionâ€™s input will be a series of scores for positive samples and, for each of them, a set of scores for corresponding negative samples. For simplicity, suppose all these sets are of the same size (if they are not, they can be padded with â€œnegative infinityâ€ values, as these are the â€œidealâ€ scores for negative edges and should thus induce no loss).<br>åœ¨æ‰€æœ‰æƒ…å†µä¸‹ï¼ŒæŸå¤±å‡½æ•°çš„è¾“å…¥å°†æ˜¯ä¸€ç³»åˆ—æ­£æ ·æœ¬çš„å¾—åˆ†ï¼Œå¯¹äºæ¯ä¸ªæ­£æ ·æœ¬ï¼Œä¼šæœ‰ä¸€ç»„æ˜¯å…¶å¯¹åº”è´Ÿæ ·æœ¬çš„å¾—åˆ†ã€‚ ä¸ºç®€å•èµ·è§ï¼Œå‡è®¾æ‰€æœ‰è¿™äº›é›†åˆçš„å¤§å°éƒ½ç›¸åŒï¼ˆå¦‚æœä¸æ˜¯ï¼Œåˆ™å¯ä»¥ç”¨â€œè´Ÿæ— ç©·å¤§â€å€¼å¡«å……ï¼Œå› ä¸ºè¿™äº›æ˜¯è´Ÿè¾¹çš„â€œç†æƒ³â€åˆ†æ•°ï¼Œå› æ­¤ä¸åº”å¼•èµ·ä»»ä½•æŸå¤±ï¼‰ã€‚</p>
<h3 id="Ranking-loss"><a href="#Ranking-loss" class="headerlink" title="Ranking loss"></a>Ranking loss</h3><p>The ranking loss compares each positive score with each of its corresponding negatives. For each such pair, no loss is introduced if the positive score is greater than the negative one by at least a given margin. Otherwise the incurred loss is the amount by which that inequality is violated. This is the hinge loss on the difference between the positive and negative score. Formally, for a margin ğ‘š, a positive score ğ‘ ğ‘– and a negative score ğ‘¡ğ‘–,ğ‘—, the loss is max(0,ğ‘šâˆ’ğ‘ ğ‘–+ğ‘¡ğ‘–,ğ‘—). The total loss is the sum of the losses over all pairs of positive and negative scores, i.e., over all ğ‘– and ğ‘—.<br>æ’åæŸå¤±å°†æ¯ä¸ªæ­£å¾—åˆ†ä¸å…¶å¯¹åº”çš„è´Ÿå¾—åˆ†è¿›è¡Œæ¯”è¾ƒã€‚å¯¹äºæ¯ä¸€ä¸ªè¿™æ ·çš„é…å¯¹ï¼Œå¦‚æœæ­£å€¼è‡³å°‘æ¯”è´Ÿå€¼å¤§ä¸€ä¸ªç»™å®šçš„ä½™é‡ï¼Œåˆ™ä¸ä¼šé€ æˆä»»ä½•æŸå¤±ï¼Œå¦åˆ™ï¼Œå…¶è´¡çŒ®çš„æŸå¤±ä¸ºè¶…è¿‡çš„å€¼ã€‚è¿™å…¶å®æ˜¯ä¸€ç§æ­£è´Ÿå¾—åˆ†å·®çš„hinge lossã€‚å½¢å¼ä¸Šï¼Œå¯¹äºä½™é‡ğ‘šï¼Œæ­£åˆ†æ•°ğ‘ ğ‘–å’Œè´Ÿåˆ†æ•°ğ‘¡ğ‘–ğ‘—ï¼ŒæŸå¤±ä¸ºmaxï¼ˆ0ï¼Œğ‘š-ğ‘ ğ‘–+ğ‘¡ğ‘–ğ‘—ï¼‰ã€‚æ€»æŸå¤±æ˜¯æ‰€æœ‰æ­£è´Ÿåˆ†æ•°å¯¹ï¼ˆå³æ‰€æœ‰ğ‘–å’Œğ‘—ï¼‰çš„æŸå¤±æ€»å’Œã€‚</p>
<p>This loss function is chosen by setting the loss_fn parameter to ranking, and the target margin is specified by the margin parameter.<br>é€šè¿‡å°†loss_fnå‚æ•°è®¾ç½®ä¸ºç­‰çº§æ¥é€‰æ‹©æ­¤æŸå¤±å‡½æ•°ï¼Œå¹¶ä¸”marginå‚æ•°æŒ‡å®šä½™é‡çš„å¤§å°ã€‚</p>
<p>This loss function is suitable when the setting requires to rank some entities by how likely they are to be related to another given entity.<br>å½“è®¾ç½®éœ€è¦æ ¹æ®æŸäº›å®ä½“ä¸å¦ä¸€ä¸ªç»™å®šå®ä½“ç›¸å…³çš„å¯èƒ½æ€§æ¥å¯¹æŸäº›å®ä½“è¿›è¡Œæ’åæ—¶ï¼Œæ­¤æŸå¤±å‡½æ•°é€‚ç”¨ã€‚</p>
<h3 id="Logistic-loss"><a href="#Logistic-loss" class="headerlink" title="Logistic loss"></a>Logistic loss</h3><p>The logistic loss instead interprets the scores as the probabilities that the edges exist. It does so by first passing each score (whose domain is the entire real line) through the logistic function (ğ‘¥â†¦1/(1+ğ‘’âˆ’ğ‘¥), which maps it to a value between 0 and 1). This value is taken as the probability ğ‘ and the loss will be its binary cross entropy with the â€œtargetâ€ probability, i.e., 1 for positive edges and 0 for negative ones. In formulas, the loss for positives is âˆ’logğ‘ whereas for negatives itâ€™s âˆ’log(1âˆ’ğ‘). The total loss of due to the negatives is renormalized so it compares with the one of the positives.<br>é€»è¾‘æŸå¤±å°†åˆ†æ•°è§£é‡Šä¸ºè¾¹å­˜åœ¨çš„æ¦‚ç‡ã€‚é¦–å…ˆè¦é€šè¿‡é€»è¾‘å‡½æ•°ï¼ˆğ‘¥â†¦1/ï¼ˆ1 + ğ‘’âˆ’ğ‘¥ï¼‰ï¼Œå°†å…¶æ˜ å°„åˆ°0åˆ°1ä¹‹é—´çš„å€¼ï¼‰ä¼ é€’æ¯ä¸ªå¾—åˆ†ï¼ˆå…¶åŸŸæ˜¯æ•´ä¸ªå®çº¿ï¼‰ã€‚ å°†è¯¥å€¼ä½œä¸ºæ¦‚ç‡ğ‘ï¼Œå¹¶ä¸”æŸå¤±å°†æ˜¯å…·æœ‰â€œç›®æ ‡â€æ¦‚ç‡çš„äºŒè¿›åˆ¶äº¤å‰ç†µã€‚æ­£è¾¹ä¸º1ï¼Œè´Ÿè¾¹ä¸º0ã€‚ åœ¨å…¬å¼ä¸­ï¼Œæ­£å€¼çš„æŸå¤±ä¸º-logğ‘ï¼Œè´Ÿæ•°çš„æŸå¤±ä¸º-logï¼ˆ1-ğ‘)ã€‚ ç”±äºè´Ÿæ•°å¯¼è‡´çš„æ€»æŸå¤±è¢«é‡æ–°å½’ä¸€åŒ–ï¼Œå› æ­¤å¯ä»¥ä¸æ­£æ•°ä¹‹ä¸€è¿›è¡Œæ¯”è¾ƒã€‚</p>
<p>One can see this as the cross entropy between two distributions on the values â€œedge existsâ€ and â€œedge doesnâ€™t existâ€. One is given by the score (passed through the logistic function), the other has all the mass on â€œexistsâ€ for positives or all the mass on â€œdoesnâ€™t existâ€ for negatives.<br>é€šè¿‡ä½¿ç”¨äº¤å‰ç†µï¼Œå¯ä»¥å¾—åˆ°è¾¹æ˜¯å¦å­˜åœ¨çš„åˆ†å¸ƒã€‚ä¸€ä¸ªç”±å¾—åˆ†ï¼ˆé€šè¿‡é€»è¾‘å‡½æ•°ä¼ é€’ï¼‰ç»™å‡ºï¼Œå¦ä¸€ä¸ªç”±é˜³æ€§å­˜åœ¨çš„â€œå­˜åœ¨â€ä»£è¡¨æ‰€æœ‰å¦å®šï¼Œæˆ–ç”±è´Ÿå­˜åœ¨æ‰€æœ‰è´¨é‡çš„â€œä¸å­˜åœ¨â€ä»£è¡¨ã€‚</p>
<p>This loss function is parameterless and is enabled by setting loss_fn to logistic.<br>æ­¤æŸå¤±å‡½æ•°æ˜¯æ— å‚çš„ï¼Œå¯ä»¥é€šè¿‡å°†loss_fnè®¾ç½®ä¸ºlogisticæ¥å¯ç”¨ã€‚</p>
<h3 id="Softmax-loss"><a href="#Softmax-loss" class="headerlink" title="Softmax loss"></a>Softmax loss</h3><p>The last loss function is designed for when one wants a distribution on the probabilities of some entities being related to a given entity (contrary to just wanting a ranking, as with the ranking loss). For a certain positive ğ‘–, its score ğ‘ ğ‘– and the score ğ‘¡ğ‘–,ğ‘— of all the corresponding negatives ğ‘— are first converted to probabilities by performing a softmax: ğ‘ğ‘–âˆğ‘’ğ‘ ğ‘– and ğ‘ğ‘–,ğ‘—âˆğ‘’ğ‘¡ğ‘–,ğ‘—, normalized so that they sum up to 1. Then the loss is the cross entropy between this distribution and the â€œtargetâ€ one, i.e., the one that puts all the mass on the positive sample. So, in full, the loss for a single ğ‘– is âˆ’logğ‘ğ‘–, i.e., âˆ’ğ‘ ğ‘–+logâˆ‘ğ‘—ğ‘’ğ‘¡ğ‘–,ğ‘—.<br>æœ€åä¸€ä¸ªæŸå¤±å‡½æ•°è®¾è®¡ç”¨äºå½“äººä»¬å¸Œæœ›é€šè¿‡ç»™å®šä¸€ä¸ªå®ä½“ï¼Œå¾—åˆ°å…¶ä»–å®ä½“å’Œå…¶ç›¸å…³æ€§çš„æ¦‚ç‡çš„åœºæ™¯ã€‚ï¼ˆä¸ä»…éœ€è¦æ’åï¼Œä¸æ’åæŸå¤±ç›¸åï¼‰ã€‚å¯¹äºæŸä¸ªæ­£ä¾‹ğ‘–ï¼Œé¦–å…ˆé€šè¿‡æ‰§è¡Œsoftmaxï¼šğ‘ğ‘–âˆğ‘’ğ‘ ğ‘–å’Œğ‘ğ‘–ï¼Œğ‘—âˆğ‘’ğ‘¡ğ‘–ï¼Œğ‘—è¿›è¡Œå½’ä¸€åŒ–ï¼Œä½¿å¾—æ¦‚ç‡å’Œä¸º1ã€‚æ­¤æ—¶æŸå¤±å°±æ˜¯è¿™ç§åˆ†å¸ƒä¸â€œç›®æ ‡â€åˆ†å¸ƒä¹‹é—´çš„äº¤å‰ç†µã€‚å³å°†æ‰€æœ‰è´¨é‡ç½®äºæ­£æ ·æœ¬ä¸Šçš„äº¤å‰ç†µã€‚ å› æ­¤ï¼Œæ€»çš„æ¥è¯´ï¼Œå•ä¸ªğ‘–çš„æŸå¤±ä¸º-logğ‘ğ‘–ï¼Œå³-ğ‘ ğ‘–+ logâˆ‘ğ‘—ğ‘’ğ‘¡ğ‘–ï¼Œğ‘—ã€‚</p>
<p>This loss is activated by setting loss_fn to softmax.<br>é€šè¿‡åœ¨é…ç½®å‚æ•°ä¸­è®¾ç½®loss_fn=softmaxæ¥å®ç°é…ç½®ã€‚</p>
<h2 id="Optimizers"><a href="#Optimizers" class="headerlink" title="Optimizers"></a>Optimizers</h2><p>The Adagrad optimization method is used to update all model parameters. Adagrad performs stochastic gradient descent with an adaptive learning rate applied to each parameter inversely proportional to the inverse square magnitude of all previous updates. In practice, Adagrad updates lead to an order of magnitude faster convergence for typical PBG models.</p>
<p>Adagradä¼˜åŒ–æ–¹æ³•ç”¨äºæ›´æ–°æ‰€æœ‰æ¨¡å‹å‚æ•°ã€‚Adagradä½¿ç”¨ä¸æ‰€æœ‰å…ˆå‰æ›´æ–°çš„å¹³æ–¹åæ¯”æˆåæ¯”çš„è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œä»¥è‡ªé€‚åº”å­¦ä¹ ç‡æ‰§è¡Œéšæœºæ¢¯åº¦ä¸‹é™ã€‚ å®é™…ä¸Šï¼ŒAdagradæ›´æ–°ä½¿å…¸å‹PBGæ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦æé«˜äº†ä¸€ä¸ªæ•°é‡çº§ã€‚</p>
<p>The initial learning rate for Adagrad is specified by the lr config parameter.A separate learning rate can also be set for non-embeddings using the relation_lr parameter.<br>Adagradçš„åˆå§‹å­¦ä¹ ç‡ç”±configæ–‡ä»¶é‡Œçš„lrå‚æ•°æŒ‡å®šã€‚è¿˜å¯ä»¥ä½¿ç”¨related_lrå‚æ•°ä¸ºéåµŒå…¥è®¾ç½®å•ç‹¬çš„å­¦ä¹ ç‡ã€‚</p>
<p>Standard Adagrad requires an equal amount of memory for optimizer state as the size of the model, which is prohibitive for the large models targeted by PBG. To reduce optimizer memory usage, a modified version of Adagrad is used that uses a common learning rate for each entity embedding. The learning rate is proportional to the inverse sum of the squared gradients from each element of the embedding, divided by the dimension. Non-embedding parameters (e.g. relation operator parameters) use standard Adagrad.<br>æ ‡å‡†Adagradè¦æ±‚ä¼˜åŒ–ç¨‹åºçŠ¶æ€çš„å†…å­˜é‡ä¸æ¨¡å‹å¤§å°ç›¸åŒï¼Œè¿™å¯¹äºPBGå®šä½çš„å¤§å‹æ¨¡å‹æ˜¯ä¸å…è®¸çš„ã€‚ä¸ºäº†å‡å°‘ä¼˜åŒ–å™¨çš„å†…å­˜ä½¿ç”¨ï¼Œä½¿ç”¨äº†Adagradçš„ä¿®æ”¹ç‰ˆæœ¬ï¼Œè¯¥ç‰ˆæœ¬å¯¹æ¯ä¸ªå®ä½“åµŒå…¥ä½¿ç”¨é€šç”¨çš„å­¦ä¹ ç‡ã€‚å­¦ä¹ ç‡ä¸åµŒå…¥å…ƒç´ çš„æ¢¯åº¦å¹³æ–¹çš„å€’æ•°ä¹‹å’Œæˆæ¯”ä¾‹ï¼Œå†é™¤ä»¥å°ºå¯¸ã€‚ éåµŒå…¥å‚æ•°ï¼ˆä¾‹å¦‚å…³ç³»è¿ç®—ç¬¦å‚æ•°ï¼‰ä½¿ç”¨æ ‡å‡†Adagradã€‚</p>
<p>Adagrad parameters are updated asynchronously across worker threads with no explicit synchronization. Asynchronous updates to the Adagrad state (the total squared gradient) appear stable, likely because each element of the state tensor only accumulates positives updates. Optimization is further stabilized by performing a short period of training with a single thread before beginning Hogwild! training, which is tuned by the hogwild_delay parameter.<br>Adagradå‚æ•°åœ¨å·¥ä½œçº¿ç¨‹ä¹‹é—´å¼‚æ­¥æ›´æ–°ï¼Œè€Œæ²¡æœ‰æ˜¾å¼åŒæ­¥ã€‚å¼‚æ­¥æ›´æ–°ä½¿å¾—AdagradçŠ¶æ€(æ€»å¹³æ–¹æ¢¯åº¦)çš„æ›´æ–°æ˜¾å¾—ç¨³å®š,è¿™å¯èƒ½æ˜¯å› ä¸ºçŠ¶æ€å¼ é‡çš„æ¯ä¸ªå…ƒç´ ä»…ç´¯ç§¯æ­£æ›´æ–°ã€‚åœ¨å¼€å§‹Hogwildä¹‹å‰ï¼Œå¯ä»¥é€šè¿‡å•çº¿ç¨‹è¿›è¡ŒçŸ­æœŸåŸ¹è®­æ¥è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚ åŸ¹è®­ï¼Œå…·ä½“å–å†³äºhogwild_delayå‚æ•°ã€‚</p>
<p>In distributed training, the Adagrad state for shared parameters (e.g. relation operator parameters) are shared via the parameter server using the same asynchronous gradient update as the parameters themselves. Similar to inter-thread synchronization, these asynchronous updates are stable after an initial burn-in period because the total squared gradient strictly accumulates positive values.<br>åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œå…±äº«å‚æ•°ï¼ˆä¾‹å¦‚å…³ç³»è¿ç®—ç¬¦å‚æ•°ï¼‰çš„AdagradçŠ¶æ€é€šè¿‡å‚æ•°æœåŠ¡å™¨ä½¿ç”¨ä¸å‚æ•°æœ¬èº«ç›¸åŒçš„å¼‚æ­¥æ¢¯åº¦æ›´æ–°è¿›è¡Œå…±äº«ã€‚ä¸çº¿ç¨‹é—´åŒæ­¥ç±»ä¼¼ï¼Œè¿™äº›å¼‚æ­¥æ›´æ–°åœ¨åˆå§‹é˜¶æ®µä¼šä¸ç¨³å®šï¼Œä¹‹åä¼šä¿æŒç¨³å®šï¼Œå› ä¸ºæ€»å¹³æ–¹æ¢¯åº¦ä¸¥æ ¼ç´¯åŠ æ­£å€¼ã€‚</p>
<blockquote>
<p>ğŸ¤” </p>
</blockquote>
<p>æºåœ°å€ï¼š<a target="_blank" rel="noopener" href="https://torchbiggraph.readthedocs.io/en/latest/loss_optimization.html">https://torchbiggraph.readthedocs.io/en/latest/loss_optimization.html</a></p>

    </div>


    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/PyTorch-BigGraph/" rel="tag"># PyTorch-BigGraph</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/10/28/PBG-Distributed-mode/" rel="prev" title="PyTorch-BigGraph åˆ†å¸ƒå¼">
      <i class="fa fa-chevron-left"></i> PyTorch-BigGraph åˆ†å¸ƒå¼
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/11/04/Eva-Classification-Problem/" rel="next" title="åˆ†ç±»é—®é¢˜ä¸­çš„è¯„ä¼°æŒ‡æ ‡">
      åˆ†ç±»é—®é¢˜ä¸­çš„è¯„ä¼°æŒ‡æ ‡ <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  
  


          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#PyTorch-BigGraph-%E6%8D%9F%E5%A4%B1%E8%AE%A1%E7%AE%97"><span class="nav-number">1.</span> <span class="nav-text">PyTorch-BigGraph æŸå¤±è®¡ç®—</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Negative-sampling"><span class="nav-number">1.1.</span> <span class="nav-text">Negative sampling</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#All-negatives"><span class="nav-number">1.1.1.</span> <span class="nav-text">All negatives</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Same-batch-negatives"><span class="nav-number">1.1.2.</span> <span class="nav-text">Same-batch negatives</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Uniformly-sampled-negatives"><span class="nav-number">1.1.3.</span> <span class="nav-text">Uniformly-sampled negatives</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Loss-functions"><span class="nav-number">1.2.</span> <span class="nav-text">Loss functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Ranking-loss"><span class="nav-number">1.2.1.</span> <span class="nav-text">Ranking loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Logistic-loss"><span class="nav-number">1.2.2.</span> <span class="nav-text">Logistic loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Softmax-loss"><span class="nav-number">1.2.3.</span> <span class="nav-text">Softmax loss</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Optimizers"><span class="nav-number">1.3.</span> <span class="nav-text">Optimizers</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Bin"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Bin</p>
  <div class="site-description" itemprop="description">æˆ‘ç†Šäººæ—æ°¸ä¸ä¸ºå¥´ï¼ï¼å¼å¼å¼ï¼ï¼ï¼</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">50</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Bin-Go2" title="GitHub â†’ https:&#x2F;&#x2F;github.com&#x2F;Bin-Go2" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:binzhangsam@gmail.com" title="E-Mail â†’ mailto:binzhangsam@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Bin</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv">æ€»è®¿é—®é‡<span id="busuanzi_value_site_pv"></span>æ¬¡</span>

<div class="theme-info">
    <div class="powered-by"></div>
    <span class="post-count">å…±50.1kå­—</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://https-bin-go2-github-io.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "http://example.com/2020/10/30/PBG-Loss-calculation/";
    this.page.identifier = "2020/10/30/PBG-Loss-calculation/";
    this.page.title = "PyTorch-BigGraph æŸå¤±è®¡ç®—";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://https-bin-go2-github-io.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
